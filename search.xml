<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[读《比特币》]]></title>
    <url>%2F2018%2F04%2F17%2Fread-bitcoin%2F</url>
    <content type="text"><![CDATA[历史基本上就是政府制造通货膨胀的过程。——哈耶克 作者简介：李钧，壹比特数字科技首席执行官。长铗，著名科幻作家，巴比特社区创始人。 在布雷顿森林体系解体之前，理论上，一美元的背后就有对应的黄金作为支撑，这样你在世界上使用美元，别人才会承认其合法地位，因为拿到美元之后可以换到等值的黄金，但是在美国相继发动朝鲜战争、越南战争、两次石油危机等之后，美国财政赤字大幅上升，导致美国政府发行大量货币来弥补财政赤字，而对应的黄金储备并没有增加，所以理论上，就是一美元已经换不到对应的等值黄金了，所以这是导致布雷顿森林体系解体的直接原因。 在布雷顿森林体系崩溃之后，美元与黄金脱钩，各国法定货币与美元挂钩，而想要维持美元的强势地位，美元的背后就必须要有硬通货，而这个硬通货从早期的黄金变成了如今的石油，那么为何石油输出国甚至是欧佩克要选择美元作为全球石油贸易的结算货币呢？可以说这是典型的美国精英们所设计的一套完美体系，因为美国依靠其强大的军事力量，和中东产油国们达成了一项协定，就是保证中东产油国中那些落后的君主制国家的国王们拥有其长期统治地位，相应地，这些产油国要使用美元结算全球石油贸易，这才让美元力挽狂澜，重新变成全球的硬通货。 法币的缺陷非常明显，从历史中的历次危机可知，任何一次经济危机的解决办法都是量化宽松政策，也就是大量印钞，这直接导致了发行法币只为政府所垄断，同时没有任何政策或者制度可以限制政府发行法币的数量，所以通胀是历史的必然，纵观历朝历代均以超级通胀而结束统治。由此哈耶克老爷子在《货币非国家化》一书的序言中指出“我相信，世界各国的君主，都是贪婪不公的。他们欺骗臣民，把货币最初的所含金属的真实成分，次第削减”，而该段序言出自亚当·斯密的《国富论》，可以说这两本都是鼎鼎大名的好书。哈耶克老爷子的主张是：在一个国家，发行具有明显差异的，并由不同货币单位构成的货币，包括让私人货币流通，并实现不同货币之间的竞争。只有这样，所有的货币发行单位才会紧缩其货币发行量，以避免因货币不断贬值而最终被淘汰的命运。这与现在的比特币和一众山寨币思想何其相似，可以说，这么有先见之明的思想已经可以指导如今的数字货币运行方式了，当然了并不是指导数字货币的内在技术原理。 人与人的差距也许真的就在那一瞬间显现出来，在此之前的多年积累可能很难给你带来质的改变，但是我一直默默坚持，从不轻言放弃，直到质变的那天到来。在李笑来了解到比特币之后，立马放弃了艾德睿智咨询公司的股份，同时放弃了knewone公司的大部分股份，而knewone也一直是我想做的内容，这是一个精品导购网站，因为我觉得在淘宝上买东西太浪费时间了，所以我只去京东，价格可能贵点，但是节约时间才是最重要的，而且没有人能够对所有的东西都非常了解，最好大家把各自了解的领域，以及筛选出来的精品贡献出来，那么我们直接选购即可，从而节省了海量的挑选精力。李笑来说“我反复强调，同样的东西放在那里，人们各自看到的是不相同的，至于他看到的是什么，完全取决于他过去的知识积累和他的思考层次，仅此而已，所以很多争论是没有必要的”，在此与大家共勉，不要与无知的人争论，有时间还是多提高自己的思考层次更加重要。 货币的非国家化，或者说一个非中心非集权的货币是很多人的梦想，哈耶克早在他的著作《货币的非国家化》中提到“如果一种货币的发行量被一个机构刻意控制着，而这个机构的自私自利驱使它满足了其使用者的愿望，那么它就是一种最佳货币”。这和我心中的想法不谋而合，现在的数字货币市场就是这样的一种雏形，以太坊的横空出世使得任何机构或个人发行Token都变得极其简单，而所发行的Token到底能不能获得市场的认可，这就是一个长期的竞争的过程，在此期间，归零币会大量的出现，到最后，大浪淘沙，经受住了市场考验的币，就是最后的金子。在哈耶克老爷子诞辰110周年的2009年，比特币呱呱坠地；经过几年的发展，他的非国家化的货币理想竟隐约实现，虽然和他设想的形式有些不同。 金属货币的世界靠天然的产量限制货币发行量，靠天然的化学属性进行防伪，靠天然的珍惜性保证购买力；纸币的世界靠中央银行的领导和经济专家决定发行多少货币，靠不断提高制作工艺和更高级的验钞机进行防伪，靠国家力量来保证购买力；而在比特币的世界，上面的规则通通失效。数字世界有自己的规则：通过数学，更确切的说是通过密码学保证比特币种种天方夜谭般不可思议的特性。没有任何集权式的中心发币系统是可靠的，而且这个社会上普遍认同的通货紧缩会导致经济萧条也只是愚昧大众罢了，他们说只有适当的通胀才能刺激经济的发展，进而不断的增加法币的供应，以此来剥削全部的国民。而其理由是，若采用通货紧缩的货币政策，那么会导致钱越来越值钱，这样大家都拿着钱，等待其更加值钱的时候再去消费，这纯粹是忽悠大众的说辞。就拿现实中的一个运行良好的通缩系统为例，电子产品市场是天然通缩的，大家都知道几个月之后的iPhone X会更加便宜，但是如果你现在就需要使用iPhone X且资金充足你会专门等待几个月之后再购买吗？换句话说，大家都知道这是个通胀的世界，那么你会避免以后物品涨价而将你这辈子的需要的东西都一次性全部买完吗？显然是不会的，所以所有当权者所鼓吹的货币通缩政策会导致经济的萧条可能仅仅是忽悠国民罢了。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《独裁者手册》]]></title>
    <url>%2F2017%2F12%2F27%2Fread-the-dictator-s-handbook%2F</url>
    <content type="text"><![CDATA[我们最美好的愿望就是那些冒着生命危险与独裁者周旋的人们能够幸福和成功。 作者简介：布鲁斯•布鲁诺•德•梅斯奎塔，纽约大学政治学系 Julius Silver 讲座教授、亚历山大•汉密尔顿政治经济学研究中心主任和斯坦福大学胡佛研究所高级研究员。他通过设立于纽约的咨询公司长期担任美国政府国家安全事务方面的顾问，也为众多公司提供谈判指导与结果预测方面的咨询服务。梅斯奎塔1971年从密歇根大学获得政治科学博士学位。2001—2002年他担任国际研究学会主席。他是美国艺术与科学院院士、美国对外关系理事会会员以及古根海姆基金会学者。梅斯奎塔迄今出版了16本书，超过120篇论文，并在《纽约时报》、《洛杉矶时报》、《芝加哥论坛报》、《国际先驱论坛报》等报刊上发表过大量文章。 阿拉斯泰尔•史密斯，纽约大学政治学教授。他此前在圣路易斯华盛顿大学和耶鲁大学任教。他从美国罗彻斯特大学获得政治科学博士学位，从牛津大学获得化学学士学位。他从美国国家科学基金会获得过三项研究津贴，2005年他获得卡尔•多伊奇奖（Karl Deutsch Award），该奖每两年一次颁发给40岁以下最出色的国际关系研究学者。1997—1998年他被斯坦福大学胡佛研究所选为国家研究员。 注：中文版为删减版，所有涉及政治敏感的内容都被删除，如果英文不错，建议阅读经典原版书籍。 统治的规则在引言中，作者列举了洛杉矶周边的一座小城贝尔为例，该城是宪章城市，即由城市宪章统治，而统治宪章的则是城市的一小撮权力顶端的人，阐述了许多统治法则的经验教训，即政治就是获得和维持政治权力，它与“我们，人民”的普遍幸福无关。其次，确保政治生存的最好方式是只依靠少数人来上位和在位。这一点与现实也很是相符，比如拥有一批同伙可以依靠的独裁者，其往往在位时间比较长，而反观依靠大多数人民上位的民主人士，其在位时间则稍短。像我们的主席任期一般是十年，美国一般是四年，如果获得连任，最多则是八年，俄罗斯目前也是八年。 在这里，作者还引出了一个有名的争论，即“是小共和国还是大共和国更有利于选出公共福利的适当守护者”。从这个问题就知道，自然是分为两派，其中麦迪逊支持后者，而孟德斯鸠则支持前者，作者以贝尔城的事实为据，同样支持后者。并指出那些觉得小共和国更有利的人往往只着眼于很少的抽样资料，缺乏现代化的分析工具，结果就是常常得出部分正确但深层次错误的结论。 任何国家或任何企业中最主要的利益推动者是居于顶端的人——领导人。那么对于一个领导人来说什么才是“最佳”的统治方式？答案就是：采取一切必要手段先攫取权力，然后维持权力，并从始至终掌握尽可能多的国家或企业收入。 政治的法则这里有一段话需要好好理解“当一个国家处于财务危机，债务超过了偿债能力的时候，对领导人来说，真正的麻烦不在于必须削减公共开支，而在于他丧失了必要的资源去换取核心支持者的政治忠诚”。在早期我的理解中，陷入财务危机，那就削减开支，开源节流，慢慢偿还债务不就行了吗？现实往往并没有这么简单，当你不断削减开支的时候，那些可能的原先支持者们已经无法从你这获取利益了，进而影响你的执政根基；而一些本来打算支持你以便换取个人利益的人们，突然发现你已经无法满足他们的需求的时候，自然也是风流云散，这意味着你的支持者们不仅无法在数量上维持现状，还会变的越来越少，同时政治忠诚度也在不断衰减。 当年的法王路易十四就面临这样的困境，而他的策略就是打破军队中士兵的上升的通道，建立一支职业化，相对平民化的军队；还要大量提拔众多新人，形成对他感恩的新阶层，而这些新阶层多是底层出身，能得到国王这样的厚爱，必然俯首帖耳。除了让新人出头，还要对老旧的贵族进行权力限制，防止发动或者煽动反君主政变。通过扩大核心集团的候选人范围，使得大家形成一个互相竞争的机制，那些老的核心集团的人们也知道，如果他们不能证明自己值得信赖，很容易就会被别人取而代之，而这种竞争的评判标准自然就是对国王的忠诚度了，通过一系列的举措，不仅稀释了老旧贵族的权利，还建立起了一批对自己更加忠诚的新阶层。 路易十四的事实证明，没有人能独自统治；没有人具有绝对权威。差别只在于有多少人需要豢养，又有多少资源能够拿出来进行豢养。 在这里作者还提出了三维政治结构，即名义选择人集团（包含了所有在选择领导人时至少具有某些法定发言权的人）、实际选择人集团（这是真正选择领导人的集团）和致胜联盟（它是实际选择人集团的一个子集，他们的支持对于一个领导人的政治生存至关重要）。在不同国家、不同企业或任何其它组织里，这三种人群的规模差异几乎决定了政治里发生的一切——领导人能做什么，什么事是他们能或不能逃脱追究的，他们必须对谁负责，在他们的领导之下每个人享有的相对生活质量或者说常常不享有。 由此就可以解释到底什么样的国家赋税重什么样的国家赋税轻了？原因若何，比如普选的国家，如果不考虑选区等细节的话，将所有人民看做是致胜联盟，这样领导人无法通过提供私人好处来有效维持权力。因为不可或缺之人的数量太大，所以他们只能着眼于提供公共物品而非私人回报，相对来讲他们也必须把税率维持在一个较低的水平。但当致胜联盟很小并且私人物品成为维持权力的有效方法时，大部分人民的福利就靠边站了，这种情形，领导人喜欢征重税，从可怜的可相互替代者以及被剥夺选举权的人那里尽可能搜刮，通过这种方式来重新分配财富，把财富转移到致胜联盟成员的手上，使他们发财致富，保持忠诚，所以看你身上的赋税程度就可以知道你是否处于民主国家，或者说你们的致胜联盟是否是普罗大众。 经过这么多分析，可以得出如果你想要在任何体制下都获得成功，那么需要遵循如下5个基本法则 让你的致胜联盟越小越好，一个小规模的致胜联盟使领导人只需依赖极少数人就能保持权位 让你的名义选择人集团越大越好，保持一个很大的选择人集团你就能很容易地替换掉实际选择人集团和致胜联盟中的捣蛋分子。一个很大的选择人集团提供了充足的替代支持者，让致胜联盟者时刻谨记必须保持忠诚、规规矩矩，不然就会被别人取代 掌控收入的分配，最有效的资金分配方式是让很多人受穷，通过重新分配让挑选出来的支持者发财，朝鲜模式，巴基斯坦也是 支付给你的核心支持者刚好足够确保他们忠诚的钱，你的优势在于你知道钱在哪里而他们不知道，给你的联盟足够的钱，以免他们到处寻找取代你的人，但一分钱都不要多给 不要从你的支持者的口袋里挪钱去改善人民的生活，以前总是希望产生一个英明的领导人能够站出来改善人民的生活，提高中下层人民的收入，现在看来是不可能了，提高人民的生活，必然会触动支持者们的利益，所以还是放弃幻想吧 上台获取权力意味着要抓住转瞬即逝的机会，快速果敢地行动，只争朝夕。然而，当上领导人并不是政治的结束。就算你已经大权在握，享受权力的好处，其他人仍在虎视眈眈。他们也想要你这拼命得来的职位！政治是危险的职业。 掌权刚刚上台要保住权位很困难，但一名成功的领导人能够攫取权力、对助他上台的联盟重新洗牌从而大大加强自身力量。一名聪明的领导人会将一些早先的支持者炒鱿鱼，替换成更可靠和更便宜的人。但不管他如何将朋友亲信塞满联盟，除非他回报他们，否则他们不会保持忠诚。 劫贫济富领导人在人民那里能搜刮到多少钱方面面临三个限制：第一，征税会削弱人们的工作热情；第二，一些税收方面的负担不可避免会落到领导人的关键支持者身上；第三，税收需要专业知识和资源。重税难以持久，这在所有国家都经过了证明，不论是民主国家还是独裁者国家。在民主国家克以重税将导致失去选民的支持，而在独裁国家克以重税将导致人民无情的反抗。除了税收，另一个领导人可以获取财富的来源就是开采资源，但是这通常导致一种“资源诅咒”的现象，拥有丰富可开采自然资源的国家系统性地落后于资源稀缺的国家，比如委内瑞拉远远落后于日本。 在这里作者提出了一个非常重要的观点，就是对贫穷国家进行的债务减免只会导致该国的债务总额攀升，并且对于人民来说，这些攀升的债务无法带来任何生活水平的提高，因为这些新形成的债务，其利益都被统治者和其致胜联盟瓜分了。所以某国就不要没脑子的只知道减免非洲国家的债务了，在你减免之后，这些国家又会重新借债。而且减免债务会缓解这些穷国的财政压力，使得独裁领导人无须改革仍能坐在位子上，继续让人民生活在水深火热之中了，从这个角度来说，减免债务相当于是助纣为虐，反而会巩固独裁统治者的地位。 获取与花费在小联盟国家，公共物品主要服务于小范围的领导层利益，只间接服务于人民的利益。在大联盟国家情况则几乎完全不同。拿我们日常生活中常见的自由举例，包括言论自由、集会自由、新闻自由等。独裁国家害怕它，民主国家却只能被迫欢迎它，这究竟是为什么呢？毫无疑问，民主领导人也巴不得避开这些自由权利，因为正是这些公共物品使得竞争对手组织起来推翻他们变得容易。但依赖大型致胜联盟的领导人无法回避这些自由，因为如果他无法保证很大数量的人们拥有自由言说、阅读、书写的权利、能够聚到一起自由探讨和辩论，他们将不可能聚集起一个致胜联盟。民主领导人必须倾听选民的声音，回应选民的期望，不然就会有别人上台来做，这也正是民主国家的普通人民的福利都远远好于独裁国家的本质原因所在。 当政府依赖大量关键支持者，它们必须分配政府资源，提供有价值的公共物品，比如可靠的建筑规范、自然灾害之后的救援行动以及如果可能的话，预防灾害的堤坝、沟渠等。为了了解人民的需要，政府必须让公众能容易表达他们想要什么样的公共物品。做到这点的最佳方法就是提供所有公共物品当中最便宜但最有价值的那个：自由。 腐败使人有权，绝对的腐败绝对使人有权如果你想保持你的地位，那么需要考虑三件事，第一是得到和保持权力，第二最大限度地掌握收入的开支权，第三把做好事的愿望置于自己的政治生存和掌控大局的程度之后。 领导人如果将联盟的忠诚看做理所当然，将面临极大风险。支配统治者的那些规则教导我们，领导人给联盟的钱绝对不能缺斤少两，无论这么做是为了给自己留下好处还是为普通人民谋福利。想要自己发财的领导人，应该从可以自由裁量的资金里面拿，不能动用给联盟的钱。同样地，那些想为人民谋福利的领导人，必须从自己口袋里掏钱，不能损害到联盟的利益，否则他们不仅将丢掉权位，而且常常要付出生命的代价。例子犯罪头目巨头保罗·卡斯特拉诺和罗马皇帝裘里斯·凯撒。 旨在消除腐败的法律手段从来不会奏效，反而常常让情况恶化。对付腐败的最佳方式就是改变深层诱因。随着联盟规模增大，腐败会逐渐消失。相同的逻辑在所有的种类的组织中都适用。通过立法和行政监管政策来揪出和检控腐败行为，这些措施只是门面功夫，烟雾弹或者猎巫行动都无法消除腐败，但是如果让领导人对更多的人负责，政治就将成为良好理念的竞争，而不再是贿赂和腐败的竞争。然而领导人不愿意负更多责任，因为这将缩短他们的在位期，减少他们的自由裁量权。这也正是下一章节将要探讨的内容。 对外援助本章将探讨如下五个问题。谁对谁提供援助？提供多少援助？为什么提供援助？援助的政治和经济后果是什么？关于国家构建，上述问题的答案能告诉我们什么？ 援助的主要目的不是为了减轻受援国的贫困或惨境；它的目的是让援助国的选民过得更好。外援无法消除贫困并非是因为援助国给穷国提供的钱太少。有大量援助给了腐败的受援国政府，但这是有意为之的，援助之所以要给这些盗用的政府恰恰是因为这些政府会出卖本国人民以换取自己的政治安全。援助国愿意给它们提供这种安全以换取政策，这些政策反过来提升了援助国本国选民的福利，从而让援助国政府也更加安全。事实就是，对世界来说，援助只有微利却有百害。 反叛中的人民一直有在思考，为什么独裁政府或者专制政府能够获得长时间的统治呢？为什么人民不起来反抗他们呢？作者给出了答案，一个关键的时间点，一个临界点，在这个点上人们认为未来继续在这个政府统治下的生活仍将足够糟糕，值得他们付出革命的代价。这里有一个微妙的平衡点，如果一个政权非常善于说服人民越雷池一步意味着极其悲惨的后果甚至死亡，反叛就不大可能发生。 即便革命这获得了成功，但是有一点不能忽视，就是大多数革命者一旦上台，就倾向于成为卑鄙的独裁者。毕竟民主制度将让他的统治变得更难，而想要领导人顺从民意，除非人民有能力迫使他们。很多革命的结果无非是让一个专制的政权取代另一个而已，这在中国的军阀混战年代就是最好的例证。 战争，和平与世界秩序关于战争的逻辑，民主领导人只在认为几乎肯定会赢的情况下才会开战，而对于独裁者来说，不用考虑输赢的问题，因为他们愿意承受更大的风险。按照这套结论，前提是该结论是正确的，那么两个民主国家不大可能互相开战，发起进攻的民主国家必须很肯定对方没有胜利的把握，这就导致了一些民主国家倾向于进攻一些非常弱小的民主国家，比如美国1965年推翻多米尼加共和国总统胡安·博什；法国1923年侵略魏玛共和国等，那么如果再进一步推演，美国是一个大的民主国家，对于进攻中国它没有必胜的把握，导致它不敢进攻中国，相反的很可能是中国率先发起对美国的进攻。 怎么办？我们常常谈到美国梦，但是你有没有想过这背后的原因？也许正是美国开放的移民政策不断的扩大了美国的选举人集团，进而扩大美国的致胜联盟，在移民政策上，使得移民相对容易获得公民权的国家，形成了一个长效的健康循环，扩大的致胜联盟导致国家会更加为普通大众谋求公共福利或政策，而这些福利或政策推动着人民生活水平的提高，经济的健康发展，国家的繁荣富强，进而使得政治体制更加的健康。 一个让独裁者国家向民主国家转变的方式是给予独裁者一些激励政策。联合国可以为独裁制向民主制的转型制定实施计划。同时它可以规定，任何面临交权压力的独裁者将获得一定的短暂时间，比如说一个星期，离开本国以交换完全的永久性特赦，在任何地方得免因其罪行遭受指控。一些受害者当然会因犯下凶残罪行的作恶者逃脱法律制裁而极其不满。不幸的是，不这样做的话，独裁者没有其它选择，只能通过犯下更多罪行牢牢抓住权力赌一把。让独裁者逃脱法律制裁当然没什么正义可言。但我们的目标应当是改善毫无前途的领导人统治下的很多人的生活，这些领导人或许有可能下台以换取特赦。 生活中的政治几乎都围绕选择人集团、有影响者集团和致胜联盟这三个维度的规模展开。把它们的规模全部扩大，并且可相互替代者集团的扩张速度不再快于致胜联盟，则对绝大多数人民来说一切都会变得更好。他们摆脱了束缚，为了自身而更加努力工作，变得更加有教养，更健康，更富裕，更快乐而自由。他们纳的税减少，生活中的机会显著扩大，或迟或早每个社会都将跨越由小联盟与大选择人集团割裂而造成的痛苦，走向一个大联盟——它本身就占选择人集团的很大一部分——和平富足因此指日可待。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 近实时搜索]]></title>
    <url>%2F2017%2F12%2F12%2Flucene-near-real-time-search%2F</url>
    <content type="text"><![CDATA[Lucene 事务有过数据库经验的人都知道ACID特性，原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。由于隔离性的存在，对于新的变更包括添加、修改、删除，如果不进行 commit 的话，那么在读端是无法看到数据的变化的，在这里简单的介绍下 Lucene 中的事务，即ACID。 原子性当你在一次 IndexWriter 的 session 中做操作（增加，删除文档），然后 commit，要么你的所有的操作修改都是可见的（commit 成功），要么所有的操作修改都不可见（commit 失败），绝不会处于某种中间状态。有些方法有它自身的原子操作：如果你调用 updateDocument 方法，其内在实现是先删除后添加文档，即使你打开了一个近实时（NRT）reader 或者使用另一个线程做 commit，绝不会出现只有删除而没有添加的情况。与此类似，如果使用 addDocuments 方法添加一组文档，对于任何 reader 而言，要么所有的文档可见，要么所有文档不可见。 一致性如果计算机或者 OS 崩溃，或者 JVM 挂掉或被杀死，亦或是电源被拔掉了，你的索引都会保持完好。注意，像 RAM 故障，CPU 位翻转或者文件系统损坏之类的问题，还是容易造成索引破坏的。 隔离性当 IndexWriter 正在做更改的时候，所有更改都不会对当前搜索该索引的 IndexReader 可见，直到你 commit 或者打开了一个新的 NRT reader。一次只能有一个 IndexWriter 实例对索引进行更改。 持久性一旦 commit 操作返回，所有变更都会被写入到持久化存储。如果计算机或者 OS 崩溃，或者 JVM 挂掉或被杀死，亦或是电源被拔掉了，所有的变更都已在索引中保存。 Lucene 近实时搜索如果对数据没有实时性的要求，那么完全不用关心 Lucene 的近实时搜索。但是由于隔离性的存在，我们知道，对于新的数据，如果写端不 commit 的话，那么搜索端则是不可见的；还有另外一种情况，就是在搜索端可以持有 IndexWriter 的情况下，利用 IndexWriter 打开 reader，则可以保证 reader 能够看到最新的数据，包括还未提交的数据。同样一旦 reader 打开索引之后，对于写端新的未提交数据同样不可见。 除此之外 Lucene 与数据库还稍有不同，就是 Lucene 使用 IndexReader 打开索引的时候，相当于是对当前索引做了一次快照（Snapshot），即便写端进行了 commit 操作，如果 IndexReader 不重新打开的话，新提交的内容对搜索端依然是不可见的。那么要实现 Lucene 的近实时搜索，关键有两个方面，一是写端定期进行 commit 操作，二是搜索端定期进行 reopen 操作。 搜索端持有 IndexWriter对于大规模的搜索应用，一般都是读写分离的，而且由于 Lucene 在同一时刻，仅允许有一个写端存在，但可以有多个读端同时存在，所以这种架构在生产环境其实很少使用，因为所有的读端基本上都是和写端隔离甚至分别部署在不同的应用之中，不过这个知识在某些场景依然很有用，同时了解这个知识可以帮助你更加深入的理解 Lucene。 由自己负责重新打开123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntPoint;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;public class NearRealTimeSearchDemo &#123; public static void main(String[] args) throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); Document document = new Document(); document.add(new TextField("title", "Doc1", Field.Store.YES)); document.add(new IntPoint("ID", 1)); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); indexWriter.addDocument(document); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(indexWriter)); int count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; //即便没有commit，依然能看到Doc1 System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); document = new Document(); document.add(new TextField("title", "Doc2", Field.Store.YES)); document.add(new IntPoint("ID", 2)); indexWriter.addDocument(document); indexWriter.commit(); count = indexSearcher.count(new MatchAllDocsQuery()); TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; //即便后来commit了，依然无法看到Doc2，因为没有重新打开IndexSearcher System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; System.out.println("================================="); indexSearcher.getIndexReader().close(); indexSearcher = new IndexSearcher(DirectoryReader.open(indexWriter)); count = indexSearcher.count(new MatchAllDocsQuery()); topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; //重新打开IndexSearcher，可以看到Doc1和Doc2 System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; System.out.println("================================="); &#125;&#125; 输出结果1234567Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;=================================Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;=================================Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;Document&lt;stored,indexed,tokenized&lt;title:Doc2&gt;&gt;================================= 使用ControlledRealTimeReopenThread定时重新打开Lucene 提供了 ControlledRealTimeReopenThread 线程工具类来负责周期性的打开 ReferenceManager（调用ReferenceManager.maybeRefresh）。该线程类控制打开间隔比较灵活，当有外部用户在等待指定的 generation 时就按最小时间间隔等待，如果没有用户着急获取最新的Searcher，则等待最大时间间隔后再打开。其构造函数如下： ControlledRealTimeReopenThread(TrackingIndexWriter writer, ReferenceManager manager,double targetMaxStaleSec, double targetMinStaleSec)//单位秒 如何判断是否有人等待？ 在调用 addDocument 的时候，IndexWriter 为每次更新索引的操作赋予一个标记（generation，代数），递增变化。用户使用 ControlledRealTimeReopenThread.waitForGeneration 告诉其期望获得更新代数，ControlledRealTimeReopenThread 记录了当前已打开的代数，当期望更新代数大于已打开代数时，就表示有用户期望获得最新的 Searcher。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntPoint;import org.apache.lucene.document.TextField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.*;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;import java.util.concurrent.TimeUnit;public class NearRealTimeSearchDemo &#123; public static void main(String[] args) throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); Document document = new Document(); document.add(new TextField("title", "Doc1", Field.Store.YES)); document.add(new IntPoint("ID", 1)); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); indexWriter.addDocument(document); indexWriter.commit(); SearcherManager searcherManager = new SearcherManager(indexWriter, null); //当没有调用者等待指定的generation的时候，必须要重新打开时间间隔5s，言外之意，如果有调用者在等待指定的generation，则只需等0.25s //防止不断的重新打开，严重消耗系统性能，设置最小重新打开时间间隔0.25s ControlledRealTimeReopenThread&lt;IndexSearcher&gt; controlledRealTimeReopenThread = new ControlledRealTimeReopenThread&lt;&gt;(indexWriter, searcherManager, 5, 0.25); //设置为后台线程 controlledRealTimeReopenThread.setDaemon(true); controlledRealTimeReopenThread.setName("controlled reopen thread"); controlledRealTimeReopenThread.start(); int count = 0; IndexSearcher indexSearcher = searcherManager.acquire(); try &#123; //只能看到Doc1 count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; searcherManager.release(indexSearcher); &#125; document = new Document(); document.add(new TextField("title", "Doc2", Field.Store.YES)); document.add(new IntPoint("ID", 2)); indexWriter.addDocument(document); try &#123; TimeUnit.SECONDS.sleep(6); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //休息6s之后，即使没有commit，依然可以搜索到Doc2，因为ControlledRealTimeReopenThread会刷新SearchManager indexSearcher = searcherManager.acquire(); try &#123; count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; searcherManager.release(indexSearcher); &#125; document = new Document(); document.add(new TextField("title", "Doc3", Field.Store.YES)); document.add(new IntPoint("ID", 3)); long generation = indexWriter.addDocument(document); try &#123; //当有调用者等待某个generation的时候，只需要0.25s即可重新打开 controlledRealTimeReopenThread.waitForGeneration(generation); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; indexSearcher = searcherManager.acquire(); try &#123; count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; searcherManager.release(indexSearcher); &#125; &#125;&#125; 输出结果123456789Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;=================================Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;Document&lt;stored,indexed,tokenized&lt;title:Doc2&gt;&gt;=================================Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;Document&lt;stored,indexed,tokenized&lt;title:Doc2&gt;&gt;Document&lt;stored,indexed,tokenized&lt;title:Doc3&gt;&gt;================================= 搜索端不持有 IndexWriter在实际的工作中，这种模式才是最常见的现象，由于索引可以有多个用户同时搜索，那么只要每一个搜索端都不持有 IndexWriter，就可以实现一个简单的分布式搜索服务。由前面所知，这种情况下，只有当写端进行 commit 操作之后，搜索端重新打开 reader 才能搜索到最新的数据。 使用DirectoryReader刷新在示例代码中虽然 IndexWriter 和 IndexSearcher 写在同一个类中，但是我们并不从 IndexWriter 打开索引，而是通过 Directory 打开索引，这里仅是简单示例，所以实际开发中，这个类中完全没有 IndexWriter 也是可以的。 想要看到新的结果就需要重新打开一个 IndexReader，DirectoryReader 提供了 openIfChanged(DirectoryReader oldReader) 函数，只有索引有变化时才返回新的 reader（不是完全打开一个 new reader，会复用 old reader 的一些资源，并入新索引，降低一些开销）， 否则返回 null。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntPoint;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;public class NearRealTimeSearchDemo &#123; public static void main(String[] args) throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); Document document = new Document(); document.add(new TextField("title", "Doc1", Field.Store.YES)); document.add(new IntPoint("ID", 1)); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); indexWriter.addDocument(document); indexWriter.commit(); DirectoryReader directoryReader = DirectoryReader.open(ramDirectory); IndexSearcher indexSearcher = new IndexSearcher(directoryReader); document = new Document(); document.add(new TextField("title", "Doc2", Field.Store.YES)); document.add(new IntPoint("ID", 2)); indexWriter.addDocument(document); //即使commit，搜索依然不可见，需要重新打开reader indexWriter.commit(); //只能看到Doc1 int count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); //如果发现有新数据更新，则会返回一个新的reader DirectoryReader newReader = DirectoryReader.openIfChanged(directoryReader); if (newReader != null) &#123; indexSearcher = new IndexSearcher(newReader); directoryReader.close(); &#125; count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); &#125;&#125; 输出结果12345Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;=================================Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;Document&lt;stored,indexed,tokenized&lt;title:Doc2&gt;&gt;================================= 使用SearcherManager刷新在实际应用中，会并行的进行搜索、建索引、打开新 reader、关闭老 reader，操作比较复杂还有线程安全问题。为了简化使用流程，Lucene 提供了 SearcherManager extends ReferenceManager 管理 IndexReader 的重建和关闭，保证了线程安全，封装了 IndexSearcher 的生成。 SearcherManager的主要职责如下，主要提供如下三个接口 acquire：获取当前已打开的最新 IndexSearcher，同时将对应的 IndexReader 引用计数加1 release：释放之前通过 acquire 方法获取的引用，本质调的是 IndexSearcher.getIndexReader().decRef();，当一个 IndexReader 的引用计数为0时，会将之关闭同时释放其持有的资源 maybeRefresh：尝试打开新的 IndexReader，本质调用的是 DirectoryReader.openIfChanged() 方法，如果多个线程同时调用该方法，那么只有第一个线程会去尝试刷新，后来的或者说随后的线程看到有其它线程在处理刷新，那么会立即返回；注意这意味着，如果有一个线程在处理刷新操作，那么后续的线程会立即返回而不是等待它刷新完成，所以后续调用该方法的线程可能是已经刷新了的或者是没有任何改变的（没有任何改变意味着通过 acquire 获取的 IndexSearcher 无法搜索到最新的数据） maybeRefreshBlocking：功能同 maybeRefresh()，但是不像 maybeRefresh()，如果有一个线程正在刷新，后续的线程将阻塞，直到前面的线程刷新完成，然后后续的线程再继续刷新；这非常有用对于想要保证下一次 acquire() 的调用将返回一个刷新的实例，否则考虑使用 maybeRefresh() SearcherManager 的一个注意点是，如果调用了 acquire，那么一定要调用 release，在该方法内部会通过 IndexSearcher 获取IndexReader，然后将 IndexReader 的引用计数减1，当这个引用计数为0的时候，这个 IndexReader 就会被关闭，从而可以释放资源。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntPoint;import org.apache.lucene.document.TextField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.*;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;public class NearRealTimeSearchDemo &#123; public static void main(String[] args) throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); Document document = new Document(); document.add(new TextField("title", "Doc1", Field.Store.YES)); document.add(new IntPoint("ID", 1)); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); indexWriter.addDocument(document); indexWriter.commit(); SearcherManager searcherManager = new SearcherManager(ramDirectory, null); document = new Document(); document.add(new TextField("title", "Doc2", Field.Store.YES)); document.add(new IntPoint("ID", 2)); indexWriter.addDocument(document); //即使commit，搜索依然不可见，需要重新打开reader indexWriter.commit(); //只能看到Doc1 IndexSearcher indexSearcher = searcherManager.acquire(); int count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); //最好放到finally语句块中 searcherManager.release(indexSearcher); if (!searcherManager.isSearcherCurrent()) &#123; //说明有新数据，需要刷新 searcherManager.maybeRefresh(); &#125; indexSearcher = searcherManager.acquire(); count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); searcherManager.release(indexSearcher); &#125;&#125; 输出结果12345Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;=================================Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;Document&lt;stored,indexed,tokenized&lt;title:Doc2&gt;&gt;================================= 使用定时线程执行刷新12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntPoint;import org.apache.lucene.document.TextField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.*;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class NearRealTimeSearchDemo &#123; private static SearcherManager searcherManager; public static void main(String[] args) throws IOException &#123; ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor(); executorService.scheduleWithFixedDelay(new RefreshThread(), 1, 1, TimeUnit.SECONDS); RAMDirectory ramDirectory = new RAMDirectory(); Document document = new Document(); document.add(new TextField("title", "Doc1", Field.Store.YES)); document.add(new IntPoint("ID", 1)); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); indexWriter.addDocument(document); indexWriter.commit(); searcherManager = new SearcherManager(ramDirectory, null); document = new Document(); document.add(new TextField("title", "Doc2", Field.Store.YES)); document.add(new IntPoint("ID", 2)); indexWriter.addDocument(document); //即使commit，搜索依然不可见，需要重新打开reader indexWriter.commit(); //只能看到Doc1 IndexSearcher indexSearcher = searcherManager.acquire(); int count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); //最好放到finally语句块中 searcherManager.release(indexSearcher); //休息2s中，定时线程应该已经刷新了 try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; indexSearcher = searcherManager.acquire(); count = indexSearcher.count(new MatchAllDocsQuery()); if (count &gt; 0) &#123; TopDocs topDocs = indexSearcher.search(new MatchAllDocsQuery(), count); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125; &#125; System.out.println("================================="); searcherManager.release(indexSearcher); &#125; static class RefreshThread implements Runnable &#123; @Override public void run() &#123; try &#123; searcherManager.maybeRefresh(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 输出结果 12345Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;=================================Document&lt;stored,indexed,tokenized&lt;title:Doc1&gt;&gt;Document&lt;stored,indexed,tokenized&lt;title:Doc2&gt;&gt;================================= SearcherLifetimeManager管理Search Session管理 IndexSearcher 的生命周期主要是用在分页的时候，例如如果不进行管理的话，那么在用户提交了更改到索引中，正好此时后台线程开始刷新 IndexSearcher，如果提交的内容和用户的搜索不相关的话还好，如果提交的内容是和用户搜索结果相关的，那么新增加的内容可能会影响结果排序，从而在分页的时候可能会使用户点击翻页却看到相同的内容。 为了保证一个良好的用户体验，需要对用户在一个搜索会话期间分页的时候使用同样一个 IndexSearcher，这样在分页期间，可以保证搜索结果的排序是稳定不变的，从而用户也不会再次看到同样的内容。SearcherLifetimeManager 主要有如下几个方法 1234public IndexSearcher acquire(long version) //根据version获取之前记录的IndexSearcher，引用计数加1public void release(IndexSearcher s) //释放之前通过acquire获取的IndexSearcher，引用计数减1public long record(IndexSearcher searcher) //把当前正在使用的IndexSearcher放入内部的ConcurrentHashMap，并返回一个token，之后可以通过该token再次获取该IndexSearcherpublic synchronized void prune(Pruner pruner) //按指示的age关闭IndexSearcher，并将它们从ConcurrentHashMap中移除 使用示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntPoint;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.SearcherLifetimeManager;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;public class SearcherLifetimeManagerDemo &#123; public static void main(String[] args) throws IOException &#123; SearcherLifetimeManager searcherLifetimeManager = new SearcherLifetimeManager(); RAMDirectory ramDirectory = new RAMDirectory(); Document document = new Document(); document.add(new TextField("title", "Doc1", Field.Store.YES)); document.add(new IntPoint("ID", 1)); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); indexWriter.addDocument(document); indexWriter.commit(); document = new Document(); document.add(new TextField("title", "Doc2", Field.Store.YES)); document.add(new IntPoint("ID", 2)); indexWriter.addDocument(document); indexWriter.commit(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); //记录当前的searcher，保存token，当有后续的搜索请求到来，例如用户翻页，那么用这个token去获取对应的那个searcher long record = searcherLifetimeManager.record(indexSearcher); indexSearcher = searcherLifetimeManager.acquire(record); if (indexSearcher != null) &#123; // Searcher is still here try &#123; // do searching... &#125; finally &#123; searcherLifetimeManager.release(indexSearcher); // Do not use searcher after this! indexSearcher = null; &#125; &#125; else &#123; // Searcher was pruned -- notify user session timed out, or, pull fresh searcher again &#125; //由于保留许多的searcher是非常耗系统资源的，包括打开发files和RAM，所以最好在一个单独的线程中，定期的重新打开searcher和定时的去清理旧的searcher //丢弃所有比指定的时间都老的searcher searcherLifetimeManager.prune(new SearcherLifetimeManager.PruneByAge(600.0)); &#125;&#125; Notes：本文代码基于 Lucene 7.0.0 开发]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 分组统计详解]]></title>
    <url>%2F2017%2F11%2F15%2Flucene-group-statistics-detailed%2F</url>
    <content type="text"><![CDATA[抛出问题在 RDBMS 中，我们可以使用 GROUP BY 来对检索的数据进行分组，同样地，想要在 Lucene 中实现分组要如何做呢？首先思考如下几个问题 Lucene 是如何实现分组的？ 用来分组的字段（域）或者说 Field 如何添加？ 组的大小如何设置？ 组内大小如何设置？ 如何实现组的分页？ 如果结果集超过了组内大小，可以通过分页解决，那么如果结果集超过了组大小的上限，如何解决？ 如何实现单类别分组，即类似SQL中的 GROUP BY A 如何实现多类别分组，即类似SQL中的 GROUP BY A, B 从 SQL 的 GROUP BY 说起如果分组后面只有一个字段，如 GROUP BY A 意思是将所有具有相同A字段值的记录放到一个分组里。那么如果是GROUP BY A, B呢？其意思是将所有具有相同A字段值和B字段值的记录放到一个分组里，在这里A和B之间是逻辑与的关系。 通常的，如果在SQL中，我们仅用 GROUP BY 语句而不加 WHERE 条件的话，那么相当于在全部数据中进行分组，对应于 Lucene 中相当于使用 GROUP 加 new MatchAllDocsQuery() 的功能。 而如果在SQL中，我们不仅用 GROUP BY 还有 WHERE 条件语句，那么相当于在满足 WHERE 条件的记录中进行分组，这种 WHERE 条件在 Lucene 中可以通过构造各种不同的 Query 进行过滤，然后在符合条件的结果中分组。 Lucene 分组有关Lucene分组问题，需要有一系列输入参数，官方Doc在此，核心点如下 groupField：用来分组的域，在 Lucene 中，这个域只能设置一个，不像 SQL 中可以根据多个列分组。没有该域的文档将被分到一个单独的组里面 groupSort：组间排序方式，用来指定如何对不同的分组进行排序，而不是组内的文档排序，默认值是Sort.RELEVANCE topNGroups：保留多少组，例如10只取前十个分组 groupOffset：指定组偏移量，比如当topNGroups的值是10的时候，groupOffset为3，则意思是返回7个分组，跳过前面3个，在分页时候很有用 withinGroupSort：组内排序方式，默认值是Sort.RELEVANCE，注意和groupSort的区别，不要求和groupSort使用一样的排序方式 maxDocsPerGroup：表示一个组内最多保留多少个文档 withinGroupOffset：每组显示的文档的偏移量 分组通常有两个阶段，第一阶段用FirstPassGroupingCollector收集不同的分组，第二阶段用SecondPassGroupingCollector收集这些分组内的文档，如果分组很耗时，建议用CachingCollector类，可以缓存 hits 并在第二阶段快速返回。这种方式让你相当于只运行了一次 query，但是付出的代价是用 RAM 持有所有的 hits。返回的结果集是TopGroups的实例。 Groups是由GroupSelector（抽象类）的实现来定义的，目前支持两种实现方式 TermGroupSelector 基于 SortedDocValues 域进行分组 ValueSourceGroupSelector 基于 ValueSource 值进行分组 通常不建议直接使用 FirstPassGroupingCollector 和 SecondPassGroupingCollector 来进行分组操作，因为Lucene提供了一个非常简便的封装类 GroupingSearch，目前分组操作还不支持 Sharding。 网上有许多讲解 Lucene 分组的文章，但是讲的都非常浅显，一般都是取 Top N 个分组，这个 N 是一个确定的值，试问如果我要对全部的结果集进行分组统计，而分组数量超过 Top N 的话，那么这种方式统计的结果显然是不准确的，因为它并没有统计全部的数据。还有的是直接把 maxDoc() 函数的值作为 groupLimit 的值，然后对某个分组内的全部文档进行迭代，无法实现组内分页的问题。 所以本文就针对这个问题，不仅解决了组内分页的问题，还解决了组间分页的问题，可以迭代完全的结果集。 另外一个需要注意的问题就是 maxDoc() 可能返回的是 Integer 型的上限，而将其直接作为 groupLimit 传入的话，是会报错的，错误如下 组内大小和组间大小如果设置为Integer.MAX_VALUE报 Exception in thread “main” java.lang.NegativeArraySizeException 组内大小和组间大小如果设置为Integer.MAX_VALUE-1报 Exception in thread “main” java.lang.IllegalArgumentException: maxSize must be &lt;= 2147483630; got: 2147483646 完整示例如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.document.*;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.store.Directory;import org.apache.lucene.store.RAMDirectory;import org.apache.lucene.util.BytesRef;import java.io.IOException;/** * &lt;p&gt; * Created by wangxu on 2017/11/14 16:41. * &lt;/p&gt; * &lt;p&gt; * Description: 基于 Lucene 7.0.0 * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class IndexHelper &#123; private Document document; private Directory directory; private IndexWriter indexWriter; public Directory getDirectory() &#123; directory = (directory == null) ? new RAMDirectory() : directory; return directory; &#125; private IndexWriterConfig getConfig() &#123; return new IndexWriterConfig(new WhitespaceAnalyzer()); &#125; private IndexWriter getIndexWriter() &#123; try &#123; return new IndexWriter(getDirectory(), getConfig()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; public IndexSearcher getIndexSearcher() throws IOException &#123; return new IndexSearcher(DirectoryReader.open(getDirectory())); &#125; public void createIndexForGroup(int ID, String author, String content) &#123; indexWriter = getIndexWriter(); document = new Document(); //IntPoint默认是不存储的 document.add(new IntPoint(&quot;ID&quot;, ID)); //如果想要在搜索结果中获取ID的值，需要加上下面语句 document.add(new StoredField(&quot;ID&quot;, ID)); document.add(new StringField(&quot;author&quot;, author, Field.Store.YES)); //需要使用特定的field存储分组，需要排序及分组的话，要加上下面语句，注意默认SortedDocValuesField也是不存储的 document.add(new SortedDocValuesField(&quot;author&quot;, new BytesRef(author))); document.add(new StringField(&quot;content&quot;, content, Field.Store.YES)); try &#123; indexWriter.addDocument(document); indexWriter.commit(); indexWriter.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import org.apache.lucene.index.Term;import org.apache.lucene.search.*;import org.apache.lucene.search.grouping.GroupDocs;import org.apache.lucene.search.grouping.GroupingSearch;import org.apache.lucene.search.grouping.TopGroups;import org.apache.lucene.util.BytesRef;import java.io.IOException;/** * &lt;p&gt; * Created by wangxu on 2017/11/14 16:21. * &lt;/p&gt; * &lt;p&gt; * Description: 基于 Lucene 7.0.0 开发 * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class GroupingDemo &#123; public static void main(String[] args) throws Exception &#123; IndexHelper indexHelper = new IndexHelper(); indexHelper.createIndexForGroup(1, "Java", "一周精通Java"); indexHelper.createIndexForGroup(2, "Java", "一周精通MyBatis"); indexHelper.createIndexForGroup(3, "Java", "一周精通Struts"); indexHelper.createIndexForGroup(4, "Java", "一周精通Spring"); indexHelper.createIndexForGroup(5, "Java", "一周精通Spring Cloud"); indexHelper.createIndexForGroup(6, "Java", "一周精通Hibernate"); indexHelper.createIndexForGroup(7, "Java", "一周精通JVM"); indexHelper.createIndexForGroup(8, "C", "一周精通C"); indexHelper.createIndexForGroup(9, "C", "C语言详解"); indexHelper.createIndexForGroup(10, "C", "C语言调优"); indexHelper.createIndexForGroup(11, "C++", "一周精通C++"); indexHelper.createIndexForGroup(12, "C++", "C++语言详解"); indexHelper.createIndexForGroup(13, "C++", "C++语言调优"); IndexSearcher indexSearcher = indexHelper.getIndexSearcher(); GroupingDemo groupingDemo = new GroupingDemo(); //把所有的文档都查出来，由添加的数据可以知道，一共有三组，Java组有7个文档，C和C++组分别都有3个文档 //当然了如果做全匹配的话，还可以用new MatchAllDocsQuery() BooleanQuery query = new BooleanQuery.Builder().add(new TermQuery(new Term("author", "Java")), BooleanClause.Occur.SHOULD).add(new TermQuery(new Term ("author", "C")), BooleanClause.Occur.SHOULD).add(new TermQuery(new Term("author", "C++")), BooleanClause.Occur.SHOULD).build(); //控制每次返回几组 int groupLimit = 2; //控制每一页的组内文档数 int groupDocsLimit = 2; //控制组的偏移 int groupOffset = 0; //为了排除干扰因素，全部使用默认的排序方式，当然你还可以使用自己喜欢的排序方式 //初始值为命中的所有文档数，即最坏情况下，一个文档分成一组，那么文档数就是分组的总数 int totalGroupCount = indexSearcher.count(query); TopGroups&lt;BytesRef&gt; topGroups; System.out.println("#### 组的分页大小为：" + groupLimit); System.out.println("#### 组内分页大小为：" + groupDocsLimit); while (groupOffset &lt; totalGroupCount) &#123;//说明还有不同的分组 //控制组内偏移，每次开始遍历一个新的分组时候，需要将其归零 int groupDocsOffset = 0; System.out.println("#### 开始组的分页"); topGroups = groupingDemo.group(indexSearcher, query, "author", groupDocsOffset, groupDocsLimit, groupOffset, groupLimit); //具体搜了一次之后，就知道到底有多少组了，更新totalGroupCount为正确的值 totalGroupCount = topGroups.totalGroupCount; GroupDocs&lt;BytesRef&gt;[] groups = topGroups.groups; //开始对组进行遍历 for (int i = 0; i &lt; groups.length; i++) &#123; long totalHits = iterGroupDocs(indexSearcher, groups[i]);//获得这个组内一共多少doc //处理完一次分页，groupDocsOffset要更新 groupDocsOffset += groupDocsLimit; //如果组内还有数据，即模拟组内分页的情况，那么应该继续遍历组内剩下的doc while (groupDocsOffset &lt; totalHits) &#123; topGroups = groupingDemo.group(indexSearcher, query, "author", groupDocsOffset, groupDocsLimit, groupOffset, groupLimit); //这里面的组一定要和外层for循环正在处理的组保持一致，其实这里面浪费了搜索数据，为什么？ //因为Lucene是对多个组同时进行组内向后翻页，而我只是一个组一个组的处理，其它不处理的组相当于是浪费的 //所以从这种角度来说，设置groupLimit为1比较合理，即每次处理一个组，而每次只将一个组的组内文档向后翻页 GroupDocs&lt;BytesRef&gt; group = topGroups.groups[i]; totalHits = iterGroupDocs(indexSearcher, group); //此时需要更新组内偏移量 groupDocsOffset += groupDocsLimit; &#125; //至此，一个组内的doc全部遍历完毕，开始下一组 groupDocsOffset = 0; &#125; groupOffset += groupLimit; System.out.println("#### 结束组的分页"); &#125; &#125; private static long iterGroupDocs(IndexSearcher indexSearcher, GroupDocs&lt;BytesRef&gt; groupDocs) throws IOException &#123; long totalHits = groupDocs.totalHits; System.out.println("\t#### 开始组内分页"); System.out.println("\t分组名称：" + groupDocs.groupValue.utf8ToString()); ScoreDoc[] scoreDocs = groupDocs.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("\t\t组内记录：" + indexSearcher.doc(scoreDoc.doc)); &#125; System.out.println("\t#### 结束组内分页"); return totalHits; &#125; public TopGroups&lt;BytesRef&gt; group(IndexSearcher indexSearcher, Query query, String groupField, int groupDocsOffset, int groupDocsLimit, int groupOffset, int groupLimit) throws Exception &#123; return group(indexSearcher, query, Sort.RELEVANCE, Sort.RELEVANCE, groupField, groupDocsOffset, groupDocsLimit, groupOffset, groupLimit); &#125; public TopGroups&lt;BytesRef&gt; group(IndexSearcher indexSearcher, Query query, Sort groupSort, Sort withinGroupSort, String groupField, int groupDocsOffset, int groupDocsLimit, int groupOffset, int groupLimit) throws Exception &#123; //实例化GroupingSearch实例，传入分组域 GroupingSearch groupingSearch = new GroupingSearch(groupField); //设置组间排序方式 groupingSearch.setGroupSort(groupSort); //设置组内排序方式 groupingSearch.setSortWithinGroup(withinGroupSort); //是否要填充每个返回的group和groups docs的排序field groupingSearch.setFillSortFields(true); //设置用来缓存第二阶段搜索的最大内存，单位MB，第二个参数表示是否缓存评分 groupingSearch.setCachingInMB(64.0, true); //是否计算符合查询条件的所有组 groupingSearch.setAllGroups(true); groupingSearch.setAllGroupHeads(true); //设置一个分组内的上限 groupingSearch.setGroupDocsLimit(groupDocsLimit); //设置一个分组内的偏移 groupingSearch.setGroupDocsOffset(groupDocsOffset); TopGroups&lt;BytesRef&gt; result = groupingSearch.search(indexSearcher, query, groupOffset, groupLimit); return result; &#125;&#125; 例如组的分页大小是2，组内分页大小是2，结果如下：12345678910111213141516171819202122232425262728293031323334353637383940414243#### 组的分页大小为：2#### 组内分页大小为：2#### 开始组的分页 #### 开始组内分页 分组名称：C 组内记录：Document&lt;stored&lt;ID:8&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通C&gt;&gt; 组内记录：Document&lt;stored&lt;ID:9&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:C语言详解&gt;&gt; #### 结束组内分页 #### 开始组内分页 分组名称：C 组内记录：Document&lt;stored&lt;ID:10&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:C语言调优&gt;&gt; #### 结束组内分页 #### 开始组内分页 分组名称：C++ 组内记录：Document&lt;stored&lt;ID:11&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C++&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通C++&gt;&gt; 组内记录：Document&lt;stored&lt;ID:12&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C++&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:C++语言详解&gt;&gt; #### 结束组内分页 #### 开始组内分页 分组名称：C++ 组内记录：Document&lt;stored&lt;ID:13&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C++&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:C++语言调优&gt;&gt; #### 结束组内分页#### 结束组的分页#### 开始组的分页 #### 开始组内分页 分组名称：Java 组内记录：Document&lt;stored&lt;ID:5&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Spring Cloud&gt;&gt; 组内记录：Document&lt;stored&lt;ID:6&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Hibernate&gt;&gt; #### 结束组内分页 #### 开始组内分页 分组名称：Java 组内记录：Document&lt;stored&lt;ID:2&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通MyBatis&gt;&gt; 组内记录：Document&lt;stored&lt;ID:3&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Struts&gt;&gt; #### 结束组内分页 #### 开始组内分页 分组名称：Java 组内记录：Document&lt;stored&lt;ID:4&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Spring&gt;&gt; 组内记录：Document&lt;stored&lt;ID:1&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Java&gt;&gt; #### 结束组内分页 #### 开始组内分页 分组名称：Java 组内记录：Document&lt;stored&lt;ID:7&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通JVM&gt;&gt; #### 结束组内分页#### 结束组的分页 例如组的分页大小是1，组内分页大小是3，结果如下123456789101112131415161718192021222324252627282930313233343536#### 组的分页大小为：1#### 组内分页大小为：3#### 开始组的分页 #### 开始组内分页 分组名称：C 组内记录：Document&lt;stored&lt;ID:8&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通C&gt;&gt; 组内记录：Document&lt;stored&lt;ID:9&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:C语言详解&gt;&gt; 组内记录：Document&lt;stored&lt;ID:10&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:C语言调优&gt;&gt; #### 结束组内分页#### 结束组的分页#### 开始组的分页 #### 开始组内分页 分组名称：C++ 组内记录：Document&lt;stored&lt;ID:11&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C++&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通C++&gt;&gt; 组内记录：Document&lt;stored&lt;ID:12&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C++&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:C++语言详解&gt;&gt; 组内记录：Document&lt;stored&lt;ID:13&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:C++&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:C++语言调优&gt;&gt; #### 结束组内分页#### 结束组的分页#### 开始组的分页 #### 开始组内分页 分组名称：Java 组内记录：Document&lt;stored&lt;ID:5&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Spring Cloud&gt;&gt; 组内记录：Document&lt;stored&lt;ID:6&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Hibernate&gt;&gt; 组内记录：Document&lt;stored&lt;ID:2&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通MyBatis&gt;&gt; #### 结束组内分页 #### 开始组内分页 分组名称：Java 组内记录：Document&lt;stored&lt;ID:3&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Struts&gt;&gt; 组内记录：Document&lt;stored&lt;ID:4&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Spring&gt;&gt; 组内记录：Document&lt;stored&lt;ID:1&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通Java&gt;&gt; #### 结束组内分页 #### 开始组内分页 分组名称：Java 组内记录：Document&lt;stored&lt;ID:7&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;author:Java&gt; stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;content:一周精通JVM&gt;&gt; #### 结束组内分页#### 结束组的分页]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 实现任意词搜索命中并返回位置信息]]></title>
    <url>%2F2017%2F10%2F13%2Flucene-implements-any-word-search-and-returns-location-information%2F</url>
    <content type="text"><![CDATA[背景如果把这个标题拆分成两个来讲，那么每一个都很好解决，下文会进行详述，而如果把这两者看做是与条件并加上其它限制，则实现起来比较困难，本文就是要探讨在需求繁多的情况下，如何优雅地实现。比如需求如下 保留标点符号，否则去掉标点的话，在标点两边的词可能会匹配上，比如“你好，小甜甜”，去掉标点切分是『你|好|小|甜|甜』，那么『好小』有可能会命中，而如果切分成『你|好|，|小|甜|甜』，则『好小』无法命中 只要包含搜索词，要求对任意搜索词均可命中 比如“我爱你中国”，不同的分词工具会切分出不同的结果 『我|爱|你|中国』或者『我爱你|中国』或者『我|爱|你|中|国』等，那么要求搜索“我爱”或者“爱你”或者“你中”等都要命中 需要获取命中词的positions信息 还是以“我爱你中国”为例，如果搜“你中”，那么需要返回结果命中，并给出positions信息，例如start=2表示“你中”在原文本中是从第2个位置开始 可以设置slop 什么是slop，简单来说slop是指两个项的位置之间允许的最大间隔距离 为什么要设置slop呢？比如小黄文为了防止敏感词被屏蔽，会在敏感词中间加上干扰词，例如性%=$虐待，那么直接搜性虐待无法命中 只要设置slop为3，即相当于搜性[***]虐待，这里面的[***]就代表slop为3，可以匹配任意三个字 不要存储Field的值 如果可以存储的话，可以通过Document获取原文本，再用TokenStream分析该文本，使用QueryScore初始化TokenStream的分析结果，遍历每个token根据TokenScore的得分判断是否命中，若命中则输出位置信息或者起始偏移量即可 但是存储Field的值是需要占用硬盘空间的，当需要索引海量的文本的时候，会导致索引体积非常大，搜索性能变差 当然还可以通过将索引拆分成多份存储实现降低索引体积的目的，这也是一个方法，不过治标不治本 不要存储TermVectors 同样地如果可以存储的话，可以通过TermVectors获取Terms再遍历TermsEnum，获取PostingsEnum得到positions信息，缺点在于只能实现单个字（Term）的搜索匹配 但是存储TermVectors同样占用硬盘空间，为了缩小索引体积，不要存储 实现与逻辑，比如搜索“你中 &amp; 我爱”表示两个词都要命中 实现或逻辑，比如搜索“你中 | 我爱”表示两个词至少要有一个命中 如果想要实现百分之百的任意词搜索命中，那么只能按字切分，因为没有任何分词工具能够保证切出来的词与搜索词是一致的。在上面说到为了达到匹配干扰词的目的，需要设置slop，但是会有一定的误判率，本来不该匹配的在设置slop之后也匹配上了。除了设置slop这种方式，还有一种方法，就是在索引阶段只保留汉字，其它的标点符号和干扰符号统统去掉，当然这也存在一定的误判率，而且获取的positions信息已经不是原文本中正确的positions信息了。两种方式的权衡与取舍可以根据业务需求而定，这两种方式都不会漏判，但是均会有误判。 简单需求的实现如果并不要求满足上面所有的需求，而仅仅满足其中任何一个，那么实现起来都是非常简单的，以下代码均基于Lucene 5.5.0实现，示例如下。 仅要求任意搜索词命中1234567891011121314151617181920@org.junit.Testpublic void testAnyMatch() throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); Document document = new Document(); document.add(new TextField("content", "我爱你中国", Field.Store.NO)); indexWriter.addDocument(document); indexWriter.commit(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(indexWriter)); PhraseQuery phraseQuery = new PhraseQuery.Builder().add(new Term("content", "你")).add(new Term("content", "中")).setSlop(0).build(); TopDocs search = indexSearcher.search(phraseQuery, Integer.MAX_VALUE); System.out.println(search.totalHits); //OR search like this MultiPhraseQuery multiPhraseQuery = new MultiPhraseQuery(); Term first = new Term("content", "你"); Term second = new Term("content", "中"); multiPhraseQuery.add(new Term[]&#123;first, second&#125;); search = indexSearcher.search(multiPhraseQuery, Integer.MAX_VALUE); System.out.println(search.totalHits);&#125; 可以存储Field的值12345678910111213141516171819202122232425262728293031323334353637383940@org.junit.Testpublic void testStoreFieldMatch() throws IOException, InvalidTokenOffsetsException &#123; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); Document document = new Document(); document.add(new TextField("content", "我爱你中国", Field.Store.YES)); indexWriter.addDocument(document); indexWriter.commit(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(indexWriter)); PhraseQuery phraseQuery = new PhraseQuery.Builder().add(new Term("content", "你")).add(new Term("content", "中")).setSlop(0).build(); TopDocs search = indexSearcher.search(phraseQuery, Integer.MAX_VALUE); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; String content = indexSearcher.doc(scoreDoc.doc).get("content"); TokenStream contentStream = new StandardAnalyzer().tokenStream("content", content); CharTermAttribute charTermAttribute = contentStream.addAttribute(CharTermAttribute.class); OffsetAttribute offsetAttribute = contentStream.addAttribute(OffsetAttribute.class); QueryScorer queryScorer = new QueryScorer(phraseQuery); queryScorer.setMaxDocCharsToAnalyze(Integer.MAX_VALUE); TokenStream init = queryScorer.init(contentStream); if (init != null) &#123; contentStream = init; &#125; contentStream.reset(); queryScorer.startFragment(null); int startOffset, endOffset; for (boolean next = contentStream.incrementToken(); next &amp;&amp; (offsetAttribute.startOffset() &lt; Integer.MAX_VALUE); next = contentStream.incrementToken()) &#123; startOffset = offsetAttribute.startOffset(); endOffset = offsetAttribute.endOffset(); if (startOffset &gt; content.length() || endOffset &gt; content.length()) &#123; throw new InvalidTokenOffsetsException("Token " + charTermAttribute.toString() + " exceeds length of provided text sized " + content.length()); &#125; float res = queryScorer.getTokenScore(); if (res &gt; Float.valueOf(0) &amp;&amp; startOffset &lt;= endOffset) &#123; System.out.println("hits: " + content.substring(startOffset, endOffset) + ", start: " + startOffset); &#125; &#125; contentStream.close(); &#125;&#125; 可以存储TermVectors的值123456789101112131415161718192021222324252627282930313233@org.junit.Testpublic void testTermVectorsMatch() throws IOException, InvalidTokenOffsetsException &#123; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); Document document = new Document(); FieldType fieldType = new FieldType(); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); fieldType.setStoreTermVectorPositions(true); fieldType.setStoreTermVectors(true); document.add(new Field("content", "我爱你中国", fieldType)); indexWriter.addDocument(document); indexWriter.commit(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(indexWriter)); Term searchTerm = new Term("content", "中"); PhraseQuery phraseQuery = new PhraseQuery.Builder().add(searchTerm).setSlop(0).build(); TopDocs search = indexSearcher.search(phraseQuery, Integer.MAX_VALUE); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; Terms content = indexSearcher.getIndexReader().getTermVector(scoreDoc.doc, "content"); TermsEnum iterator = content.iterator(); BytesRef bytesRef; while ((bytesRef = iterator.next()) != null) &#123; PostingsEnum postings = iterator.postings(null, PostingsEnum.ALL); if (postings.nextDoc() != Spans.NO_MORE_DOCS) &#123; for (int i = 0; i &lt; postings.freq(); i++) &#123; if (searchTerm.text().equals(bytesRef.utf8ToString())) &#123; System.out.println("hits: " + bytesRef.utf8ToString() + ", start: " + postings.nextPosition()); &#125; &#125; &#125; &#125; &#125;&#125; 复杂需求的实现实现某一个简单的需求就不再举例了，下面要讲解如何实现复杂的需求，也就是说，要同时满足上面的需求列表，而不仅仅是只满足其中的某一条。首先需要解决的就是分词之后保留标点符号的问题，在Lucene中，我并没有找到原生的支持保留标点符号的Analyzer，于是只能自己造轮子了。 保留标点符号的分词器1234567891011121314151617181920212223import lombok.extern.log4j.Log4j2;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.Tokenizer;import org.apache.lucene.analysis.core.LowerCaseFilter;import org.apache.lucene.analysis.pattern.PatternTokenizer;import java.util.regex.Pattern;@Log4j2public class ReservePunctuationAnalyzer extends Analyzer &#123; public ReservePunctuationAnalyzer() &#123; &#125; @Override protected TokenStreamComponents createComponents(String fieldName) &#123; final Tokenizer source; source = new PatternTokenizer(Pattern.compile(""), -1); TokenStream result = new LowerCaseFilter(source); return new TokenStreamComponents(source, result); &#125;&#125; 分词测试如下123456789101112@Testpublic void test() throws IOException &#123; String input = "你好，小甜甜。"; TokenStream test = new ReservePunctuationAnalyzer().tokenStream("test", input); CharTermAttribute charTermAttribute = test.addAttribute(CharTermAttribute.class); OffsetAttribute offsetAttribute = test.addAttribute(OffsetAttribute.class); test.reset(); while (test.incrementToken()) &#123; System.out.println("token:[" + charTermAttribute + "], offset:[" + offsetAttribute.startOffset() + "]"); &#125; test.close();&#125; 分词结果输出如下 token:[你], offset:[0]token:[好], offset:[1]token:[，], offset:[2]token:[小], offset:[3]token:[甜], offset:[4]token:[甜], offset:[5]token:[。], offset:[6] 任意词搜索命中并返回positions信息下面再来解决在不存储Field、不存储TermVectors的情况下，如何实现任意词搜索命中并返回positions信息，同时还可以设置slop的值。要实现这些功能就需要用到SpanQuery及其一系列的子类，先来看一张继承关系图，这些都是即将要使用到的类。 SpanQuery的doc注释很简单，就一句话“Base class for span-based queries”，基于跨度查询的基类。而真正具有实际作用的是其各个子类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.document.LongField;import org.apache.lucene.index.*;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.spans.SpanNearQuery;import org.apache.lucene.search.spans.SpanTermQuery;import org.apache.lucene.search.spans.SpanWeight;import org.apache.lucene.search.spans.Spans;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;import java.util.List;import static org.apache.lucene.search.spans.SpanNearQuery.newOrderedNearQuery;/** * &lt;p&gt; * Created by wangxu on 2017/10/13 14:29. * &lt;/p&gt; * &lt;p&gt; * Description: TODO * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class SpanNearQueryDemo &#123; @org.junit.Test public void test() throws IOException &#123; String input = "现有的中文分词算法可分为三大类：基于字符串匹配的类基分词方法、基于理解的分词方法和基于统计的分词方法。"; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new ReservePunctuationAnalyzer()); try (IndexWriter indexWriter = new IndexWriter(ramDirectory, indexWriterConfig)) &#123; Document document = new Document(); FieldType fieldType = new FieldType(); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); Field field = new Field("title", input, fieldType); LongField IDX = new LongField("IDX", 1, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); input = "计算机算法是很难很复杂滴"; document = new Document(); field = new Field("title", input, fieldType); IDX = new LongField("IDX", 2, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); input = "计算机算法可以大幅度提升程序性能"; document = new Document(); field = new Field("title", input, fieldType); IDX = new LongField("IDX", 3, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); indexWriter.commit(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); SpanTermQuery first = new SpanTermQuery(new Term("title", "类")); SpanTermQuery second = new SpanTermQuery(new Term("title", "基")); SpanNearQuery spanNearQuery = newOrderedNearQuery("title").addClause(first).addClause(second).build(); SpanWeight weight = spanNearQuery.createWeight(indexSearcher, true); List&lt;LeafReaderContext&gt; leaves = indexSearcher.getIndexReader().getContext().leaves(); for (LeafReaderContext leaf : leaves) &#123; Spans spans = weight.getSpans(leaf, SpanWeight.Postings.POSITIONS); while (spans.nextDoc() != Spans.NO_MORE_DOCS) &#123; Document doc = leaf.reader().document(spans.docID()); while (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) &#123; System.out.println("doc id = " + spans.docID() + ", doc IDX= " + doc.get("IDX") + ", start position = " + spans.startPosition() + ", end " + "position = " + spans.endPosition()); &#125; &#125; &#125; //================================================================ // 输出结果是 // doc id = 0, doc IDX= 1, start position = 24, end position = 26 //================================================================ System.out.println(); //修改slop，设置1，默认是0 spanNearQuery = newOrderedNearQuery("title").addClause(first).addClause(second).setSlop(1).build(); weight = spanNearQuery.createWeight(indexSearcher, true); leaves = indexSearcher.getIndexReader().getContext().leaves(); for (LeafReaderContext leaf : leaves) &#123; Spans spans = weight.getSpans(leaf, SpanWeight.Postings.POSITIONS); while (spans.nextDoc() != spans.NO_MORE_DOCS) &#123; Document doc = leaf.reader().document(spans.docID()); while (spans.nextStartPosition() != spans.NO_MORE_POSITIONS) &#123; System.out.println("doc id = " + spans.docID() + ", doc IDX= " + doc.get("IDX") + ", start position = " + spans.startPosition() + ", end " + "position = " + spans.endPosition()); &#125; &#125; &#125; //================================================================ // 输出结果是 // doc id = 0, doc IDX= 1, start position = 14, end position = 17 // doc id = 0, doc IDX = 1, start position = 24, end position = 26 //================================================================ &#125; &#125;&#125; 实现逻辑与查询通过上面的图，想必你也知道，Lucene官方并不对SpanAndQuery提供支持，在Lucene的官方讨论组中，有人发起过支持SpanAndQuery的issue，但是一直没有获得官方的回应。不过已经有商业公司实现了这种搜索技术，公司名称是SearchTechnologies，API参见SpanAndQuery，但是并不开源（So Sad），我没有找到其实现的具体源码，如果你知道的话，烦请告知我一下。 既然官方不予支持，那就只能自己造轮子了，逻辑上来讲，也不复杂，有两种方式可以实现。 第一种方式，从词的角度，例如『爱你』和『你中』两个搜索词实现逻辑与，那么只要分别地把每一个搜索词都单独搜一下，最后在命中结果中取交集，就可以实现逻辑与的功能，代码写起来也很简单，在此不予示例。 第二种方式，从Term的角度，例如『爱你』如果按字切分，那么能够切成两个Term，分别是『爱』和『你』，这时候使用SpanNearQuery构造查询语句，加上一个很大的slop，但是不管slop多大，它总是有上限的，万一两个Term之间的距离超过slop，同样无法命中，所以说这种实现方式是存在漏洞的，除非你确定你的两个Term之间的距离不会超过某个具体的slop值，那么可以使用之。 注意在使用SpanNearQuery获取positions信息的时候，你不能够同时保证按字切分，又可以在两个搜索词之间设置slop值，这是因为如果用BooleanQuery去包装两个SpanNearQuery，那么将丢失positions信息。如果不按字切分，那么切出来的某个词就是一个Term，即将『爱你』和『你中』看成是两个Term，这时候是可以设置slop值的。如果按字切分，那么切成『爱|你』和『你|中』，实现逻辑与，如果设置slop值，相当于是『爱|slop值|你|slop值|你|slop值|中』，已经与逻辑与不匹配了，逻辑与的本意是『爱|slop值为0|你|slop为任意值|你|slop值为0|中』，请细细体会。 实现逻辑或查询官方已经对或逻辑提供了支持，就是SpanOrQuery，直接操练起来即可。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.document.LongField;import org.apache.lucene.index.*;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.spans.*;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;import java.util.List;import static org.apache.lucene.search.spans.SpanNearQuery.newOrderedNearQuery;/** * &lt;p&gt; * Created by wangxu on 2017/10/13 14:29. * &lt;/p&gt; * &lt;p&gt; * Description: TODO * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class SpanOrQueryDemo &#123; @org.junit.Test public void test() throws IOException &#123; String input = "现有的中文分词算法可分为三大类：基于字符串匹配的类基分词方法、基于理解的分词方法和基于统计的分词方法。"; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new ReservePunctuationAnalyzer()); try (IndexWriter indexWriter = new IndexWriter(ramDirectory, indexWriterConfig)) &#123; Document document = new Document(); FieldType fieldType = new FieldType(); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); Field field = new Field("title", input, fieldType); LongField IDX = new LongField("IDX", 1, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); input = "计算机算法是很难很复杂滴"; document = new Document(); field = new Field("title", input, fieldType); IDX = new LongField("IDX", 2, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); input = "计算机算法可以大幅度提升程序性能"; document = new Document(); field = new Field("title", input, fieldType); IDX = new LongField("IDX", 3, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); indexWriter.commit(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); SpanTermQuery first = new SpanTermQuery(new Term("title", "类")); SpanTermQuery second = new SpanTermQuery(new Term("title", "基")); SpanNearQuery spanNearQueryFirst = newOrderedNearQuery("title").addClause(first).addClause(second).build(); first = new SpanTermQuery(new Term("title", "算")); second = new SpanTermQuery(new Term("title", "法")); SpanNearQuery spanNearQuerySecond = newOrderedNearQuery("title").addClause(first).addClause(second).build(); SpanOrQuery spanOrQuery = new SpanOrQuery(spanNearQueryFirst, spanNearQuerySecond); SpanWeight weight = spanOrQuery.createWeight(indexSearcher, true); List&lt;LeafReaderContext&gt; leaves = indexSearcher.getIndexReader().getContext().leaves(); for (LeafReaderContext leaf : leaves) &#123; Spans spans = weight.getSpans(leaf, SpanWeight.Postings.POSITIONS); while (spans.nextDoc() != Spans.NO_MORE_DOCS) &#123; Document doc = leaf.reader().document(spans.docID()); while (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) &#123; System.out.println("doc id = " + spans.docID() + ", doc IDX= " + doc.get("IDX") + ", start position = " + spans.startPosition() + ", end " + "position = " + spans.endPosition()); &#125; &#125; &#125; //================================================================ // 输出结果是 // doc id = 0, doc IDX= 1, start position = 7, end position = 9 // doc id = 0, doc IDX= 1, start position = 24, end position = 26 // doc id = 1, doc IDX= 2, start position = 3, end position = 5 // doc id = 2, doc IDX= 3, start position = 3, end position = 5 //================================================================ &#125; &#125;&#125; 实现SpanAllNearQuery这是一个附加功能，因为目前还没有碰到这样的需求，但是这种查询实现起来非常有意思，所以在此简单讲解一下。这个问题的来源是有人提了个issue，请求官方支持SpanAllNearQuery，但是同样地，官方不理不睬。果然公益的就是拽啊，完全不倾听用户的需求，不像商业公司，为了赚用户的钱，只要用户有需求，就尽力实现。 那么这个需求是什么样的呢？简单表示如下a WITHIN 5 WORDS OF (b AND c)，还可以把它换一种方式理解(a WITHIN 5 WORDS OF b) AND (a WITHIN 5 WORDS OF c)，就是说我要查询，在a的前面5个或者后面5个token中出现b和c的所有结果集。要实现这个功能，需要借助于SpanNotQuery和SpanOrQuery，SpanOrQuery在实现或逻辑中已经介绍过了，那么SpanNotQuery又是什么意思呢？举例如下 SpanNotQuery(a, b, 5, 5)表示在a的前5个或者后5个token中不能出现bSpanNotQuery(a, c, 5, 5)表示在a的前5个或者后5个token中不能出现c 下面先从逻辑上先实现这个需求，要获得在a的前面或后面5个token中出现b和c，需要将其反转理解，先查询在a的前面5个或者后面5个token中不能出现b『SpanNotQuery(a, b, 5, 5)』或者在a的前面5个或者后面5个token中不能出现c的结果『SpanNotQuery(a, c, 5, 5)』，再用SpanOrQuery来组合『SpanNotQuery(a, b, 5, 5)』和『SpanNotQuery(a, c, 5, 5)』实现或逻辑，最后用SpanNotQuery排除掉SpanOrQuery的结果集，那么剩下的就是在a的前面5个或者后面5个能出现b也能出现c的结果。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntField;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.Term;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.spans.SpanNotQuery;import org.apache.lucene.search.spans.SpanOrQuery;import org.apache.lucene.search.spans.SpanTermQuery;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;/** * &lt;p&gt; * Created by wangxu on 2017/06/16 16:02. * &lt;/p&gt; * &lt;p&gt; * Description: TODO * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class SpanAllNearQueryDemo &#123; @org.junit.Test public void test() throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new WhitespaceAnalyzer())); Document document = new Document(); document.add(new TextField("key", "X b X X X X a X X X X c X", Field.Store.YES));//命中 document.add(new IntField("IDX", 1, Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("key", "X X X X X b a c X X X X X", Field.Store.YES));//命中 document.add(new IntField("IDX", 2, Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("key", "X b X X X X a a X X X X c", Field.Store.YES));//不命中，不能同时以两个a为中心，两个a必选其一 document.add(new IntField("IDX", 3, Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("key", "X b X X X X X a X X X X c", Field.Store.YES));//不命中 document.add(new IntField("IDX", 4, Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("key", "X b X X X X a X X X X X c", Field.Store.YES));//不命中 document.add(new IntField("IDX", 5, Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("key", "b X X X X X a X X X X X c", Field.Store.YES));//不命中 document.add(new IntField("IDX", 6, Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("key", "b X X X X X a X X X X X X", Field.Store.YES));//不命中 document.add(new IntField("IDX", 7, Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("key", "X X X X X X a X X X X X X", Field.Store.YES));//不命中 document.add(new IntField("IDX", 8, Field.Store.YES)); indexWriter.addDocument(document); indexWriter.commit(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(indexWriter)); SpanTermQuery a = new SpanTermQuery(new Term("key", "a")); SpanTermQuery b = new SpanTermQuery(new Term("key", "b")); SpanTermQuery c = new SpanTermQuery(new Term("key", "c")); SpanOrQuery exclude = new SpanOrQuery(new SpanNotQuery(a, b, 5, 5), new SpanNotQuery(a, c, 5, 5)); //排除在a的前5个或者后5个不能出现b也不能出现c的document，那么剩下的就是在a的前5个token或者后5个token能够出现b和c的document SpanNotQuery spanNotQuery = new SpanNotQuery(a, exclude); TopDocs search = indexSearcher.search(spanNotQuery, Integer.MAX_VALUE); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("hist IDX: " + indexSearcher.doc(scoreDoc.doc).get("IDX")); &#125; indexSearcher.getIndexReader().close(); indexWriter.close(); &#125;&#125; SpanNearQuery实现通配符查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import com.yuewen.nrzx.character.analyzer.ReservePunctuationAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FieldType;import org.apache.lucene.document.LongField;import org.apache.lucene.index.*;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.WildcardQuery;import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;import org.apache.lucene.search.spans.SpanNearQuery;import org.apache.lucene.search.spans.SpanQuery;import org.apache.lucene.search.spans.SpanTermQuery;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;public class SpanNearQueryAndWildcardQueryDemo &#123; @org.junit.Test public void test() throws IOException &#123; String input = "现有的中文分词算法可分为三大类：基于字符串匹配的类基分词方法、基于理解的分词方法和基于统计的分词方法。"; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new ReservePunctuationAnalyzer()); try (IndexWriter indexWriter = new IndexWriter(ramDirectory, indexWriterConfig)) &#123; Document document = new Document(); FieldType fieldType = new FieldType(); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); Field field = new Field("title", input, fieldType); LongField IDX = new LongField("IDX", 1, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); input = "计算机算法是很难很复杂滴"; document = new Document(); field = new Field("title", input, fieldType); IDX = new LongField("IDX", 2, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); input = "计算机算法可以大幅度提升程序性能"; document = new Document(); field = new Field("title", input, fieldType); IDX = new LongField("IDX", 3, Field.Store.YES); document.add(field); document.add(IDX); indexWriter.addDocument(document); indexWriter.commit(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); // 用?和*均可以实现SpanNearQuery的通配符查询，但是注意*在通配符查询中表示可以匹配0个或多个字符 // 但是在SpanQuery中只能匹配相当于slop=1的情形，不能匹配slop大于1的情形 SpanTermQuery first = new SpanTermQuery(new Term("title", "复")); SpanQuery wildcard = new SpanMultiTermQueryWrapper&lt;&gt;(new WildcardQuery(new Term("title", "?"))); SpanTermQuery last = new SpanTermQuery(new Term("title", "滴")); SpanNearQuery spanNearQuery = new SpanNearQuery.Builder("title", true).addClause(first).addClause(wildcard).addClause(last).build(); TopDocs search = indexSearcher.search(spanNearQuery, Integer.MAX_VALUE); System.out.println("IDX: " + indexSearcher.doc(search.scoreDocs[0].doc).get("IDX")); wildcard = new SpanMultiTermQueryWrapper&lt;&gt;(new WildcardQuery(new Term("title", "*"))); spanNearQuery = new SpanNearQuery.Builder("title", true).addClause(first).addClause(wildcard).addClause(last).build(); search = indexSearcher.search(spanNearQuery, Integer.MAX_VALUE); System.out.println("IDX: " + indexSearcher.doc(search.scoreDocs[0].doc).get("IDX")); &#125; &#125;&#125; 实验服务器 CPU 以及内存信息 $ cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 24 Intel(R) Xeon(R) CPU E5-2420 v2 @ 2.20GHz $ free -g total used free shared buffers cached Mem 62 56 6 0 0 1 -/+ buffers/cache 54 8 Swap 1 0 1 在公司内部，仅仅索引了十分之一的文档（Document数量：20023911），鉴于没有存储Field，也没有存储TermVectors，索引不算太大，简单测试了下，如果存储TermVectors的话，索引会从112GB增长到162GB，如果再存储Field的话，那么索引要超过200GB。此处的实验只是简单的单次搜索，没有测试与逻辑和或逻辑情况下的搜索情况。搜索阶段实验的结果如下所示 线程数目 搜索总次数 命中次数 搜索总耗时 平均单次耗时 搜索加构建Query耗时 平均单次耗时 索引大小 1 18820 18139 36001,698ms 1912.95ms 36011,989ms 1913.50ms 112GB 5 18820 18139 30775,283ms 1635.24ms 30785,538ms 1635.79ms 112GB 10 18820 18139 21637,515ms 1149.71ms 21647,953ms 1150.26ms 112GB 50 18820 18139 21572,506ms 1146.25ms 21583,101ms 1146.82ms 112GB 索引阶段并没有做详细完备的实验，只是简单拉取了一点数据，记录如下，仅供参考。索引2080550个Document，统计从数据库拉取数据加更新数据到索引耗时2133s，平均每次耗时0.001s。只计算更新数据到索引中，不计算从数据库拉取数据耗时464s，平均每次耗时0.0002s，可见索引速度也是很快的，这全部得益于Lucene的优良设计。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 中获取没有存储的字段值的几种方法]]></title>
    <url>%2F2017%2F10%2F10%2Fseveral-ways-to-get-the-field-values-that-are-not-stored-in-lucene%2F</url>
    <content type="text"><![CDATA[一般来说，如果想要从Lucene索引中获取Field的值，那么需要在索引阶段设置Field.Store.YES才可以，然后在搜索阶段得到TopDocs对象之后，用它去获取ScoreDoc再取出Document，使用Document获取存储在索引中的值。但是我们都知道，存储字段是需要硬盘空间的，如果想要追求极致的存储空间并且获取Field的值，那么在不存储的情况下，如何获取呢？其实仔细思索一下，在我们只索引不存储的情况下，Lucene依然可以判断搜索是否命中，这说明在Lucene索引中依然存有一份Field的值，这样在搜索阶段才能判断是否匹配。本文就是探讨在这种情形下，使用Lucene的核心包获取没有存储的Field的值的几种方法，如果你还有其它不同的方法请留言。 testGetFieldByStore 演示存储Field值时如何获取 testGetFieldByTerms 演示通过Terms获取没有存储的Field值 testGetFieldByFieldDocWithSorted 演示通过FieldDoc获取没有存储的值 testGetFieldByTermVector 演示通过TermVector获取没有存储的值 testGetFieldByTermVectors 演示通过TermVectors获取没有存储的值 这里补充一下，在lucene-suggest包中，有LuceneDictionary类，通过该类的getEntryIterator方法也能获取没有存储的Field的值，不过其本质和通过Terms获取方式一样，在此不再列举。源码示例如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.document.*;import org.apache.lucene.index.*;import org.apache.lucene.search.*;import org.apache.lucene.store.RAMDirectory;import org.apache.lucene.util.BytesRef;import org.junit.Test;import java.io.IOException;import static org.apache.lucene.search.SortField.Type.STRING;/** * &lt;p&gt; * Created by wangxu on 2017/10/10 17:33. * &lt;/p&gt; * &lt;p&gt; * Description: Lucene 6.5.0 * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class GetNonStoredFieldDemo &#123; private RAMDirectory ramDirectory = new RAMDirectory(); private IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new WhitespaceAnalyzer())); public GetNonStoredFieldDemo() throws IOException &#123; &#125; @Test public void testGetFieldByStore() throws IOException &#123; initIndexForStore(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); int count = indexSearcher.count(new MatchAllDocsQuery()); TopDocs search = indexSearcher.search(new MatchAllDocsQuery(), count); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; Document doc = indexSearcher.doc(scoreDoc.doc); System.out.println(doc.get("IDX") + "=&gt;" + doc.get("title")); &#125; ramDirectory.close(); &#125; @Test public void testGetFieldByTerms() throws IOException &#123; initIndexForTerms(); Fields fields = MultiFields.getFields(DirectoryReader.open(ramDirectory)); Terms idx = fields.terms("IDX"); Terms title = fields.terms("title"); //or you can use like this //TermsEnum idxIter = MultiFields.getTerms(DirectoryReader.open(ramDirectory), "IDX").iterator(); TermsEnum idxIter = idx.iterator(); TermsEnum titleIter = title.iterator(); BytesRef bytesRef; while ((bytesRef = idxIter.next()) != null) &#123; System.out.println(bytesRef.utf8ToString() + "=&gt;" + titleIter.next().utf8ToString()); &#125; ramDirectory.close(); &#125; @Test public void testGetFieldByFieldDocWithSorted() throws IOException &#123; initIndexForFieldDocWithSorted(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); int count = indexSearcher.count(new MatchAllDocsQuery()); //must use method which returns TopFieldDocs TopFieldDocs search = indexSearcher.search(new MatchAllDocsQuery(), count, new Sort(new SortField("IDX", STRING))); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; FieldDoc fieldDoc = (FieldDoc) scoreDoc; Object[] fields = fieldDoc.fields; if (fields[0] instanceof BytesRef) &#123; BytesRef temp = (BytesRef) fields[0]; System.out.println(temp.utf8ToString() + "=&gt;" + indexSearcher.doc(scoreDoc.doc).get("title")); &#125; &#125; ramDirectory.close(); &#125; @Test public void testGetFieldByTermVector() throws IOException &#123; initIndexForTermVector(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); int count = indexSearcher.count(new MatchAllDocsQuery()); TopDocs search = indexSearcher.search(new MatchAllDocsQuery(), count); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; int doc = scoreDoc.doc; Terms idx = indexSearcher.getIndexReader().getTermVector(doc, "IDX"); TermsEnum iterator = idx.iterator(); BytesRef bytesRef; while ((bytesRef = iterator.next()) != null) &#123; System.out.println(bytesRef.utf8ToString() + "=&gt;" + indexSearcher.doc(doc).get("title")); &#125; &#125; ramDirectory.close(); &#125; @Test public void testGetFieldByTermVectors() throws IOException &#123; initIndexForTermVector(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); int count = indexSearcher.count(new MatchAllDocsQuery()); TopDocs search = indexSearcher.search(new MatchAllDocsQuery(), count); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; int doc = scoreDoc.doc; Fields termVectors = indexSearcher.getIndexReader().getTermVectors(doc); Terms idx = termVectors.terms("IDX"); TermsEnum iterator = idx.iterator(); BytesRef bytesRef; while ((bytesRef = iterator.next()) != null) &#123; System.out.println(bytesRef.utf8ToString() + "=&gt;" + indexSearcher.doc(doc).get("title")); &#125; &#125; ramDirectory.close(); &#125; private void initIndexForStore() throws IOException &#123; Document document = new Document(); document.add(new StringField("IDX", "TEST01", Field.Store.YES)); document.add(new StringField("title", "TITLE01", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new StringField("IDX", "TEST02", Field.Store.YES)); document.add(new StringField("title", "TITLE02", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new StringField("IDX", "TEST03", Field.Store.YES)); document.add(new StringField("title", "TITLE03", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new StringField("IDX", "TEST04", Field.Store.YES)); document.add(new StringField("title", "TITLE04", Field.Store.YES)); indexWriter.addDocument(document); indexWriter.close(); &#125; private void initIndexForTerms() throws IOException &#123; Document document = new Document(); document.add(new StringField("IDX", "TEST01", Field.Store.NO)); document.add(new StringField("title", "TITLE01", Field.Store.NO)); indexWriter.addDocument(document); document = new Document(); document.add(new StringField("IDX", "TEST02", Field.Store.NO)); document.add(new StringField("title", "TITLE02", Field.Store.NO)); indexWriter.addDocument(document); document = new Document(); document.add(new StringField("IDX", "TEST03", Field.Store.NO)); document.add(new StringField("title", "TITLE03", Field.Store.NO)); indexWriter.addDocument(document); document = new Document(); document.add(new StringField("IDX", "TEST04", Field.Store.NO)); document.add(new StringField("title", "TITLE04", Field.Store.NO)); indexWriter.addDocument(document); indexWriter.close(); &#125; private void initIndexForTermVector() throws IOException &#123; FieldType fieldType = new FieldType(); fieldType.setStoreTermVectors(true); fieldType.setIndexOptions(IndexOptions.DOCS); Document document = new Document(); document.add(new Field("IDX", "TEST01", fieldType)); document.add(new StringField("title", "TITLE01", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new Field("IDX", "TEST02", fieldType)); document.add(new StringField("title", "TITLE02", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new Field("IDX", "TEST03", fieldType)); document.add(new StringField("title", "TITLE03", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new Field("IDX", "TEST04", fieldType)); document.add(new StringField("title", "TITLE04", Field.Store.YES)); indexWriter.addDocument(document); indexWriter.close(); &#125; private void initIndexForFieldDocWithSorted() throws IOException &#123; Document document = new Document(); document.add(new SortedDocValuesField("IDX", new BytesRef("TEST01"))); document.add(new StringField("title", "TITLE01", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new SortedDocValuesField("IDX", new BytesRef("TEST02"))); document.add(new StringField("title", "TITLE02", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new SortedDocValuesField("IDX", new BytesRef("TEST03"))); document.add(new StringField("title", "TITLE03", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new SortedDocValuesField("IDX", new BytesRef("TEST04"))); document.add(new StringField("title", "TITLE04", Field.Store.YES)); indexWriter.addDocument(document); indexWriter.close(); &#125;&#125;]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logstash离线安装插件]]></title>
    <url>%2F2017%2F09%2F29%2Flogstash-offline-install-plugin%2F</url>
    <content type="text"><![CDATA[如果线上服务器可以连外网的话，当然是用官方提供的命令来安装插件最简单了，但是可惜的是，好多公司线上服务器是没有外网访问权限的，这就需要在使用某些插件的时候，进行离线安装。而离线安装有两种方式，一种是在可以联网的机器上安装插件，之后使用prepare-offline-pack命令打包，然后将打包文件上传到不能联网的服务器，再使用prepare-offline-pack解包，安装。但是这种方式太麻烦，要求你必须要有一个可以联网的机器，最好还是和不能联网的服务器相同的配置环境，这里推荐一种更好的方案，来解决离线安装插件的问题。 先演示一下，正常的联网环境是如何操作的，如下所示123456[elastic@escluster logstash-5.5.0]$ pwd/home/elastic/elasticsearch/logstash-5.5.0[elastic@escluster logstash-5.5.0]$ ./bin/logstash-plugin install logstash-filter-fingerprintValidating logstash-filter-fingerprintInstalling logstash-filter-fingerprintInstallation successful 那么无法联网，首先需要在可以联网的机器上下载对应插件的压缩包，打开Logstash Plugins地址，直接搜索需要安装的插件名称，然后下载对应的zip或者tar.gz压缩包即可。也许你会说，我用的Logstash是5.5.0的，但是并没有对应版本的插件啊？不用担心，试一下如下命令，显示已经安装的所有插件12345678910111213141516[elastic@escluster logstash-5.5.0]$ ls -la vendor/bundle/jruby/1.9/gemstotal 652drwxrwxr-x. 203 elastic elastic 8192 Jun 30 23:56 .drwxrwxr-x. 9 elastic elastic 104 Jun 30 23:56 ..drwxrwxr-x. 7 elastic elastic 4096 Jun 30 23:56 addressable-2.3.8drwxrwxr-x. 3 elastic elastic 4096 Jun 30 23:56 arr-pm-0.0.10drwxrwxr-x. 6 elastic elastic 4096 Jun 30 23:56 atomic-1.1.99-javadrwxrwxr-x. 5 elastic elastic 55 Jun 30 23:56 avl_tree-1.2.1drwxrwxr-x. 4 elastic elastic 4096 Jun 30 23:56 awesome_print-1.8.0drwxrwxr-x. 3 elastic elastic 16 Jun 30 23:56 aws-sdk-2.3.22drwxrwxr-x. 5 elastic elastic 104 Jun 30 23:56 aws-sdk-core-2.3.22drwxrwxr-x. 3 elastic elastic 16 Jun 30 23:56 aws-sdk-resources-2.3.22drwxrwxr-x. 5 elastic elastic 4096 Jun 30 23:56 aws-sdk-v1-1.67.0drwxrwxr-x. 7 elastic elastic 4096 Jun 30 23:56 backports-3.8.0............ 可以看到，各种版本的插件都有，所以说，插件的版本和Logstash的版本并不要求一致，以安装logstash-filter-fingerprint插件为例，目前最新版是v3.1.1，下载上传到无法联网的服务器上，解压12[elastic@escluster ~]$ gunzip logstash-filter-fingerprint-3.1.1.tar.gz[elastic@escluster ~]$ tar -xvf logstash-filter-fingerprint-3.1.1.tar 进入Logstash的目录，编辑Gemfile文件，在文件开头添加1gem "logstash-filter-fingerprint", :path =&gt; "/home/elastic/logstash-filter-fingerprint-3.1.1" 保存退出，执行命令./bin/logstash-plugin install --no-verify安装，提示如下信息1234567891011121314151617[elastic@escluster logstash-5.5.0]$ ./bin/logstash-plugin install --no-verifyInstalling...LogStash::GemfileError: duplicate gem logstash-filter-fingerprint add_gem at /home/elastic/elasticsearch/logstash-5.5.0/lib/pluginmanager/gemfile.rb:109 gem at /home/elastic/elasticsearch/logstash-5.5.0/lib/pluginmanager/gemfile.rb:207 (eval) at (eval):52 instance_eval at org/jruby/RubyBasicObject.java:1598 parse at /home/elastic/elasticsearch/logstash-5.5.0/lib/pluginmanager/gemfile.rb:195 load at /home/elastic/elasticsearch/logstash-5.5.0/lib/pluginmanager/gemfile.rb:19 gemfile at /home/elastic/elasticsearch/logstash-5.5.0/lib/pluginmanager/command.rb:4 install_gems_list! at /home/elastic/elasticsearch/logstash-5.5.0/lib/pluginmanager/install.rb:146 execute at /home/elastic/elasticsearch/logstash-5.5.0/lib/pluginmanager/install.rb:61 run at /home/elastic/elasticsearch/logstash-5.5.0/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/command.rb:67 execute at /home/elastic/elasticsearch/logstash-5.5.0/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/subcommand/execution.rb:11 run at /home/elastic/elasticsearch/logstash-5.5.0/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/command.rb:67 run at /home/elastic/elasticsearch/logstash-5.5.0/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/command.rb:132 (root) at /home/elastic/elasticsearch/logstash-5.5.0/lib/pluginmanager/main.rb:48 很清晰，这是说在Gemfile中存在重复的logstash-filter-fingerprint，因为在原始的Gemfile中，存在通过联网进行安装的logstash-filter-fingerprint，打开Gemfile，找到并注释掉即可1# gem "logstash-filter-fingerprint" 再次执行安装命令，即可安装成功123[elastic@escluster logstash-5.5.0]$ ./bin/logstash-plugin install --no-verifyInstalling...Installation successful]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年中随想]]></title>
    <url>%2F2017%2F09%2F24%2F2017-first-half-random-thoughts%2F</url>
    <content type="text"><![CDATA[楔子好久没有发文了，最近有价值的技术总结并不多，或者说，已经懒的去总结技术了，毕竟这事也挺耗费时间与精力的。虽说初衷是为了避免无病呻吟，结果却导致自己懈怠了，但是这并不能作为懒惰的理由。最近被一篇《一个月就辞职：一个北大女生的求职悲欢》文章刷屏了，一枚学霸姐，放弃在鹅厂的实习转正机会，毅然投奔在杭州网易的男友，只因为她无法接受异地或者同城异地的感情生活，遂写了这篇半是吐槽半是炫耀的软文，妹子和她的男友确实优秀，两个北大的研究生，一个拿着网易的高薪OFFER，一个弃鹅厂OFFER于不顾，毅然选择为了爱情抛弃面包。如果说这么优秀的妹子仅仅是吐槽找工作困难那是不可能的，头顶拥有着中国最顶级的学府北大的光环，试问全中国795万毕业生（2017年数据），光环度能比之更亮的恐怕凤毛麟角吧。所以这注定是一篇炫耀的软文，炫耀爱情也罢，炫耀能力也罢，炫耀北大的光环也罢，这都无可厚非，毕竟优秀的人有资格炫耀。当然了只有一种例外的情况会得到我的鄙视，那就是靠北京户口考上的北大，因为北大一方面是中国人的北大，另一方面是北京人的北大，这些拿着北京户口考上北大的人侵占了本该属于全中国的有为青年接受更好教育的机会，只希望他们能对得起对他们足够偏心的祖国以及那些因为偏心而导致无缘北大的有为学子们。所幸这个姑娘并不是北京人，所以我对她并无半点鄙视。 在这篇文章中，我无意中学到了这个优秀学霸的一个小习惯，就是多总结。原来在她准备校招的时候，做了详细的Excel表格，总结了投了哪些公司，收到了哪些公司的面试邀约，以及各个面试公司的进展，一轮、二轮、三轮、终面等等，这是一个非常值得学习的好习惯，对比我自身而言，在当时校招的时候，投了哪些公司我自己都不清楚，甚至说，哪些公司有结果了，哪些公司还毫无反馈，我自己都不甚清楚，与之一比，顿觉自惭形秽啊，所以一定要向优秀的人靠拢，观察他们，长期练习，日积月累，学会并总结出一套自我升级的理论，在以后的慢慢人生路上必然是大有裨益的。 其实这篇文章本来并不在计划之中，原先的计划是一年一篇年终总结就够了，不用劳心费力的去写那么多。但是最近想法有变，互联网的世界发展太快了，我脑子里的东西也太多了，这些想法整天在我的脑子里翻滚、碰撞、融合、升级等等，如果不做个记录的话，真的担心以后再也记不起来了，虽然我文笔浅薄，难登大雅，但是记得一个牛人说过，写好文章并没有什么特别的技巧，提起笔写就是了，所以从此刻开始，动手提笔，挥毫泼墨，您姑且一看吧。 斜杠青年了解斜杠青年这个词是在逛知乎的时候偶然知道的，或许这是未来就业的一种潮流吧。随着社会的发展，个人必将更加地追求自我，这也导致多数人不可能一种职业干一辈子，所以将来人在短暂的一生中会承担更加多样化的职业角色，这样就诞生了诸多斜杠青年。 我本身是一名程序员，有着稳定的全职工作，但是我也在想，如果我不写程序的话，我能靠什么养活自己呢？或许我也该试试成为一名斜杠青年，这样不管以后我还写不写程序，或者以后还是不是人写程序，我都能够有一份收入的保障以免自己饿死。当然了有点杞人忧天，不过话说回来，即便不是为了免于饿死，如果你斜杠青年做的成功的话，多几项额外的收入不也是一件幸事吗？ 说干就干，互联网的世界里机会还是很多的，鉴于我国单身男女已达2亿之多， 我一直觉得婚恋市场还有机会产生一个互联网巨头，当然不是世纪佳缘、珍爱网、百合网等的样子，这些网站已经彻底拉低了婚恋行业的道德品质，所谓的红娘已经被训练成了合格的保险推销员的样子，对用户资料不加审核，只管推销会员付费，至于你们合不合适，更不是他们思考的问题，只要给你配对，让你有更多付费的名目即可，这样即便你配不上，对他也没有任何损失，后续还可以继续为你配对牵线，又一个赚钱的好名目。 这些网站婚恋网站肆意造假，毫无底线，甚至可以说为了钱已经到了公然行骗的地步，只要你注册了这些婚恋网站，上来就是给你发站内信，如果你是男性，那么内容肯定是什么高学历白富美对你感兴趣，赶紧联系她吧，下一步想要联系就要付费，而至于这些对你有兴趣的美女之类的都是假的罢了。如果你是女性，那不说你也猜到了站内信的内容是什么，某某上市公司高管精英海龟对你感兴趣之类。 成为一名斜杠青年，我选择从做一个相亲公众号切入『本硕博相亲角』，这种试错成本最小，足不出户，一台电脑，一根网线，很契合程序员的生活，而且公众号放开了，任何个人都可以申请，几乎不需要任何成本，这样的好东西拿来随意鼓捣最适合不过了。但是万事开头难，在没有粉丝没有订阅用户的情况下，如何运营下去呢？我也加入了一些公众号运营者的群，看着这些虚伪的人天天去推荐他们的公众号却还在嘴里喊着为了兴趣才做的，不禁对之嗤之以鼻，如果仅仅为了兴趣，那么你就不应该花费那么大的精力去推广你的公众号去吸粉，这样的话，你已经被你的兴趣绑架了，本来兴趣是为了营造更好的生活，结果呢？你为了兴趣天天累死累活，然后恬不知耻地去到处骚扰别人，轰炸朋友圈推广自己的公众号，还在嘴里大喊我是为了兴趣！做公众号没什么可耻的，推广吸粉没什么可耻的，做公众号为了赚钱同样没什么可耻的，你本可以光明正大地说，我做公众号就是为了赚钱，我们多数人都是完全理解的，毕竟这世界上完全不顾利益却投入大量的精力去做事的人是很少的，然而他们最可耻的就是这点，做了荡妇却还要立贞节牌坊，这就实在是让人难以下咽了。 在做公众号以前我是非常讨厌那些标题党的，不过在你亲身体验之后，开始慢慢理解了，毕竟吸粉太困难了，不靠内容、不靠标题、不靠噱头根本得不到用户的关注啊，在新增用户已经很少的情况下，每天还有大量的人取消关注，你甚至都开始怀疑人生了，如果我直接推送毛片是不是可以得到更多的关注呢？当然了，这是不可取的，我想表达的是，在互联网这个充斥这垃圾信息的世界，尤以今日头条为最，几乎全是垃圾信息，想要让优质内容触达每位用户是非常难的，这也正是微信的难题，所以微信也在不断变化，从支持原创，到转载原创内容变成分享模式，都是在鼓励原创，也是想让更多优质的原创内容直达用户。 房子房子这个话题，是每个人都躲不开的，不管你是耄耋之年还是弱冠之年，房子都是一个迈不过的坎，也许你会说，对于那些即将入土的人了，还有必要考虑房子吗？少年啊，你还是too young，too simple啊，君未见最近国家开始推动以房养老了吗？其实国家的算盘也很简单，让每个人都背负着一生的房贷，穷尽毕生之力创造的财富，都不过是为银行打工为国家打工罢了。现在倒好，国家的上层人士真是人才辈出啊，你以为你打了一辈子的工可以传给子女一套房子，太天真了，现在来个以房养老，让你在死的时候再把房子还给国家，这真真的是来也空空，去也空空啊，白白在世间劳累一生，也许有人说你太以小人之心度国家之腹了，呵呵，我倒也希望是呢，这样毕竟我错了，那么国民可以幸福，罢特（But）现实总是这么残酷，不论你信也罢不信也罢，它就在那里，不悲不喜、不来不去、不增不减、不舍不弃。有兴趣的可以去看看《太子党关系网络》，多少大公司都掌握在私人手里，多少财富都进了私人腰包，从这个关系网里，我还学到了一点，一定要多生孩子，你看看那些只有一个孩子的开国牛人们，其关系网多么形单影只啊，就一条直线发展下去，反观那些子女多的，关系网密密麻麻，势力庞大无比，还可以与其它强大的家族联姻，结果是一代更胜一代。看完真的是心痛啊，用鲁迅的话说，就是哀其不幸，怒其不争，现在的愚民们依然逃不过这句话。 其实并不是少数人有此想法，现在可以说部分有识之士已经看出了国家的企图，根据他们的分析，我先抛出结论，那就是赶紧买房。何来此说，大城市的的房价压的人喘不过气来，其实方法很简单，敞开土地供应即可，不是有好多人抢房子吗？还有好多屯房子的？那就让他屯让他抢，只要土地敞开供应，房价绝不会涨到现在这个地步的。那么为什么有利于人民的好事总是无法实现呢？因为土地财政啊，同志们，国家卖了几十年的房子，现在已经发现一个问题了，国家开始后悔了，为什么？因为土地卖的太便宜了。你说现在的房价已经上天了，为什么还说土地卖便宜了呢？且慢，土地和房价并不能简单的挂钩的，土地卖的是七十年使用权，而价钱是在土地出让的时候一次性收取，把时间线拉长了来看，这七十年只要货币随便贬点值，随便多印点钞票，国家就会发现自己亏了，土地卖贱了，所以现在开始大力鼓吹什么租售同权，这样从长远来看，房租每年都可以涨，国家现在直接放弃一锤子买卖，可以大摇大摆的拼命印钞了，货币贬值了，不怕，我涨房租就是了，反正我不会亏。那倒霉的是谁呢？就是普通的老百姓，从总价来算，租七十年的房子最后付出的金钱一定是比买一套七十年产权的房产要多的。你以为租售同权了，你就不用买房了？用一些有识之士的人的话说就是，你们就是等着被收割的韭菜，还有大量的智商税都是向你们这些人收的，这才得出了我一开始就抛出的结论，有钱赶紧买，现在就买，否则你真的会租一辈子房子的。 摩拜 VS. OFO为什么谈这个话题，只因为朱啸虎和马化腾在朋友圈互怼，火爆了整个互联网圈子。如果就短期来看的话，OFO是有机会吞并摩拜的，为什么这么说，因为在短期如果有合并事件发生的话，那么由于OFO的铺货量远超摩拜，再加上OFO的假订单数量远超摩拜（假订单指的是我在家不动，一天就能用OFO刷个十几二十单，而摩拜则完全无法刷单），会给予投资人一种假象，认为OFO的市场占有率超过摩拜，进而在合并两者的时候给予OFO更大的估值。但是同样地，拉长时间战线，摩拜一定是最后的胜者，不光源于摩拜过硬的整车质量，以及一些细节都让我非常欣喜。比如摩拜的后齿轮链条两边是有凸起的塑料包裹，这样可以防止掉链子，还有摩拜的车筐设计，是固定在前管(头管)上，而不是车把手上，这样车筐不会随着把手的转向而左右晃动。摩拜这些贴心的细节设计，树立了一个有追求企业的标杆，反观OFO呢？垃圾的一塌糊涂，虽说OFO的车身重量比摩拜轻，但是亲身体验之后发现，OFO骑起来比摩拜重多了，这就是质量的价值。除此之外，OFO的车身设计几乎没有值得称道的地方，满大街的OFO，扫过十辆几乎有一半是坏的，即便现在OFO换掉了哑终端，换上了号称有自主研发技术的智能锁，那也是和摩拜无法相提并论的，摩拜依靠太阳能（车筐底部的黑色太阳能板）和用户骑行的动力（有些人反馈摩拜重就源于此）来给智能锁充电，而OFO居然恬不知耻的也说自己的是智能锁，其实OFO的锁中是依靠纽扣电池提供动力的，而这些纽扣电池的电量最多维持两年左右，所以两年之后，不仅满大街都是坏的OFO，还有满大街的扫了毫无反应的OFO了。 对于长期的发展而言，我更加看好摩拜。如果我预言错误，欢迎您回来留言打脸，这锅我背。遥想当年，晋惠帝的那句“何不食肉糜”，现在的投资人也几乎到了同样的地步，他们不去街头巷尾亲身体验一下摩拜和OFO的真实差距，只知道坐在高楼大厦里看数据，殊不知，数据也是完全可以造假的。其实还可以进一步引申，那就是阶级分层，因为顶层的人士现在已经完全和底层的人脱节了，他们无法了解底层的人的真实生活情况。朱啸虎说，OFO的订单量超过摩拜，OFO的投放量超过摩拜，在街头的OFO数量超过摩拜，我只想说，你个二傻子，当你扫了十辆OFO有一半坏的时候，你最想做的是看到一辆OFO砸一辆，而不是投资了。 比特币最初了解到比特币是在2013年，当时也是偶然看到，但是只研究了一天，发现个人挖矿的机会已经消失，剩下是专业矿机和矿池的天下了，所以果断的放弃了，现在想来实在可惜，虽然挖不到了，但是我可以买啊，而之所以没有投资，就是没有敏锐的察觉到比特币会涨成今天这个样子。刚查了一下，2013年8月时候是108美元/BTC，而现在的价格是3800美元/BTC，一个百万富翁的机会就这样错失了，简单算下，一枚净赚3700美元的话，折合24400元人民币，那想获利一百万，只需要持有41个BTC左右。但是今天我想说的并不是回首往昔，空余嗟叹，而是比特币是无法成为合法货币的。 现在的币圈已经火得如楼市一般了，再加上ICO项目的加持，大量资金汇集而来，不管是投资者还是投机者，在这场ICO的盛宴中，他们都是可怜的散户。ICO的本质就是一场赌局，一场零和博弈。而据我所知，多数人的脑子已经烧到了对风险视而不见的地步了，这本不是什么高深的骗局，但面对巨大的诱惑，几十枚就能达到百万富翁的地步，许多人已经是对风险不管不顾了，就算是骗局，也要博上一把。不管是BTC还是其它任何Coin其本身都是无法产生任何价值的，当然会有人站出来说，我用它做为支付媒介了，这就是它的价值啊，可是在现在的币圈，基本上98%的交易都是为了投机而已，而并不是实实在在的因商业贸易而做的交易。另一方面，任何一个合法的政府都不会让BTC成为合法的货币，印钞票是政府获得财政收入的最大手段，也是政府用来制造通胀进而剥削全体国民的最有效有段，这不仅是执政之基，也是立国之本，当货币的发行权不掌握在自己手中，并且这个货币还是无法通胀的时候，任何不需要有脑子的政府也知道，是绝不可能让它成为合法货币的，最多是合法交易而已。 我从来不反对投资，自然也不反对投机，只是愿赌服输，不要让国家兜底，或者说不要像那些傻逼炒房族在房价下跌之后去打砸售楼处一样，这种粗俗智商低劣的行为我是很看不惯的，堂堂中华上下五千年，自古买定离手，而你在炒房之后，赫然发现房价下跌进而打砸售楼处，这得是多么恶心的人才能干的事啊。那如果我大中国13亿人都在买定东西之后，发现东西有任何降价行为都去打砸超市、菜市场、商场等等，那画面太美，我不敢想。我对投机是没有任何偏见的，投机的世界一定是个零和博弈的世界，有的人赚就一定有人亏，而前提是每个人都是自愿并且抱着对未来的美好愿景而入局的，那就只能靠自己的实力各安天命吧，还是那句话，即便你输了，也要维持人品这个最低的道德底线，因为你是人，不是畜生。 中印对峙为什么要谈这个事呢？主要是鲁迅已逝，但是阿Q重生，让我不得不说。不论是公众号还是简书任何允许自由发声的世界里，我都发现脑残居多，真正有智慧的人总是处于数量上的劣势。谈中印对峙主要是因为印度撤军而起，在维基百科上2017年中印军队洞朗对峙事件说的清清楚楚，中印对峙是因为印度入侵了中国的洞朗地区而起，在印度主动撤军之后，中国的阿Q党们群起而攻之，比如有说“阿三怕了，吓跑了”、“印度阿三认输了”、“印度服软了，扛不住了”、“阿三怂了”、“阿三再张狂，也得灰溜溜走了”…… 这些人都是什么心态啊，人家入侵你的国家，你就一顿嘲讽？人家入侵你的国家，只是撤退了而已，你就说人家怕了？人家入侵你的国家，只是回到了入侵之前的状态而已，你就说人家怂？本身而言，印度撤军当然值得庆贺，说明我国强大，但是我不希望你们都是一群阿Q党，这就像比如有人跑到你家院子里，然后一顿打砸，最后走了，你在那拍拍手，说，“嘻，吓跑了吧”，那我也是服你的。 抛开这事不谈，在其它方面，中国也远远不够强硬。中国和印度有领土争端，但是争议领土在印度手里（藏南地区）；中国和日本有领土争端，但是争议领土在日本手里（钓鱼岛），中国和越南有领土争端，但是争议领土在越南手里（南沙群岛、西沙群岛）；中国和菲律宾、文莱、马来西亚有领土终端，但是争议领土同样不在中国手里（南沙群岛、黄岩岛）；其它还有很多就不一一列举。总之我知道，俄罗斯和任何国家有领土终端，那么争议领土一定在俄罗斯手里，还有远东地区，被俄罗强占，现在倒好，开采油气转手卖给中国，躺着都赚钱。都说中国人民是世界上最勤劳最辛苦的人民，中国有这么多的阿Q党，不辛苦天理何在呢？中国还远远不够强大，至少就拿老祖宗给我们留的国土来说，近两百年丢了太多太多…… 金融杠杆加杠杆，没什么好说的，我是举双手赞成的。现在通胀这么厉害，加杠杆是对抗通胀的一种利器。只要能有机会加杠杆的，一定要加上，那么说到底，买房其实也是一种加杠杆行为，比如你首付一百万，买了个三百万的房子，你本身是没有另外两百万的，但是你依然撬动了三百万的房子，这就是加杠杆行为。买房的人都赚钱了，这是肯定的，试想你用一百万，撬动了三百万的资金，即使你不买房子，你去干任何事情，都是有机会赚钱的，这本身的逻辑是完全正确的，买房赚钱的本质不是因为买房，而是你动用了杠杆，另外还有就是通胀，钱越来越多，钞票越来越毛，房价自然涨。 知识服务谈知识服务就必然要谈得到，因为罗振宇是中国的知识服务商们的执牛耳者。就我个人而言，听了得到上面的很多音频，不管是读书系列，还是订阅系列，亦或者罗振宇自己主讲系列，都有很多谬误，我希望你们把得到作为一种获取知识的渠道，而不是圣经，以为他讲什么都是对的，他讲什么都是有道理的，这是不然的。就本质来讲，罗振宇是个商人，商人自然就要盈利，他不是布道师，也不是来民间义务为你们开智的天使，其实他自己也坦言，他所做一切节目，所搞的一切社群活动的最终目的就是为了赚钱，然而他的特殊之处仅仅在于他不是卖普通的商品，而是卖知识罢了，毕竟在这个脑残盛行的时代、在这个已经很少有人愿意读书的时代、在这个垃圾信息泛滥的时代，能坚持把知识做下去，在成年人当中引领者一股读书的风潮是值得称贺的。 我引用一段知乎上的用户回答 有一期，说北伐军进入南京的时候轮奸了领事夫人，青龙刀杀了领事。这个段子是林思云写的《真实的汪精卫》，而且已经在当时辟谣说是误传了。我给罗振宇发了邮件，他的制片人邮件，他的微博私信，都没有回应。看来，对历史的严谨，他们节目差太多了。——刘三叔 逻辑思维最好听的就是写历史故事，这些玩意也是他们扒的别人写的书，例如多次引用张宏杰先生的书，甚至连语气和修辞都照搬，原创性几乎为零。但是我们把这个节目定义成打发时间的节目，只要听别人讲故事就够了嘛。所以还是可以接受的。但是一旦开始讲道理，就只能呵呵笑了。因为太傻叉了。逻辑思维每一期都想要讲个道理，或者给你什么启示，你一旦听他总结，就觉得我靠这人什么脑子。——改之理zcw 罗振宇是一个媒体人，不是个学者，所以他的节目内容，多是结合热门事件，糅合社科观点的大杂烩。具有传播宣传效果，是为了吸引观众，但大多并不值得推敲。同理可证，相亲节目不是用来相亲的。——陈新 别的我就不多说了，逻辑思维中的错误很多，而且已经越来越多的人开始觉醒了，他们开始思考，开始质疑，发现得到上的知识很多也不过都是一家之言罢了，所谓的知识服务，也就是东拼西凑拼书而已。有时候好多想法在我脑子里慢慢诞生，但是偶然在一本书或者一篇文章中或者其他的某个地方，又与有同样想法的人不期而遇，真是不觉击节赞叹，英雄所见略同啊。 工作我这人有一个坏习惯，做事总想做到完美，所以把《阿里巴巴Java开发手册》经常放在手边，不时翻看，对应地，在现实的开发世界中，总想把东西做到在我当前水平内的最优，这必然导致几个结果，其一，领导不知，因为代码写的烂和好，功能都能够实现，根本没人关注你代码写的多漂亮，耦合度有多低，扩展有多方便，他们只关注功能是否按时上线，这导致我做了大量的幕后工作，但是并没有得到应有的回报。另一个就是前期很多人都是草草开发，力求尽快上线即可，后期当用户量上来了或者系统的各种瓶颈出现的时候，再力求优化或者重构，这样居然能获得公司的团队激励奖励，为公司的某某产品重构，性能提升了多少，BUG解决了多少，我真是呵呵了，尼玛，这不都是你们当年埋下的吗。 我现在真的是越来越理解比尔·盖茨当年的全员持股是多么牛逼的壮举了，因为就现在来说，多数人并不把公司的命运、前景、发展与个人利益直接挂钩，对他们来说，这个公司是明天倒闭还是二十年之后倒闭，真的差别不大，多数人的唯一目标就是当一天和尚撞一天钟，什么时候这个钟坏了或者说再也撞不了了，那么我换一个钟，继续当一天和尚撞一天钟即可。所以就我了解的开发而言，多数人都是不合格的，更不用谈那些培训机构出来的了，简历公然造假居然都不以为意，被那些培训机构里完全以挣钱为目的的老师教出来的学生，其核心价值观我真的很怀疑。 理财现在的社会，如果你有能力加杠杆，那就不要客气，但是如果你没有这个能力，那就一定要学会理财，俗话说，你不理财，财不理你。农村的很多人真是辛苦一辈子，最后一夜回到解放前，当然这只是个比喻的说法，因为他们没有任何金融方面的知识，只知道把钱存在银行，包括我父母也是这样，虽然我父母并没有多少钱可以存，但是不论多寡，只要放在银行中，你就是亏本的，而你亏的钱就是通胀利率减去银行利率再乘以本金，可怜中国的老百姓，每当我看到他们辛苦攒钱都是莫名的心痛，你攒的这一点点钱，国家随便印个钞票都给你通胀掉了。 从毕业到现在我已经存了二十万了，钱虽不多，但是这是我从无到有，白手起家，一点点积累而来，和那些含着金钥匙出生的人自然无法相提并论，不用说富二代官二代了，光是普通的上海土著，碾压我都没有丝毫压力，但是如果真正的拼个人，而不是拼父母，不是拼上一代，我想我也并不输他们多少吧。 我的多数资产都在投资P2P，目前在投的有七八个平台吧，期间也玩过黄金，薅过羊毛，还有就是少量的公众号的收入，虽然微不足道，但是积少成多，集腋成裘啊，你一个山里出来的人，没有任何人可以指望的上，我想我能做的也只有这么多了吧。前路漫漫，我始终坚信互联网的世界里还是有很多机会的，哪怕你只要抓住一次，就有机会翻身，这也导致我在当年校招的时候，嚣张的和HR说，我只去互联网公司，只要一直浸淫在这个行业中，总会有机会的，希望我不要被自己打脸吧，然后啥机会也没抓住，那就丢人了。现在虽然辛苦，但是为了更好的下一代，为了父母老有所依、老有所养，我别无他路，只能躬身前行。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Byteman 使用教程]]></title>
    <url>%2F2017%2F09%2F22%2Fbyteman-uses-tutorials%2F</url>
    <content type="text"><![CDATA[Byteman简介Byteman由JBoss出品，JBoss大家应该都熟悉，顶顶大名的应用服务器JBoss也出自其手。Byteman的代码插入能力相比BTrace而言更强，似乎可以在代码中任意的位置插入我们的跟踪代码（当然，你可能需要对Java代码生成、字节码技术有一定的了解），以及访问当前方法中变量的能力（包括方法参数、局部变量、甚至于调用其它函数的参数值、返回值等），而BTrace在这方面的能力要弱很多。 安装Byteman首先去官网下载最新的压缩包，解压，配置环境变量，开始操练，老熟悉了。新建BYTEMAN_HOME值是E:\byteman-3.0.10，编辑Path环境变量，在末尾添加;%BYTEMAN_HOME%/bin，打开cmd，输入bminstall验证一下 usage: bminstall [-p port] [-h host] [-b] [-s] [-m] [-Dname[=value]]* pid pid is the process id of the target JVM -h host selects the host name or address the agent listener binds to -p port selects the port the agent listener binds to -b adds the byteman jar to the bootstrap classpath -s sets an access-all-areas security policy for the Byteman agent code -m activates the byteman JBoss modules plugin -Dname=value can be used to set system properties whose name starts with “org.jboss.byteman.” expects to find a byteman agent jar and byteman JBoss modules plugin jar (if -m is indicated) in BYTEMAN_HOME 使用示例（读取局部变量）为什么选择这个示例呢？因为BTrace无法追踪到局部变量的值，那么为了显示Byteman的强大，必须让它做别人做不到的事。首先编写一个待追踪的示例，这个例子很简单，接收用户的输入内容，并原样输出即可，如果输入中包含end，那么程序自动结束，如下12345678910111213141516171819202122232425262728293031323334353637383940import java.io.BufferedReader;import java.io.DataInputStream;import java.io.IOException;import java.io.InputStreamReader;public class BytemanDemo &#123; public static void main(String[] args) &#123; new BytemanDemo().start(); &#125; private void start() &#123; new Thread(() -&gt; &#123; DataInputStream in = new DataInputStream(System.in); BufferedReader buf = new BufferedReader(new InputStreamReader(in)); try &#123; String next = buf.readLine(); while (next != null &amp;&amp; !next.contains("end")) &#123; consume(next); next = buf.readLine(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125; public void consume(String text) &#123; //这是个局部变量，将会在byteman中追踪她 final String arg = text; Thread thread = new Thread(() -&gt; System.out.println("program confirm " + arg)); thread.start(); try &#123; thread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return; &#125;&#125; 好了，启动程序测试一下 byteman test demoprogram confirm byteman test demo 那么试想一下，如果程序输出的和我们输入的总是不同，会不会是consume方法中出现了Bug呢？那么如果想要确认传入到consume方法中的参数是否正确要怎么做呢？所以这时候就需要在byteman中读取consume方法中的局部变量arg了。 首先加载byteman到JVM中，并将它attach到需要监听的进程上，指定byteman监控程序监听55000端口（这个端口是用来对byteman的脚本进行响应的）1234567C:\Users\wangxu&gt;jps18592 Jps20068 AppMain858418668 LauncherC:\Users\wangxu&gt;bminstall -b -Dorg.jboss.byteman.transform.all -Dorg.jboss.byteman.verbose -p 55000 20068 再回到被监控程序的控制台看到如下输出 byteman test demoprogram confirm byteman test demoSetting org.jboss.byteman.transform.all=Setting org.jboss.byteman.verbose=TransformListener() : accepting requests on localhost:55000 从输出信息中可以看到，byteman监听55000端口，并在该端口接收请求，那就来请求一把呗，打开编辑器，编写byteman的btm脚本，ShowLocalVar.btm，如下123456789RULE trace line local varCLASS BytemanDemoMETHOD consume(String)AFTER WRITE $argIF TRUE DO traceln("*** transfer value is : " + $arg + " ***")ENDRULE 接下来就安装脚本吧，运行bmsubmit提交脚本 C:\Users\wangxu&gt; bmsubmit -p 55000 -l ShowLocalVar.btminstall rule trace line local var 再回到被监控程序的控制台，输入一段测试语句”byteman test demo2”查看输出123456789101112131415TransformListener() : handling connection on port 55000retransforming BytemanDemoorg.jboss.byteman.agent.Transformer : possible trigger for rule trace line local var in class BytemanDemoRuleTriggerMethodAdapter.injectTriggerPoint : inserting trigger into BytemanDemo.consume(java.lang.String) void for rule trace line local varorg.jboss.byteman.agent.Transformer : inserted trigger for trace line local var in class BytemanDemobyteman test demo2Rule.execute called for trace line local var_0HelperManager.install for helper class org.jboss.byteman.rule.helper.Helpercalling activated() for helper class org.jboss.byteman.rule.helper.HelperDefault helper activatedcalling installed(trace line local var) for helper classorg.jboss.byteman.rule.helper.HelperInstalled rule using default helper : trace line local vartrace line local var execute*** transfer value is : byteman test demo2 ***program confirm byteman test demo2 可以看到星号打头和结尾的那一句，就是监控程序从被监控程序中得到的局部变量，是不是很强大啊。 当然了可以安装脚本，就可以卸载脚本，同样是使用bmsubmit命令 C:\Users\wangxu&gt; bmsubmit -p 55000 -u ShowLocalVar.btmuninstall RULE trace line local var 卸载完成之后，再回到被监控程序的控制台，输入测试语句123456789TransformListener() : handling connection on port 55000retransforming BytemanDemoHelperManager.uninstall for helper class org.jboss.byteman.rule.helper.Helpercalling uninstalled(trace line local var) for helper class org.jboss.byteman.rule.helper.HelperUninstalled rule using default helper : trace line local varcalling deactivated() for helper classorg.jboss.byteman.rule.helper.HelperDefault helper deactivatedbyteman test demo3program confirm byteman test demo3 可以看到程序又回到了最初的摸样，这种完全无侵入式的设计真是太完美了，对于需要被监控的程序来说几乎是透明的，它不需要做任何改变，我们就可以通过动态追踪技术得到我们感兴趣的信息，而且在追踪结束后卸载脚本，被监控程序依然还是最初的摸样。 注意点：我的被监控程序是在IDEA中运行的，也就是说，IDEA会自动帮我编译源码，我只需要运行即可，那么如果你是使用命令行自己编译的Java源码，很可能无法追踪到局部变量的值，这是因为自己编译的话，需要指定-g参数，也就是像这样javac -g Byteman.java编译出来的class文件才能够被追踪到局部变量。对细节感兴趣的话请看这里。 使用示例（统计方法耗时）与上例不同，这个例子主要展示如何使用bmjava和bmcheck这两个命令。先上待追踪代码，很简单的例子，working方法模拟工作方法，我们通过动态追踪获得工作方法花费的时间 12345678910111213141516171819202122232425262728293031323334import java.io.BufferedReader;import java.io.DataInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.concurrent.TimeUnit;public class WorkerDemo &#123; public static void main(String[] args) &#123; WorkerDemo workerDemo = new WorkerDemo(); DataInputStream dataInputStream = new DataInputStream(System.in); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(dataInputStream)); try &#123; String next; while ((next = bufferedReader.readLine()) != null) &#123; System.out.println("before working"); workerDemo.working(); System.out.println("after working"); if (next.contains("end")) &#123; break; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private void working() &#123; try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 编写byteman脚本1234567891011121314151617181920RULE trace worker time startCLASS WorkerDemoMETHOD workingAT ENTRYIF TRUEDO createTimer($0)ENDRULERULE trace worker time stopCLASS WorkerDemoMETHOD workingAT EXITIF TRUEDO traceln("*** working time " + getElapsedTimeFromTimer($0) + " ms ***"); deleteTimer($0);ENDRULE 注意一点，在规则脚本中用到的方法createTimer，getElapsedTimeFromTimer，deleteTimer都是在org.jboss.byteman.rule.helper.Helper类中定义好的，并且在创建Timer之后，一定要记得删除，这里面方法的形参必须要从0开始，感兴趣的可以看看这几个方法的详细说明 createTimer can be called to create a new Timer associated with o. createTimer returns true if a newTimer was created and false if a Timer associated with o already exists. getElapsedTimeFromTimer can be called to obtain the number of elapsed milliseconds since the Timerassociated with o was created or since the last call to resetTimer. If no timer associated with o exists anew timer is created before returning the elapsed time. resetTimer can be called to zero the Timer associated with o. It returns the number of seconds since theTimer was created or since the last previous call to resetTimer If no timer associated with o exists anew timer is created before returning the elapsed time. deleteTimer can be called to delete the Timer associated with o. deleteTimer returns true if a new Timerwas deleted and false if no Timer associated with o exists. 将WorkerDemo类和规则脚本置于同一目录下，打开cmd运行命令行，先检查一下规则脚本是否合法 12345678910C:\Users\wangxu&gt;bmcheck -cp . -v TimeTracer.btmChecking rule trace worker time start against class WorkerDemoParsed rule &quot;trace worker time start&quot; for class WorkerDemoType checked rule &quot;trace worker time start&quot;Checking rule trace worker time stop against class WorkerDemoParsed rule &quot;trace worker time stop&quot; for class WorkerDemoType checked rule &quot;trace worker time stop&quot;TestScript: no errors bmcheck命令可以在应用规则脚本之前对其进行检查，bmcheck [-cp classpath]* [-p package]* [-v] script1 . . . scriptN，如果是带包名的类，那么需要使用-p参数即可。 再将脚本和程序一起启动1234567891011C:\Users\wangxu&gt;javac -g WorkerDemo.javaC:\Users\wangxu&gt;bmjava -l TimeTracer.btm WorkerDemotrace working time demobefore working*** working time 5003 ms ***after workingtrace endbefore working*** working time 5000 ms ***after working 这样操作有一个限制，就是无法追踪已经启动了的程序，如果已经有个程序正在运行，而这时候程序的输出与我们预期的不符，如何用脚本来追踪呢？这时候需要用到bminstall和bmsubmit命令。首先就按最普通的方式启动程序再说 C:\Users\wangxu&gt;java WorkerDemo 再开一个cmd命令行，先把byteman attach到需要追踪的进程上，再提交规则脚本123456789101112C:\Users\wangxu&gt;jps117241468 Jps17308 WorkerDemo5948 LauncherC:\Users\wangxu&gt;bminstall -b -Dorg.jboss.byteman.tranform.all 17308C:\Users\wangxu&gt;bminstall -b -Dorg.jboss.byteman.tranform.all 17308C:\Users\wangxu&gt;bmsubmit -l TimeTracer.btminstall rule trace worker time startinstall rule trace worker time stop 这时候再回到启动程序的命令行，可以看到有新的输出 Setting org.jboss.byteman.tranform.all= 我们输入一行测试语句试一下，看看规则脚本是否生效123456C:\Users\wangxu&gt;java WorkerDemoSetting org.jboss.byteman.tranform.all=trace working time demo2 and then endbefore working*** working time 5007 ms ***after working 使用这种方式就可以动态的追踪已经启动的程序了。 Byteman脚本介绍首先了解byteman的脚本结构，如下所示12345678####################################### Example Rule Set## a single rule definitionRULE example rule# comment line in rule body. . .ENDRULE 其中合法的规则事件可以是如下这些12345678# rule skeletonRULE &lt;rule name&gt;CLASS &lt;class name&gt;METHOD &lt;method name&gt;BIND &lt;bindings&gt;IF &lt;condition&gt;DO &lt;actions&gt;ENDRULE 同样全部的合法定位点或者说注入点如下所示1234567891011121314151617AT ENTRYAT EXITAT LINE numberAT READ [type .] field [count | ALL ]AT READ $var-or-idx [count | ALL ]AFTER READ [ type .] field [count | ALL ]AFTER READ $var-or-idx [count | ALL ]AT WRITE [ type .] field [count | ALL ]AT WRITE $var-or-idx [count | ALL ]AFTER WRITE [ type .] field [count | ALL ]AFTER WRITE $var-or-idx [count | ALL ]AT INVOKE [ type .] method [ ( argtypes ) ] [count | ALL ]AFTER INVOKE [ type .] method [ ( argtypes ) ][count | ALL ]AT SYNCHRONIZE [count | ALL ]AFTER SYNCHRONIZE [count | ALL ]AT THROW [count | ALL ]AT EXCEPTION EXIT 更多更详细的信息可以去参考Byteman的官方手册。也许你注意到了，在脚本中我们使用了traceln语句，那么这个调用的其实是Byteman的org.jboss.byteman.rule.helper.Helper类的方法，这些方法都是已经内置的，可以直接在脚本中调用。 当然了如果你觉得这些内置的方法不够用，也可以使用自定义的Helper类，例如官方给出的一个例子是1234567891011# helper example 2RULE help yourself but rely on othersCLASS com.arjuna.wst11.messaging.engines.CoordinatorEngineMETHOD commitHELPER HelperSubAT ENTRYIF NOT flagged($this)DO debug("throwing wrong state"); flag($this); throw new WrongStateException()ENDRULE HelperSub的源码示例如下12345class HelperSub extends Helper &#123; public boolean debug(String message) &#123; super("!!! IMPORTANT EVENT !!! " + message); &#125;&#125; Byteman环境变量在Byteman中，总共的环境变量有如下这些 org.jboss.byteman.compileToBytecode org.jboss.byteman.dump.generated.classes org.jboss.byteman.dump.generated.classes.directory org.jboss.byteman.dump.generated.classes.intermediate org.jboss.byteman.verbose org.jboss.byteman.debug org.jboss.byteman.transform.all org.jboss.byteman.skip.overriding.rules org.jboss.byteman.allow.config.updates org.jboss.byteman.sysprops.strict 而我们已经使用过了其中的两个，就是在bminstall命令中所示的 org.jboss.byteman.transform.all 如果设置了将允许注入java.lang和其子包的class org.jboss.byteman.verbose 如果设置将显示执行的各种跟踪信息到System.out，包括类型检查，编译，和执行规则 有关更加详细的解释可以到官方网站中的Environment Settings一节查看。 javaagent技术简单来说，javaagent 技术是一个开发者可以构建一个独立于应用程序的代理程序（Agent），用来监测和协助运行在 JVM 上的程序，甚至能够替换和修改某些类的定义。有了这样的功能，开发者就可以实现更为灵活的运行时虚拟机监控和 Java 类操作了，这样的特性实际上提供了一种虚拟机级别支持的 AOP 实现方式，使得开发者无需对 JDK 做任何升级和改动，就可以实现某些 AOP 的功能了。有关 javaagent 更详细的介绍，可以参看Instrumentation 简介这篇文章。 用javaagent加载规则文件并启动程序javaagent 选项支持在所有的个人机或服务器的JVM中使用Byteman，那么看一下如何使用 javaagent 技术来启动Byteman吧。还是以BytemanDemo为例，BytemanDemo和ShowLocalVar.btm文件都在C:/Users/wangxu路径下123456C:\Users\wangxu&gt;javac -g BytemanDemo.javaC:\Users\wangxu&gt;java -javaagent:%BYTEMAN_HOME%\lib\byteman.jar=script:ShowLocalVar.btm BytemanDemojavaagent byteman demo*** transfer value is : javaagent byteman demo ***program confirm javaagent byteman demo script用于指示 Byteman 规则文件的位置。Byteman agent 读取到这个选项之后从规则文件中加载和注入Byteman规则。如果要加载多个script:file规则文件，使用逗号（,）分隔即可。 用javaagent启动程序并动态加载规则文件这种方式是将规则文件和需要监控的程序一起启动，那么如果被监控程序已经启动，该如何注入规则文件呢？严格执行如下步骤即可 需要在启动程序的时候采用java -javaagent方式启动，同样地先编译javac -g BytemanDemo.java 新开cmd，输入 1C:\Users\wangxu&gt;java -javaagent:%BYTEMAN_HOME%\lib\byteman.jar=listener:true,boot:%BYTEMAN_HOME%\lib\byteman.jar -Dorg.jboss.byteman.transform.all BytemanDemo Linux系统只需要把%BYTEMAN_HOME%换成${BYTEMAN_HOME}即可，注意Linux上分隔符是正斜杠，这个监听器会开启一个网络监听。注意：当没有规则加载的时候，程序的的行为不会发生任何变化，仅输出我们输入的内容。这时候需要第三步来加载规则文件 新开cmd，输入C:\Users\wangxu&gt;bmsubmit -l ShowLocalVar.btm，cmd反馈出install rule trace line local var，再回到第二步的cmd窗口，输入测试语句”javaagent byteman demo2” 1234C:\Users\wangxu&gt;java -javaagent:%BYTEMAN_HOME%\lib\byteman.jar=listener:true,boot:%BYTEMAN_HOME%\lib\byteman.jar -Dorg.jboss.byteman.transform.all BytemanDemojavaagent byteman demo2*** transfer value is : javaagent byteman demo2 ***program confirm javaagent byteman demo2 可以看到规则已经生效。这种方式虽然可以实现同样地监控，但是还是存在一个明显的弊端，就是要求在启动程序的时候使用javaagent方式，但是通常我们并不会用这种方式，大多数人都是用最普通的java -jar命令，那么且看下节。 动态安装agent到正在运行的程序中如果你启动了一个长时程序，并且没有加载 Byteman agent。你不需要重启程序也可以使用 Byteman。在Byteman的bin目录下有很多脚本，Windows上的是bminstall.bat，Linux上的是bminstall.sh，由于所有线上服务器都是用的Linux，那么接下来我就在CentOS上演示一下动态安装agent技术。 首先编译，启动程序再说123456789[elastic@escluster ~]$ lltotal 12-rw-rw-r--. 1 elastic elastic 1218 Sep 21 15:59 BytemanDemo.javadrwxr-xr-x. 10 elastic elastic 4096 Aug 29 14:45 elasticsearch-rw-rw-r--. 1 elastic elastic 171 Sep 21 15:59 ShowLocalVar.btm[elastic@escluster ~]$ javac -g BytemanDemo.java[elastic@escluster ~]$ java BytemanDemoByteman agent in linuxprogram confirm Byteman agent in linux 这个时候可见就是一个普通的程序，下面在Linux配置环境变量，编辑/etc/profile1234BYTEMAN_HOME=/usr/program/byteman-3.0.10PATH=$PATH:$BYTEMAN_HOME/binexport BYTEMAN_HOMEexport PATH 新开一个终端，按如下所示操作123456789[elastic@escluster ~]$ jps24739 Elasticsearch12215 BytemanDemo25128 Elasticsearch24874 Elasticsearch12251 Jps[elastic@escluster ~]$ bminstall.sh -b -Dorg.jboss.byteman.transform.all 12215[elastic@escluster ~]$ bmsubmit.sh -l ShowLocalVar.btminstall rule trace line local var bminstall.sh并没有加载任何规则脚本，只是开启了agent监听器。然后就可以通过bmsubmit.sh提交规则。在将规则脚本sumbit或者unsubmit到程序中，会看到程序的行为被动态修改了。返回到第一个终端，继续输入测试语句1234567[elastic@escluster ~]$ java BytemanDemoByteman agent in linuxprogram confirm Byteman agent in linuxSetting org.jboss.byteman.transform.all=Byteman agent in linux after install and submit*** transfer value is : Byteman agent in linux after install and submit ***program confirm Byteman agent in linux after install and submit Perfect规则生效了，无论是Windows还是Linux，Byteman都可以完美地工作。 Byteman其它命令Byteman的启动方式非常多，个人精力有限，我不能一一介绍，想要了解更多的信息，直接去翻阅压缩包中的docs/byteman-programmers-guide.pdf用户指南，上面有详细地介绍。 在解压的Byteman的bin目录下，有非常多的脚本，简单介绍如下所示 bmcheck 在注入规则文件之前，该命令可以在线下对你的规则脚本进行解析和类型检查 bminstall 安装agent到一个正在运行的程序中 bmjava 该脚本包装了-javaagent选项。它的用法很像java命令，但是它能以-javaagent script:选项的方式接受Byteman规则脚本。并且自动以boot:的方式绑定了Byteman的Jar文件 bmsetenv 该脚本用来设置环境，agent对配置其行为的各种环境设置非常敏感 bmsubmit 上传和卸载规则脚本]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Byteman</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BTrace 使用教程]]></title>
    <url>%2F2017%2F09%2F22%2Fbtrace-uses-tutorials%2F</url>
    <content type="text"><![CDATA[背景在日常开发中，有一些常见的环境，比如Dev、UAT、预发、生产等，当然并不是每个公司都是这样。有时候开发环境一切正常，但是到线上的UAT环境或预发等等会出现各种问题，那么你是不是经常需要进行本地修改代码、提交、编译、打包、上传、运行、查看日志等这一系列步骤呢？这种方式不仅低效、繁琐而且容易引入诸多不可控的因素，比如你在任意一个环节出现问题，可能都会影响到程序最终的运行结果。而如果能有一种神器，可以对正在运行的程序，进行动态追踪、错误诊断、性能剖析等，是不是无形中为你延长了生命呢？如果你之前不知道也就罢了，然而如果你看到这里了，却还不学习的话，就是你自己的锅了。 Java运行时追踪工具常见的动态追踪工具有BTrace、HouseMD（该项目已经停止开发）、Greys-Anatomy（国人开发，个人开发者）、Byteman（JBoss出品），注意Java运行时追踪工具并不限于这几种，但是这几个是相对比较常用的，本文主要介绍BTrace。 BTraceBTrace简介BTrace是SUN Kenai云计算开发平台下的一个开源项目，旨在为java提供安全可靠的动态跟踪分析工具。先看一下BTrace的官方定义 BTrace is a safe, dynamic tracing tool for the Java platform. BTrace can be used to dynamically trace a running Java program (similar to DTrace for OpenSolaris applications and OS). BTrace dynamically instruments the classes of the target application to inject tracing code (“bytecode tracing”) 简洁明了，大意是一个Java平台的安全的动态追踪工具。可以用来动态地追踪一个运行的Java程序。BTrace动态调整目标应用程序的类以注入跟踪代码（“字节码跟踪”）。 动手之前再了解一下BTrace的主要术语 Probe Point: “location” or “event” at which a set of tracing statements are executed. Probe point is “place” or “event” of interest where we want to execute some tracing statements.（探测点，就是我们想要执行一些追踪语句的地方或事件） Trace Actions or Actions: Trace statements that are executed whenever a probe “fires”.（当探测触发时执行追踪语句） Action Methods: BTrace trace statements that are executed when a probe fires are defined inside a static method a class. Such methods are called “action” methods.（当在类的静态方法中定义了探测触发时执行的BTrace跟踪语句。这种方法被称为“操作”方法。） 安装BTrace目前，BTrace已经托管在Github上了，主页在这里，下载地址在这里，目前最新版本是V1.3.9。新建环境变量BTRACE_HOME值为E:/btrace-bin-1.3.9，然后编辑Path变量，在值的末尾追加;%BTRACE_HOME%/bin即可，验证是否安装成功，打开cmd，输入btrace，显示如下则证明配置成功 Usage: btrace where possible options include: –version Show the version -v Run in verbose mode -o The path to store the probe output (will disable showing the output in console) -u Run in trusted mode -d Dump the instrumented classes to the specified path -pd The search path for the probe XML descriptors -classpath Specify where to find user class files and annotation processors -cp Specify where to find user class files and annotation processors -I Specify where to find include files -p Specify port to which the btrace agent listens for clients -statsd Specify the statsd server, if any 根据上面的提示，btrace使用起来很简单，而且官方提供了一个简易的使用指南，在解压下载的压缩包中E:/btrace-bin-1.3.9/docs下有usersguide.html，用浏览器打开即可。BTrace支持四种方式的注解，分别是 Method Annotations @com.sun.btrace.annotations.OnMethod @com.sun.btrace.annotations.OnTimer @com.sun.btrace.annotations.OnError @com.sun.btrace.annotations.OnExit @com.sun.btrace.annotations.OnEvent @com.sun.btrace.annotations.OnLowMemory @com.sun.btrace.annotations.OnProbe Argument Annotations @com.sun.btrace.annotations.Self @com.sun.btrace.annotations.Return @com.sun.btrace.annotations.CalledInstance @com.sun.btrace.annotations.CalledMethod Field Annotations @com.sun.btrace.annotations.Export @com.sun.btrace.annotations.Property @com.sun.btrace.annotations.TLS Class Annotations @com.sun.btrace.annotations.DTrace @com.sun.btrace.annotations.DTraceRef @com.sun.btrace.annotations.BTrace 关于这些注解的具体解释可以去翻看docs目录下的用户指南，好了，废话不多说，下面简单操练起来。 使用示例如果是在Maven项目中开发，那么首先需要引入BTrace的Jar包，由于Maven的中央仓库中只有1.x版本的BTrace，并没有高版本的，所以一般的做法是自己编译BTrace源码，将高版本的Jar发布到私服（Nexus）中，为简单起见，此处通过Maven指定依赖本地Jar即可，修改pom.xml123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-agent&lt;/artifactId&gt; &lt;version&gt;1.3.9&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;E:/btrace-bin-1.3.9/build/btrace-agent.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-boot&lt;/artifactId&gt; &lt;version&gt;1.3.9&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;E:/btrace-bin-1.3.9/build/btrace-boot.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-client&lt;/artifactId&gt; &lt;version&gt;1.3.9&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;E:/btrace-bin-1.3.9/build/btrace-client.jar&lt;/systemPath&gt;&lt;/dependency&gt; 首先编写想要追踪的示例，如下123456789101112131415161718192021import java.util.concurrent.TimeUnit;public class BTraceOnMethodDemo &#123; public static void main(String[] args) &#123; try &#123; TimeUnit.SECONDS.sleep(15); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("start main method..."); new Thread(() -&gt; &#123; while (true) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125; 再编写追踪代码示例，如下123456789101112import com.sun.btrace.annotations.BTrace;import com.sun.btrace.annotations.OnMethod;import static com.sun.btrace.BTraceUtils.println;@BTracepublic class Tracer &#123; @OnMethod(clazz = "java.lang.Thread", method = "start") public static void onThreadStart() &#123; println("tracing method start"); &#125;&#125; 注意点 Tracer类中的println方法并不是JDK中的方法，而是BTraceUtils中的静态方法 在追踪代码，原则上只能使用BTraceUtils中的静态方法，如果想要使用JDK中的方法，那么在命令行中需要使用-cp指定依赖的Jar OnMethod这个注解只会在方法启动的时候被触发，如果该方法已经启动，再运行追踪代码是无法触发的，所以在示例中，先休息了15s钟 运行BTraceOnMethodDemo，打开cmd，到Tracer类所在的目录下，运行 D:/Demo/src/main/java&gt;jps39808 Launcher15480 RemoteMavenServer40280 Jps41880 AppMain4764D:/Demo/src/main/java&gt;btrace -v 41880 Tracer.javaDEBUG: assuming default port 2020DEBUG: assuming default classpath ‘.’DEBUG: compiling Tracer.javaDEBUG: compiled Tracer.javaDEBUG: attaching to 41880DEBUG: checking port availability: 2020DEBUG: attached to 41880DEBUG: loading E:\btrace-bin-1.3.9\build\btrace-agent.jarDEBUG: agent args: port=2020,statsd=,debug=true,bootClassPath=.,systemClassPath=C:\Program Files\Java\jdk1.8.0_45\jre/../lib/tools.jar,probeDescPath=.DEBUG: loaded E:\btrace-bin-1.3.9\build\btrace-agent.jarDEBUG: registering shutdown hookDEBUG: registering signal handler for SIGINTDEBUG: submitting the BTrace programDEBUG: opening socket to 2020DEBUG: setting up client settingsDEBUG: sending instrument commandDEBUG: entering into command loopDEBUG: received com.sun.btrace.comm.OkayCommand@396f6598DEBUG: received com.sun.btrace.comm.RetransformationStartNotification@394e1a0fDEBUG: received com.sun.btrace.comm.OkayCommand@27a5f880DEBUG: received com.sun.btrace.comm.MessageCommand@53f65459DEBUG: received com.sun.btrace.comm.MessageCommand@3b088d51tracing method start 注意要带上-v参数，否则控制台看不到任何输出，另外还可以利用-o参数将信息输出到指定的文件，运行BTraceOnMethodDemo，打开cmd，到Tracer类的目录下，运行 D:/Demo/src/main/java&gt;jps12064 Jps24560 AppMain41764 Launcher15480 RemoteMavenServer4764D:/Demo/src/main/java&gt;btrace -o out.csv 24560 Tracer.java 注意这时候out.csv文件时在Tracer.java所在目录的根目录下，也就是在D:/Demo下，在当前目录下是找不到的，是不是很变态。找到out.csv打开看看，就是追踪代码的输出内容 BTrace Log: 17-9-20 下午3:16tracing method start 好了，整个流程就打通了，剩下的就是自己动手实战吧。此处仅给出一个简单示例，详情可以参看BTrace的用户指南，里面给出了更多详细的示例，只要打开动手一一实战即可。 BTrace虽然功能强大，但是并不完美，这是因为它有着诸多的限制，例如 can not create new objects. can not create new arrays. can not throw exceptions. can not catch exceptions. can not make arbitrary instance or static method calls - only the public static methods of com.sun.btrace.BTraceUtils class may be called from a BTrace program. can not assign to static or instance fields of target program’s classes and objects. But, BTrace class can assign to it’s own static fields (“trace state” can be mutated). can not have instance fields and methods. Only static public void returning methods are allowed for a BTrace class. And all fields have to be static. can not have outer, inner, nested or local classes. can not have synchronized blocks or synchronized methods. can not have loops (for, while, do..while) can not extend arbitrary class (super class has to be java.lang.Object) can not implement interfaces. can not contains assert statements. can not use class literals. BTrace命令详解 btrace功能：用于运行BTrace跟踪程序。命令格式：btrace [-I &lt;include-path&gt;] [-p &lt;port&gt;] [-cp &lt;classpath&gt;] &lt;pid&gt; &lt;btrace-script&gt; [&lt;args&gt;]示例：btrace -cp build/ 1200 AllCalls1.java参数含义：include-path指定头文件的路径，用于脚本预处理功能，可选；port指定BTrace agent的服务端监听端口号，用来监听clients，默认为2020，可选；classpath用来指定类加载路径，默认为当前路径，可选；pid表示进程号，可通过jps命令获取；btrace-script即为BTrace脚本；btrace脚本如果以.java结尾，会先编译再提交执行。可使用btracec命令对脚本进行预编译。args是BTrace脚本可选参数，在脚本中可通过$和$length获取参数信息。 btracec功能：用于预编译BTrace脚本，用于在编译时期验证脚本正确性。btracec [-I &lt;include-path&gt;] [-cp &lt;classpath&gt;] [-d &lt;directory&gt;] &lt;one-or-more-BTrace-.java-files&gt;参数意义同btrace命令一致，directory表示编译结果输出目录。 btracer功能：btracer命令同时启动应用程序和BTrace脚本，即在应用程序启动过程中使用BTrace脚本。而btrace命令针对已运行程序执行BTrace脚本。命令格式：btracer &lt;pre-compiled-btrace.class&gt; &lt;application-main-class&gt; &lt;application-args&gt;参数说明：pre-compiled-btrace.class表示经过btracec编译后的BTrace脚本。application-main-class表示应用程序代码；application-args表示应用程序参数。该命令的等价写法为：java -javaagent:btrace-agent.jar=script=&lt;pre-compiled-btrace-script1&gt;[,&lt;pre-compiled-btrace-script1&gt;]* &lt;MainClass&gt; &lt;AppArguments&gt; BTrace基本就介绍完了，但是BTrace并不是完美的，比如当你想要追踪一个局部变量的，查看具体值的时候，却无能为力，不仅扼腕叹息，真是天妒英才啊，这么小的一个需求都无法cover？不用着急，后面就介绍一个更加强大的工具，Byteman。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>BTrace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《金字塔原理》]]></title>
    <url>%2F2017%2F07%2F09%2FRead-the-minto-pyramid-principle%2F</url>
    <content type="text"><![CDATA[作者简介：芭芭拉•明托，麦肯锡公司第一位女性咨询顾问。明托是哈佛商学院录取的第二批女学员之一，毕业时被国际知名管理咨询公司——麦肯锡聘请为该公司有史以来的第一位女性咨询顾问。她在写作方面的优势很快得到麦肯锡高层的赏识，负责提高麦肯锡公司员工的写作能力，致力于探索条理清晰的文章所必需的思维结构。本书在1973年首次出版，当即引起轰动，畅销欧美市场30年不衰，书中传授的方式与技巧，被公认为即便对于最有经验的管理者来说也是十分关键的。 表达的逻辑为什么要用金字塔结构文章中的思想必须符合以下规则： 纵向：文章中任一层次上的思想必须是其下一层次思想的概括 横向：每组中的思想必须属于同一逻辑范畴 横向：每组中的思想必须按逻辑顺序组织 组织思想基本上只可能有4种逻辑顺序： 演绎顺序：大前提、小前提、结论 时间顺序：第一、第二、第三 结构顺序：波士顿、纽约、华盛顿 程度顺序：最重要、次重要、等等 金字塔内部的结构利用金字塔中的子结构，能够加快你梳理思想的过程： 主题与子主题之间的纵向关系 各子主题之间的横向关系 序言的叙述方式 如何构建金字塔自上而下法构建金字塔的步骤： 提出主题思想 设想受众的主要疑问 写序言：背景-冲突-疑问-回答 与受众进行疑问/回答式对话 对受众的新疑问，重复进行疑问/回答式对话 自下而上思考： 列出你想表达的所有思想要点 找出各要点之间的逻辑关系 得出结论 序言的具体写法文章的序言（前言、引言、导言）概括读者已知的信息，并将这些信息与文章将要回答的疑问建立联系，然后作者就可以将全部精力放在提供回答疑问的答案上。如果是写文章，就是“序言”；如果是演讲，就是“开场白”。 文章的序言必须用讲故事的形式，也就是说，序言必须先介绍读者熟悉的某些“背景”，说明发生的“冲突”，并由此引发读者的“疑问”，然后针对该“疑问”给出“答案”。这种讲故事的形式对于组织读者已知的信息非常有用。你一旦掌握了这种方法，就能够迅速构思出较短文章的结构。 之所以序言要用讲故事形式，是为了让读者抛开复杂的思想，专注于你的话题。激发读者兴趣，吸引注意力：新奇、悬念、与读者本人相关。 序言应当介绍4要素： 介绍背景 指出冲突 引发疑问 给出答案 演绎推理与归纳推理演绎是一种线性的推理方式，最终是为了得出一个由逻辑词“因此”引出的结论。归纳推理是将一组具有共同点的事实、思想或观点归类分组，并概括其共同性或论点。在演绎过程中，每个思想均由前一个思想导出；而在归纳过程中则不存在这种关系。 演绎推理非常繁琐，主要是因为演绎推理必须从简单明了的思想推导出复杂的思想，所以在写作过程中不要过多的使用演绎推理。而归纳与此相反，归纳法先说明如何做到，然后再详细阐述为什么，在实际情况中，归纳法往往效果更好，这是因为先说明行动，后说明原因，而读者往往最关心的就是那些行动。 如果把文章想象成金字塔的多层级结构，那么在较低的层次上使用演绎推理是可以的，但是在较高的层次上，归纳法总是比演绎法更容易理解，为什么这样讲？如果读者必须读完十几页，才能找到演绎推理的第一步和第二步之间的关系，又必须再读十几页才能找到第二步和第三步之间的关系，那么读者就无法快速理解这次演绎推理。 如果准备在金字塔结构的较低层次上使用演绎推理，需要注意如下两点 演绎推理的过程不要超过4个步骤 推导出的结论不要超过两个 演绎推理需要完成3个步骤 阐述世界上已存在的某种情况 阐述世界上同时存在的相关情况。如果第二个表述是针对第一个表述的主语或谓语的，则说明这两个表述相关 说明这两种情况同时存在时隐含的意义 演绎推理与归纳推理的区别 演绎推理，第二点是对第一点主语或谓语的论述 归纳推理，同组中的思想具有类似的主语或谓语 思考的逻辑应用逻辑顺序所有列入同一组中的思想必须具有某种逻辑顺序，这条规则可以保证你列入同一组中的思想确实属于这一组，还可以防止你遗漏任何相关的思想。常用的顺序主要有三种：时间顺序、结构顺序、程度顺序。 概括各组思想思想的表达方式可以是行动性语句，即告诉读者做什么事；也可以是描述性语句，即告诉读者关于某些事的情况。 概括行动性思想（介绍采取的行动、行为、步骤、流程）时，应说明采取行动后取得的“结果”（效果、达到的目标） 概括描述性思想（介绍背景、信息）时，应说明这些思想具有的“共同点的含义”（共同点的意义） 解决问题的逻辑界定问题判断问题是否存在，通常要看经过努力得到的结果与希望得到的结果之间是否有差距。由某一特定背景导致的某一特定结果，称为非预期结果（R1，Undesired Result）。“问题”是指你不喜欢某一结果，想得到其他结果，称为期望结果（R2，Desired Result）。解决方案则是指如何从现状R1到目标R2。 结构化分析问题分析问题的标准流程是：收集信息-&gt;描述发现-&gt;得出结论-&gt;提出方案。经过一系列有意识、有条理地去收集事实，得出符合逻辑的发现。但是多数人都是收集该领域一切可以收集到的资料，等所有事实和资料齐备时才正式开始分析。这种做法可行，但是需要付出额外的辛苦，比较好的方法是建立诊断框架和逻辑树指导，来分析和引导思维，不仅能提高解决问题的效率，而且简化了把结果构建成金字塔的工作。 结构化分析问题的方法主要是 从信息资料入手 设计诊断框架 使用诊断框架 建立逻辑树 是非问题分析 演示的逻辑在纸面上或屏幕上展示你的思想时，无论采用何种形式，在视觉上，你都需要确保能加强组成金字塔的思想之间的逻辑性和它们之间的相互关系。读者或观众总是先看到逻辑关系的存在，然后才能理解它。因此你要用读者眼睛所看到的来强化她们的头脑所接收的。 在书面上呈现金字塔一般来说，文章较短，读者很容易明白文中的要点及其相互之间的关系。而对于长文，要突出显示文章的框架结构，在页面上呈现出金字塔层级，常见的方法很多，如下所示： 多级标题法 下划线法 数字编号法 行首缩进法 项目符号法 另一个注意点是文章的上下文之间要有过渡，在写完序言后，进入文章正文，需要一段简短的文字，在每组主要思想观点开始或结束的地方稍作铺垫，让读者知道已经论述的和下一步计划论述的内容，同时保证论点和论点之间的连接流畅且不机械。 在PPT演示文稿中呈现金字塔设计PPT演示文稿的最基本原则 文字幻灯片应只包含最重要的、经过适当分组和总结的思想，叙述时应尽量简洁 演示文稿应图文并茂，使用各种图表相配合 演示文稿应呈现经过深思熟虑后的故事梗概和剧本 演示文稿中，理想的图表比例占90%，文字占10% 制作现场演示用的文字幻灯片时，需要强调的是，只有你，演示者，才是表演的明星。房间里所有听众最感兴趣的不是幻灯片，而是你。幻灯片只不过是视觉上的辅助手段，其作用主要是让演示更加生动。 决定一张幻灯片的内容，应牢记以下指导性原则 每次只演示和说明一个论点 论点应使用完整的陈述句，而不是标题性语言 文字应尽量简短 使用简单的词汇和数字 字号应足够大 注意幻灯片的趣味性 用逐级展开呈现，提高趣味性 在字里行间呈现金字塔在理解每个字词的时候，你的头脑中应该逐步描绘出一幅思想图画。当你理解一个又一个的短语和句子时，图画的细节变得更加丰富。你所描绘的图像不是摄影意义上的图像，而是作者所说的“记忆图像”，随着文章的进展而逐渐清晰起来。有关记忆的研究已经证明图像能帮助增加记忆，但同时也表明，忘记和添加哪些细节取决于人们的感情偏好。尽管如此，记忆图像确实能在阅读过程中，帮助你逐字逐句记忆段落和从中提取信息。 以图像形式存储知识非常重要，因为阅读是一个逐字逐句进行的过程，而且我们的头脑只能容纳有限的词汇。通过从文字恢复图像的方法，读者不仅能大量传递知识，使头脑更加有效地处理，而且能用清晰的图像传递信息，便于回忆。 引用威廉·明托教授的一段话来阐释作者与读者或者说听众的关系 写作时，你好像一位司令官，指挥着千军万马，排队通过一个每次只能通过一个人的狭隘关口；而你的读者则在另一边迎接，将部队进行重新编排和组织。无论主题多么大或者多么复杂，你只能以这种方式表达。 最终你会发现，这就是我们在顺序和编排上对读者应尽的义务，以及为什么修辞学者们除了强调修辞得体和别出心裁外，还把顺序和编排当作对那些给予自己厚爱的人的应尽职责。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用RAMDirectory做缓存提高Lucene性能]]></title>
    <url>%2F2017%2F06%2F28%2FUse-RAMDirectory-to-do-the-cache-to-improve-lucene-s-performance%2F</url>
    <content type="text"><![CDATA[RAMDirectory和FSDirectory都继承自BaseDirectory，而BaseDirectory继承自Directory，Directory是Lucene中设计的一个顶层抽象类，可以将其看做本地文件系统的一个目录。 RAMDirectory是基于内存实现的，具有较高的存储速度，但是受到内存大小的限制，而FSDirectory是基于文件系统实现的，针对不同的操作系统有不同的具体实现类，这些实现类无需用户操心，只需要调用FSDirectory.open(Path path)方法，它就会帮助我们选择最适合的子类，FSDirectory的瓶颈在于磁盘I/O。 如果机器内存足够大的话，那么组合使用RAMDirectory和FSDirectory将能够极大地提高Lucene的性能。组合使用两者的应用场景很多，不同的场景可以分别解决不同的需求，仅列举如下 批量索引，而无需检索的情况下，先把document存到RAMDirectory中，当达到一定数量之后，再把这些索引一次性加入FSDirectory里 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.StringField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;import java.nio.file.Paths;/** * &lt;p&gt; * Created by wangxu on 2017/06/19 15:31. * &lt;/p&gt; * &lt;p&gt; * Description: 基于Lucene 6.5开发 * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class RamFSDirectoryDemo &#123; public static void main(String[] args) throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriter ramWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new WhitespaceAnalyzer()).setOpenMode(IndexWriterConfig.OpenMode.CREATE)); FSDirectory fsDirectory = FSDirectory.open(Paths.get("F:/index_RAM_FS")); IndexWriter fsWriter = new IndexWriter(fsDirectory, new IndexWriterConfig(new WhitespaceAnalyzer()).setOpenMode(IndexWriterConfig.OpenMode .CREATE_OR_APPEND)); //简单起见，这里的数量都比较少，例如索引10000个document，每100个document作为一批 int tempCount = 0; for (int i = 0; i &lt; 10000; i++) &#123; Document document = new Document(); document.add(new StringField("key", String.valueOf(i), Field.Store.YES)); ramWriter.addDocument(document); tempCount++; if (tempCount % 100 == 0) &#123; ramWriter.commit(); ramWriter.close(); fsWriter.addIndexes(ramDirectory); ramDirectory.close(); fsWriter.commit(); ramDirectory = new RAMDirectory(); ramWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new WhitespaceAnalyzer()).setOpenMode(IndexWriterConfig.OpenMode.CREATE)); &#125; &#125; //退出时确保RAMDirectory中内容都被加入本地 if (ramWriter != null) &#123; ramWriter.commit(); ramWriter.close(); fsWriter.addIndexes(ramDirectory); ramDirectory.close(); &#125; IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(fsWriter.getDirectory())); fsWriter.close(); int count = indexSearcher.count(new MatchAllDocsQuery()); System.out.println(count); &#125;&#125; 索引的同时需要搜索，前提是内存总量大于索引文件总量，如果要求新加入的索引对搜索可见，即实时搜索，要怎么做呢？显然实时搜索需要writer和searcher共用同一份索引，同时要定时的将内存中索引备份到文件系统，否则机器一旦宕机，内存中所有的索引文件都将丢失。代码实现也很简单，在备份的时候，可以使用调度线程去进行备份操作，同时还不影响主线程继续接受索引请求；备份策略有两种：全量和增量，增量直接比较文件名，将新增文件拷贝到文件系统，同时删除已过时的索引文件即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.StringField;import org.apache.lucene.index.*;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.store.IOContext;import org.apache.lucene.store.RAMDirectory;import java.io.File;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;import java.util.*;import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;/** * &lt;p&gt; * Created by wangxu on 2017/06/19 15:31. * &lt;/p&gt; * &lt;p&gt; * Description: 基于Lucene 6.5开发 * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class RamFSDirectoryDemo &#123; private static IndexWriter ramWriter; private static RAMDirectory ramDirectory; private static final Backup backup = new Backup(); public static void main(String[] args) throws IOException &#123; //将文件系统中索引文件加载到内存中 FSDirectory fsDirectory = FSDirectory.open(Paths.get("F:/index_RAM_FS")); ramDirectory = new RAMDirectory(fsDirectory, IOContext.READONCE); fsDirectory.close(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new WhitespaceAnalyzer()); indexWriterConfig.setIndexDeletionPolicy(new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy())); ramWriter = new IndexWriter(ramDirectory, indexWriterConfig); //先添加一条记录 Document document = new Document(); document.add(new StringField("first", "first", Field.Store.YES)); ramWriter.addDocument(document); ramWriter.commit(); //定时备份线程 ScheduledExecutorService backupThread = Executors.newSingleThreadScheduledExecutor(); backupThread.scheduleAtFixedRate(backup, 0, 2, TimeUnit.SECONDS); //可以继续接受索引请求 document = new Document(); document.add(new StringField("key", "key", Field.Store.YES)); ramWriter.addDocument(document); ramWriter.commit(); //等待索引拷贝完成 try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //接受搜索请求，可实现实时搜索 IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(ramWriter.getDirectory())); int count = indexSearcher.count(new MatchAllDocsQuery()); System.out.println("total hits: " + count); System.out.println("内存中索引文件：" + Arrays.asList(ramDirectory.listAll())); //查看备份索引是否完整，此处有一个注意点，如果需要在备份索引上打开searcher，那么在备份索引文件的时候需要先备份其它文件，最后再备份segments_N文件 //因为开searcher的时候，会先加载segments_N文件，这种方式可以保证加载完segments_N文件之后，再加载其它文件一定成功 fsDirectory = FSDirectory.open(Paths.get("F:/index_RAM_FS")); System.out.println("备份中索引文件：" + Arrays.asList(fsDirectory.listAll())); backupThread.shutdown(); &#125; static class Backup implements Runnable &#123; public void run() &#123; IndexWriterConfig config; SnapshotDeletionPolicy indexDeletionPolicy = null; IndexCommit snapshot = null; try &#123; ramWriter.commit(); config = (IndexWriterConfig) ramWriter.getConfig(); indexDeletionPolicy = (SnapshotDeletionPolicy) config.getIndexDeletionPolicy(); snapshot = indexDeletionPolicy.snapshot(); config.setIndexCommit(snapshot); Collection&lt;String&gt; fileNames = snapshot.getFileNames(); Path path = Paths.get("F:/index_RAM_FS"); //全量增量任选一种即可 boolean b = incrementalBackup(fileNames, path); //boolean b = fullBackup(fileNames, path); if (!b) &#123; System.err.println("Backup occurs error!"); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); System.err.println(e.getMessage()); &#125; finally &#123; try &#123; indexDeletionPolicy.release(snapshot); &#125; catch (IOException e) &#123; e.printStackTrace(); System.err.println(e.getMessage()); &#125; &#125; &#125; private boolean fullBackup(Collection&lt;String&gt; fileNames, Path path) &#123; Objects.requireNonNull(path); Directory to; try &#123; to = FSDirectory.open(path); // 全量备份，直接清空拷贝 for (File file : path.toFile().listFiles()) &#123; file.delete(); &#125; for (String fileName : fileNames) &#123; to.copyFrom(ramDirectory, fileName, fileName, IOContext.DEFAULT); &#125; to.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); return false; &#125; return true; &#125; private boolean incrementalBackup(Collection&lt;String&gt; fileNames, Path path) &#123; // 增量备份，稍微复杂一些，比较文件名，将ramDirectory中新增索引拷贝过去，同时将ramDirectory中不存在的索引但是在path中存在的旧索引删除 Objects.requireNonNull(path); //fileNames被IndexCommit引用，需要重新构造set集合，进行移除操作 Set&lt;String&gt; files = new HashSet&lt;&gt;(fileNames); for (File file : path.toFile().listFiles()) &#123; if (files.contains(file.getName())) &#123; //该索引已存在，则不拷贝 files.remove(file.getName()); &#125; else &#123; //删除已经过时的索引 file.delete(); &#125; &#125; //拷贝全部新增索引 try &#123; Directory to = FSDirectory.open(path); for (String file : files) &#123; to.copyFrom(ramDirectory, file, file, IOContext.DEFAULT); &#125; to.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); return false; &#125; return true; &#125; &#125;&#125; 如果内存不够大，不能够存放全部的本地文件系统索引，那么如何实现呢？这时候，需要两套IndexWriter，一套用来处理本地文件系统中的索引，另一套在内存中可以接受新的索引请求；对索引的操作无非就是新增、删除、更新这三种，下面分别论述 删除索引： 对于删除操作比较简单，两个IndexWriter都执行一次即可，如果其中一个IndexWriter中不存在待删除的索引的话，那么对索引文件不会有任何影响；这时候又分两种情况，一是待删除的索引在内存中，那么删除-提交-重新打开索引，耗时很短；二是待删除索引在硬盘上，这时候删除-提交，到底要不要重新打开，需要视业务而定，因为硬盘上的索引通常是很大的，如果频繁删除频繁重新打开的话，是很耗性能的，而如果不重新打开，这时候删除的索引对searcher是不可见的，也就是说用户仍然可以搜索到已删除的索引，比较好的方式是用一个后台线程去定时的重新打开硬盘索引，然后更新searcher即可，但是这样对于删除操作无法做到准实时，会有一定的延时。 新增索引：所有的新增操作都使用内存中的IndexWriter（称为A），然后提交-重新打开索引-更新searcher，一切都是在内存中操作，耗时极短，对于实时搜索是解决了，而且Lucene中已经提供了MultiReader可以组合多个子reader，很适合这种情况；但是当内存中索引尺寸达到一定大小之后，需要将其合并到文件系统中；在合并的时候还需要另外再开一个内存中的IndexWriter（称为B）用来接受索引合并期间的索引新增操作。不过这种方式实现起来很复杂，需要处理很多问题，例如在内存索引 A 和硬盘索引合并期间如果有更新删除操作怎么处理？在合并之后需要更新MultiReader，但是旧的MultiReader上面还挂有搜索请求怎么办？在新开一个内存索引 B 之后，如何让MultiReader覆盖住它？因为在 B 中必须先有索引，然后才能开Reader，进而才能更新MultiReader；若新增索引速度实在太快，在合并过程没有完成的时候，内存索引又满了，要怎么办？除此以外还有很多问题需要考虑。 更新索引：更新其实可以看作是两步操作，先删除后新增，这两步可以借鉴上面的论述。更新操作是无法同时应用于两个IndexWriter的，因为在Lucene中更新的逻辑是这样的，如果存在则更新，如果不存在则新增，那么假设一个IndexWriter中存在，另一个不存在，如果两个都应用更新，那么最后的结果很简单就是存在两份一样的document了。 个人理解优雅的程序应该是简单的，越是简单的程序其实越是健壮，这种方案实现起来很复杂，即使这么复杂的程序实现了，其健壮性仍然值得担忧，所以不是特别推荐使用这种方案。比较好的方案当然是上Elasticsearch集群了，Elasticsearch的刷新时间默认是1s钟，也就是说，最迟1s之后，就可以看到新的数据。当然如果非要针对Lucene进行开发，可以参考Linked公司开源的Zoie搜索引擎。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven引入本地依赖Jar到可执行Jar包中]]></title>
    <url>%2F2017%2F06%2F13%2FMaven-introduces-local-dependency-jar-to-executable-jar-packages%2F</url>
    <content type="text"><![CDATA[在Maven中，默认地，是不会将依赖的Jar包打入可执行Jar包的，如果需要将依赖打入可执行Jar包，需要在pom中添加maven-assembly-plugin插件，这个很容易实现，但是在正规开发中不推荐这样使用，为什么？因为稍微大型一些的项目都至少有几十个依赖项，而每次打包都将这些Jar包打入可执行Jar，使得最后生成的可执行Jar体积非常大。标准的做法是，将所有的依赖Jar包都打入lib目录中，而在可执行Jar的MANIFEST.MF中指定lib路径即可。这也很容易实现，并不是本文的重点，本文的重点是如何将不在Maven中央仓库中的Jar包，或者说依赖本地的Jar包打入可执行Jar，并更新MANIFEST.MF文件。 例如在我的Maven项目中，需要依赖本地Jar，首先将依赖的Jar复制到src/main/resources/lib目录下，引用如下1234567&lt;dependency&gt; &lt;groupId&gt;com.yuewen&lt;/groupId&gt; &lt;artifactId&gt;lucene&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHORT&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/lib/lucene-1.0.0-SNAPSHORT.jar&lt;/systemPath&gt;&lt;/dependency&gt; 这里的scope只能是system范围，systemPath属性指定Jar包的路径。 下一步将所有依赖的Jar包打入lib目录中，方式如下123456789101112131415161718192021222324&lt;!--将依赖的资源全部打入lib目录--&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 至此，你在Maven项目中，依赖的所有Jar都被打入到target/lib目录下了，剩下的关键一步就是如何添加MANIFEST.MF文件了。在pom中添加如下插件 1234567891011121314151617181920212223&lt;!--打包插件，在Jar包中添加Class-Path和Main-Class--&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;!--使用自己的Manifest文件，运行正常--&gt; &lt;!--&lt;manifestFile&gt;src/main/resources/META-INF/MANIFEST.MF&lt;/manifestFile&gt;--&gt; &lt;!--使用插件添加的Manifest文件，运行正常，一定要注意Manifest中jar包名称和lib文件夹下jar包名称版本号后缀等一定要一致，否则找不到依赖jar，此处有坑--&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;!--指定依赖资源路径前缀--&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;mainClass&gt;cn.codepub.maven.test.Main&lt;/mainClass&gt; &lt;/manifest&gt; &lt;!--可以把依赖本地系统的Jar包加入Manifest文件中--&gt; &lt;manifestEntries&gt; &lt;Class-Path&gt;lib/lucene-1.0.0-SNAPSHORT.jar&lt;/Class-Path&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;/configuration&gt;&lt;/plugin&gt; 运行mvn clean package执行打包，完成之后，将包含依赖资源的lib目录和可执行Jar放在同一级目录下即可，这样在运行java -jar xxx.jar的时候，可执行Jar包可以准确地找到依赖Jar包。并且以这种方式打出来的可执行Jar体积非常小，一般都是几百KB而已。完整的MANIFEST.MF文件如下所示 1234567Manifest-Version: 1.0Built-By: wangxuClass-Path: lib/lombok-1.16.12.jar lib/guava-20.0.jar lib/log4j-api-2. 7.jar lib/log4j-core-2.7.jar lib/lucene-1.0.0-SNAPSHORT.jarCreated-By: Apache Maven 3.3.3Build-Jdk: 1.8.0_45Main-Class: cn.codepub.maven.test.Main]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《未来简史》]]></title>
    <url>%2F2017%2F06%2F11%2FRead-a-brief-history-of-tomorrow%2F</url>
    <content type="text"><![CDATA[作者简介：尤瓦尔•赫拉利，1976年生，青年怪才，全球瞩目的新锐历史学家，牛津大学历史学博士，耶路撒冷希伯来大学历史系教授，著有国际畅销书《人类简史》。其新作《未来简史》，以宏大视角审视人类未来的终极命运，甫一出版就在全球掀起一股风潮，引起广泛关注。 人类的新议题在这一章中，作者提出在21世纪，人类面临的三大议题似乎是不死、快乐和神性。现今因肥胖而死亡的人数已经远超因饥饿而死亡的人数了；同样因自杀而死亡的人数也已经远超因战争和恐怖主义而死亡的人数了，以目前的科技向前推进，在可以想象的未来，也许不死或者说永生可以成为一部分富人可选的一种生活方式呢。至于快乐不仅仅是物质上的，当然也包括精神上的，在这本书中，作者似乎特别喜欢谈动物，比如说松鼠储藏各类坚果不就是为了满足内心的快乐吗？这和人类储藏房产、纸币等等不是异曲同工吗？其实所谓的欲壑难填是有缘由的，在得到快感时，我们的反应不是满足，反而是想得到更多。这就不断的促使我们去持续地追求能够给我们带来快感的东西。在追求幸福和不死的过程中，人类事实上是试着把自己提升到神的地位。这时候的神不仅仅是宗教崇拜中那高高在上的神了，当人类自己能够实现永生并且可以获取无穷无尽的快乐，那么他们的心中估计已经把自己看作是地球的神了，那个时候，也许宗教就自然而然消逝了。 人类世为什么叫人类世，很奇怪的一个名字，科学家将地球的历史分为不同的“世”，例如更新世、上新世和中新世。过去的七万年称为“人类世”，也就是人类的时代。原因在于这几万年来，人类已经成为全球生态变化唯一最重要的因素。现在拜人类所赐，全球大型动物（体重不只是几公斤）有超过90%不是人类就是家畜。全球大型动物数量中，人类约有3亿吨，野生大型动物约有1亿吨，家畜约有7亿吨。好莱坞大片中经常会以外星人入侵啦、或者行星撞击地球啦为噱头，其实完全不必担心，这样的风险小之又小。与其害怕小行星，还不如害怕人类自己。在过去不过短短一世纪，人类造成的影响可能已经超过6500万年前那颗灭绝恐龙的小行星。 应该说，对地球上除了人类来说的所有其他物种来说，人类是最危险的杀手。在智人全球迁徙的过程中，他们灭掉了所有其他人类物种（现在的人类主要是智人，当年除了智人，还有很多人种）、澳大利亚90%的大型动物、美国75%的大型哺乳动物、全球大约50%的大型陆上哺乳动物；而且此时他们甚至还没开始种小麦，还没开始制作金属工具、还没开始写下任何文字，也还没铸出任何钱币。 人类的特殊之处这一章非常有趣，按理说啊，人类主宰世间万物，那么人类必然有优越于其他所有物种的地方，大多数人都坚信，人类拥有灵魂和心灵，而其他劣等动物并不具备这两者，这在科学上被证明了是胡诌，因为科学家无论怎样实验都无法证明人类拥有灵魂。而心灵和灵魂大不相同，心灵是脑中主观体验的流动。这些心理上的体验，就是各种紧密相连的感觉、情感和思想，忽闪忽现、立刻消失，接着其他体验又倏然浮现与消散，于电光火石间来了又去。把这种种体验集合起来，就构成了意识流。灵魂有人信有人不信，但就意识流来说，几乎所有人都相信。但是在这一点上，所有动物并不输于人类，所有哺乳动物和鸟类以及至少某些爬行动物和鱼类，都具有感觉和情感。就这两点来说，人类和其他各种动物其实是平等的。 关于意识这个东西，不仅人类无法自证，而且无法他证，什么意思？假设人工智能不断发展，有一天人工智能说自己有意识，我们应该相信吗？早在几千年前，哲学家就已经发现，没有办法明确证明除了自己以外的任何事物具有意识。根据目前的科学定论，我所体验到的一切都是脑活动的结果，所以理论上确实能够模拟出一个我完全无法与“真实”世界分辨的虚拟世界。这不就是各种脑洞大开的电影里所展现的吗，甚至我们其实已经是活在营养液中而不自知呢，我们所感受到的驱壳等等一切行为感受，都是那个泡在营养液的大脑里的神经元之间不断流动的电子所致呢？所有的生老病死等等其实都是人工智能虚拟出来的结果呢？ 在这里作者点出，人类真正胜出的地方在于大规模且灵活的合作，这种合作与蜂群和象群、黑猩猩有所不同，蜜蜂虽然也能进行大规模的合作，但是它们缺乏灵活性，比如无论有多么严重的压迫与剥削，它们从不会把自己的蜂后送上断头台，进而建立共和国；而象群与猩猩很难进行陌生者之间的合作，它们的合作多是基于血缘关系或者是熟知关系，两只不认识的大象，别说进行合作，单是一只大象融入另一个陌生的象群都是难上加难，而只有智人才能进行这种大规模的无数陌生个体之间进行的合作。正是这种实际具体的能力，决定了为何目前主宰地球的是人类，而不是什么永恒的灵魂或是独有的意识。 关于革命的精髓也是合作，以俄国爆发的十月革命为例，当时俄国中上阶层人数至少有300万人，但共产主义者仅有23000名，然而就是这么一小撮人建立的强大的苏维埃政权。在十月革命之前，俄国有1.8亿农民和工人，仅有300万俄国贵族、官员等，但是这些精英却知道如何合作守卫其共同利益，但那1.8亿平民却无法有效动员，直到23000名共产主义者的出现。想掀起一场革命，只靠人数绝对远远不够。 智人统治世界，是因为只有智人能编织出互为主体的意义之网：其中的法律、约束力、实体和地点都只存在于他们共同的想象之中。这张网，让所有动物中只有人类能组织十字军、革命和人权运动。除此以外，另一个特殊之处是在所有动物中只有人类能够想象虚幻的东西的存在，例如猎豹埋伏着等待捕获猎物，但是它仅能想象这个世界上真实存在的东西例如羚羊，却无法想象不存在的东西，例如美元、谷歌或欧盟，如果它知道美元能买到大把的羚羊，也许它就不会费那么大力气去亲自捕猎了。到了21世纪，虚构想象有可能成为世界上最强大的力量，甚至超越自然选择。因此，如果我们想了解人类的未来，只是破译基因组、处理各种数据数字还远远不够，我们还必须破解种种赋予世界意义的虚构想象。 说书人所有的现存世界，其立身之本都是依赖于人类虚构出来的各种概念，而且这就像既得利益阶层一样，每一个依靠这种虚构概念上位的人最后都会挺身而出继续维护这种虚构的权威性。虚构有其存在的必要性，正是所有人都在同一个虚构故事的体系之内，智人才能进行更加复杂的合作，因为所有人共同认同的价值体系，为内部的融合以及建立组织提供了先决条件，而动物世界就算是最大的族群也达不到人类组织一场现代化战争的规模，或者说，即便动物世界存在这样的族群，那么其必然无法进行有效的组织，结果就是乱作一团，每只动物都可以自作主张，毫无秩序可言。 在21世纪，我们还会创造出比以往更强大的虚构概念以及更极权的宗教。在生物科技和计算机算法的帮助下，这些宗教不但会控制我们每分每秒的存在，甚至将塑造我们的身体、大脑和心智，创造出完整的虚拟世界。真要如此，在区分虚构与真实、宗教与科学将会变得更加困难，但又比以往更加重要。也许在不久的世纪里，人工智能统治世界，而人类的大脑仅仅存在于人工智能所为其构建的完美的虚拟世界之中。 一对冤家这里说的一对冤家指的是科学与宗教。在人类的发展进程中，科学与宗教有合作有冲突；科学与宗教追求的都是真理，只是有时候它们追求的是各自推崇且不同的真理。但另外一些时候，科学或宗教都不那么在乎真理，因此二者十分容易妥协、共存甚至合作。宗教最在乎的是秩序，宗教的目的就是创造和维持社会结构；而科学最在乎的则是力量，科学的目的是通过研究得到力量。 与“现代”的契约如今，经济增长在全球已经取得了宗教般的地位，不管一个国家或地区存在什么样的问题，所有的当权者或者领导人都寄希望于经济增长，认为只要经济增长就能解决潜在的问题，包括贫困、落后、纷争、混乱等。所有的领导人都把经济的发展成果看成是一张大饼，只要尽量把这张饼摊得越大就能养活更多的人，所以理所应当的尽一切力量去推动经济的发展。过于关注摊大饼的结果就是，在发展的过程中忽略了其它许多重要的东西，例如维持社会平等、确保生态和谐、维持底层群众的上升通道等。 科学的力量只是在近代才获得了最完美的体现，其实早期的人类发展中，之所以无法用科学来推动增长，是因为大家误以为各种宗教经典和古老传统里已经提供了世界上所有的重要知识。近代科学真正爆发的原因就是，科学让人类发现了自己的无知。人类发现自己对这个真实的世界所知甚少，就突然有了很好的理由去追求新知识，开启了用科学推动进步的道路。 人文主义革命人文主义把意义和权威的源头从天上转移到人类的内心，这个宇宙的本质也随之改变了。人文主义者认为生命就是一种内在的渐进变化过程，靠着经验，让人从无知走向启蒙。人文主义生活的最高目标，就是通过各式智力、情绪及身体体验，充分发展人的知识。人文主义相信自己的“感觉”，于是我们在与现代性的契约中虽然得利，却无须付出代价。所有的规则，以及远古的上帝都从人民的脑中移除，现在对于任何有关价值观的判断，并没有绝对的是非之分，比如古代同性恋会遭到整个社会的鄙视，而现在呢？管他呢，你觉得舒服，那就做吧，你是同性恋，还不能鄙视你，否则就涉嫌歧视了，所有的判断出发点都是人的主观感受。再比如雇佣童工在早期的资本主义世界很是普遍，现在为什么行不通了？因为虚伪的道德主义逼迫着人去同情这些孩子，并从孩子的角度出发去思考，孩子正在玩乐的年纪却要呆在工厂里，简直丧尽天良，而真实情况呢？对于那些家庭极度贫困的来说，童工可能就是他们家庭的最大收入来源了，也未可知呢，这样从人心出发而不论对错的人文主义，占尽了表面的道德高点，却未必是最好的。 实验室里的定时炸弹所谓人类拥有的自由意识，在本书中作者给出了否定，并列举出了详细的证据，所谓的自由意识并不存在，无非是人类自己忽悠自己而已。人类有的就是一条意识流，欲望会在意识流中起伏来去，并没有什么永远不变的自我能够拥有这些欲望。自由意识流的本质不外乎两点，要么来自于生物预设，也就是说基因携带，天生所有；要么来自于随机发生，一切的自由意识并不是提前准备，而在某一个时刻将之释放出来的。说了这么多废话来证明人类没有自由意识到底是为何呢？如果这个证明正确，那么我们就可以利用药物、基因工程或者直接对脑做出刺激，就能操纵甚至控制人的欲望了。因为这并没有违反任何人类的天性，也没有改变人类的自由，所有利用科技手段对大脑实施的刺激控制等等都是合理且合法的。在过往的时期，自由主义还没有受到致命的威胁，现今面临着科技的进步，会给自由主义带来实实在在的挑战。我们即将拥有各种超级实用的设备、工具和制度，但这些设备、工具和制度并不允许个人自由意志的存在。 大分离自由主义面临着三个实际威胁，一是人类将完全不具价值；二是人类整体仍然有价值，但个人将不再具有权威，而是由外部算法来管理；三是有些人仍然会不可或缺，算法系统也难以了解，而且会形成一个人数极少的特权精英阶层，由升级后的人类组成。 如果科学发现和科技发展将人类分为两类，一类是绝大多数无用的普通人，另一类是一小部分经过升级的超人类，又或者各种事情的决定权已经完全从人类手中转移到具备高度智能的算法，在这两种情况下，自由主义都将崩溃。尽管现阶段所有人的共识是经济的发展使得穷人与富人的医疗水平差距在逐渐缩小，但这只是在特殊时代背景下的特殊情况，因为现代还需要大量的健康的士兵和产业工人，一旦科技的发展使得高精尖武器或者机器人取代士兵与产业工人，对于富人阶级来说，再继续维持庞大的底层人民的医疗开支将变成一笔巨大的负担，而这才是真正的转折点，当富人阶层不再需要依赖底层的大量人民的时候，不仅仅是医疗水平，各个方面，富人阶层都将开始起飞，而穷人阶层只能原地踏步，这导致的最终结果就是超人类阶层的出现，而这将会是全部由精英阶层所组成。 意识的海洋这一章详尽地探讨了科技人文主义。科技人文主义希望让人类的心智升级，让我们能够接触到目前未知的经验、目前未闻的意识状态。然而想要足够地了解我们的心智已经是一项极其艰巨的任务，在人类目前所知的范围内，已经掌握的知识犹如沧海一粟，真正的未知世界浩瀚无边，在追求改造心智的过程中，也许我们成功地让身体与大脑都升级了，却在过程中失去了心智。科技人文主义到最后可能会造成人类的降级。 信数据得永生在历史进程中，人类创造了一个全球性的网络，不论面对任何事物，都以它在这个网络中有何功能来给予评价。几千年来，这让人充满了自尊和偏见。人类在这个网络中执行着最重要的功能，也就很容易认为自己是这个网络所有成就的主要功臣，并认为自己就是造物的巅峰。至于其他所有动物，因为它们执行的只是网络中次要的功能，于是其生命和经验都遭到低估；只要动物不再能发挥任何功能，就躲不开惨遭灭绝的命运。然而，一旦人类对网络也不再能发挥重要功能，就会发现自己到头来也不是造物巅峰。我们自己设定的标准，会让我们也走上长毛象和白鱀豚的灭绝之路。到时回首过去，人类也只会成为宇宙数据流里的一片小小涟漪。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用DocumentStoredFieldVisitor提高Lucene检索速度]]></title>
    <url>%2F2017%2F05%2F31%2FImprove-the-retrieval-speed-of-lucene-by-DocumentStoredFieldVisitor%2F</url>
    <content type="text"><![CDATA[FieldSelector提高Lucene检索性能的方法有很多种，这里简单介绍一种常用且便捷可行的方法快速提高Lucene检索性能。在早期的Lucene版本中，使用FieldSelector来决定哪些Fields应该被加载，并以何种方式加载，但是在LUCENE-3309中该接口被废弃，并且提出了新的替代接口StoredFieldVisitor。 FieldCache另一种提高检索性能的方案是使用FieldCache来缓存Lucene的term values信息，不过该接口目前已被移至org.apache.lucene.uninverting包下，并且访问权限变成包级私有，也就是说，用户再也无法直接使用FieldCache了，该接口以后仅限于Lucene内部使用。FieldCache的主要作用是缓存用来排序field的值，Lucene会将需要排序的字段都读到内存中进行排序，所占内存大小和文档数量相关。其替代方案是使用DocValues类。其实深入一步，当你的Document只有一个Token的时候，FieldCache还可以被用来快速获取每个Document的field值，因为Lucene只做了反向索引，这种Document-&gt;field正向索引是极其耗时的，而FieldCache正好能解决这个问题。 由于这两个接口基本相当于被废弃，这里不再赘述，主要讲解目前实用的StoredFieldVisitor方案。 StoredFieldVisitorStoredFieldVisitor是一个抽象类，它有一个唯一对外暴露的实现类DocumentStoredFieldVisitor，查看该实现类的Doc文档说明，可知其作用是支持加载所有的stored fields，或者通过Set集合指定请求的fields。 查看DocumentStoredFieldVisitor构造函数123456789101112131415public DocumentStoredFieldVisitor() &#123; this.fieldsToAdd = null;&#125;public DocumentStoredFieldVisitor(Set&lt;String&gt; fieldsToAdd) &#123; this.fieldsToAdd = fieldsToAdd;&#125;/** Load only fields named in the provided fields. */public DocumentStoredFieldVisitor(String... fields) &#123; fieldsToAdd = new HashSet&lt;&gt;(fields.length); for(String field : fields) &#123; fieldsToAdd.add(field); &#125;&#125; 一个无参构造函数，一个接收Set参数的构造函数，还有一个接收可变参数的构造函数，而可变参数的构造函数中其实就是把可变参数加入Set集合，所以其原理和接收Set集合的构造函数是一样的。 讲了这么多，那么DocumentStoredFieldVisitor的使用场景是什么呢？当用户需要访问各个文档中的某个field的值时，使用IndexSearcher.doc(int docID)可以获得Document，然后再从Document中获得某个域值，当一个Document中field非常多的时候，这种访问速度比较慢，而且只能获得Stored域的值。这时候使用DocumentStoredFieldVisitor可以极大地提高访问速度。下面写个简单的测试代码来看看其性能差距。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136import com.google.common.base.Stopwatch;import org.apache.commons.lang3.RandomStringUtils;import org.apache.lucene.analysis.cjk.CJKAnalyzer;import org.apache.lucene.document.*;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.RAMDirectory;import java.io.IOException;import java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Set;import java.util.concurrent.*;/** * &lt;p&gt; * Created by wangxu on 2017/05/27 17:37. * &lt;/p&gt; * &lt;p&gt; * Description: 基于Lucene 6.5.0实现 * &lt;/p&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 &lt;br/&gt; * WebSite: http://codepub.cn &lt;br&gt; * Licence: Apache v2 License */public class TestStoredFieldVisitor &#123; static IndexSearcher indexSearcher; static Query query; public static void main(String[] args) throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new CJKAnalyzer())); for (int i = 0; i &lt; 1000000; i++) &#123; Document document = new Document(); document.add(new LongPoint("ID", i)); document.add(new StringField("title", i + "title", Field.Store.YES)); document.add(new TextField("content", i + "content", Field.Store.YES)); if (i % 2 == 0) &#123; document.add(new StringField("sex", "male", Field.Store.YES)); document.add(new TextField("tags", "The " + i + "th male!", Field.Store.YES)); &#125; else &#123; document.add(new StringField("sex", "female", Field.Store.YES)); document.add(new TextField("tags", "The " + i + "th female!", Field.Store.YES)); &#125; document.add(new TextField("hobbies", "I like playing the " + i + " toys!", Field.Store.YES)); document.add(new StringField("testField1", RandomStringUtils.randomAlphabetic(10), Field.Store.YES)); document.add(new StringField("testField2", RandomStringUtils.randomAlphabetic(10), Field.Store.YES)); document.add(new StringField("testField3", RandomStringUtils.randomAlphabetic(10), Field.Store.YES)); document.add(new StringField("testField4", RandomStringUtils.randomAlphabetic(10), Field.Store.YES)); document.add(new StringField("testField5", RandomStringUtils.randomAlphabetic(10), Field.Store.YES)); document.add(new StringField("testField6", RandomStringUtils.randomAlphabetic(10), Field.Store.YES)); document.add(new StringField("testField7", RandomStringUtils.randomAlphabetic(10), Field.Store.YES)); document.add(new StringField("testField8", RandomStringUtils.randomAlphabetic(10), Field.Store.YES)); indexWriter.addDocument(document); &#125; indexWriter.commit(); indexWriter.close(); indexSearcher = new IndexSearcher(DirectoryReader.open(ramDirectory)); query = LongPoint.newRangeQuery("ID", 0, Math.addExact(1000000, -1)); int count = indexSearcher.count(query); long total = 0; for (int i = 0; i &lt; 10; i++) &#123; Future&lt;Long&gt; submit = Executors.newSingleThreadExecutor().submit(new Worker1(count));//average time cost by 10 times : 8024 ms //Future&lt;Long&gt; submit = Executors.newSingleThreadExecutor().submit(new Worker2(count));//average time cost by 10 times : 6507 ms try &#123; total += submit.get(); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("average time cost by 10 times : " + total / 10 + " ms"); &#125; static class Worker1 implements Callable&lt;Long&gt; &#123; int count = 0; public Worker1(int count) &#123; this.count = count; &#125; @Override public Long call() throws Exception &#123; Stopwatch started = Stopwatch.createStarted(); List&lt;String&gt; titles = new ArrayList&lt;&gt;(); if (count &gt; 0) &#123; TopDocs docs = indexSearcher.search(query, count); ScoreDoc[] scoreDocs = docs.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; Document doc = indexSearcher.doc(scoreDoc.doc); titles.add(doc.get("title")); &#125; System.out.println("No DocumentStoredFieldVisitor get title counts = " + titles.size()); &#125; long elapsed = started.elapsed(TimeUnit.MILLISECONDS); System.out.println("No DocumentStoredFieldVisitor: " + elapsed + " ms"); return elapsed; &#125; &#125; static class Worker2 implements Callable&lt;Long&gt; &#123; int count = 0; public Worker2(int count) &#123; this.count = count; &#125; @Override public Long call() throws Exception &#123; Stopwatch started = Stopwatch.createStarted(); Set&lt;String&gt; title = new HashSet&lt;&gt;(); title.add("title"); DocumentStoredFieldVisitor titleVisitor = new DocumentStoredFieldVisitor(title); if (count &gt; 0) &#123; TopDocs docs = indexSearcher.search(query, count); ScoreDoc[] scoreDocs = docs.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; indexSearcher.doc(scoreDoc.doc, titleVisitor); &#125; Document document = titleVisitor.getDocument(); System.out.println("With DocumentStoredFieldVisitor get title counts = " + document.getValues("title").length); &#125; long elapsed = started.elapsed(TimeUnit.MILLISECONDS); System.out.println("With DocumentStoredFieldVisitor: " + elapsed + " ms"); return elapsed; &#125; &#125;&#125; 在索引1000000个文档之后，每个文档添加14个不同类型的Field，分别运行Worker1和Worker2，进行十次的基于ID的范围查询，取十次结果的平均值，得到使用DocumentStoredFieldVisitor平均单次耗时6507 ms，不使用DocumentStoredFieldVisitor平均单次耗时8024 ms。可见性能提升还是很可观的，当然该测试并不权威，但是可以给出一个简单直观的比较。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的图书馆时光]]></title>
    <url>%2F2017%2F05%2F23%2Fmy-library-time-in-SHU%2F</url>
    <content type="text"><![CDATA[我的图书馆时光姓名：codepub&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;学号：13721035&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;学院：计算机工程与科学学院 一、入馆记录2013年—2016年4月，一共到馆205次，第一次走进宝山校区图书馆是在2013年9月6日10时21分22秒。13级硕士生平均入馆138.52次，我的次数是205次，我的入馆次数是平均次数的1.48倍。 二、选座记录2013年—2013年10月，一共预约座位4次，第一次预约的是本部图书馆的二楼综合阅览（东区）的141号座位，时间在2013年9月6日10时23分1秒。 三、上机记录您无上机记录。 四、借还记录2013年—2015年10月，一共借还图书69本，我第一次借还的书籍是《语义Web技术基础》，借阅时间是2013年11月11日，我最后一次借还的书籍是《文明之光.:第三册》，借阅时间是2015年10月18日，我的借阅偏好是：专业技术。13级硕士生人均借还30.76册，我借还图书69册，我的借阅量是平均量的2.24倍。 五、我的借还清单 序号 书名 作者 出版社 出版年 借阅时间 1 语义Web技术基础 Pascal Hitzler … [等] 著 清华大学出版社 2012 20131111 2 Hadoop实战 (美) Chuck Lam著 人民邮电出版社 2011 20131117 3 Hadoop实战 陆嘉恒著 机械工业出版社 2011 20131117 4 Lucene in action中文版 (美) Otis Gospodnetic, Erik Hatcher著 电子工业出版社 2007 20131117 5 解密搜索引擎技术实战 :Lucene &amp; Java精华版 罗刚编著 电子工业出版社 2011 20131117 6 Hadoop源代码分析 张鑫著 中国铁道出版社 2013 20131117 7 搜索引擎原理与实践 袁津生, 李群, 蔡岳编著 北京邮电大学出版社 2008 20131117 8 数据挖掘原理与算法 邵峰晶 … [等] 编著 科学出版社 2009 20131117 9 大数据时代的历史机遇 :产业变革与数据科学 赵国栋 … [等] 著 清华大学出版社 2013 20131117 10 自己动手写网络爬虫 罗刚, 王振东编著 清华大学出版社 2010 20131117 11 增长的极限 :the 30-year update (美) 德内拉·梅多斯, 乔根·兰德斯, 丹尼斯·梅多斯著 机械工业出版社 2006 20131117 12 大数据挑战与NoSQL数据库技术 陆嘉恒编著 电子工业出版社 2013 20131117 13 迅速搭建全文搜索平台 :开源搜索引擎实战教程 于天恩编著 清华大学出版社 2007 20131117 14 数据挖掘与数据化运营实战 :思路、方法、技巧与应用 卢辉著 机械工业出版社 2013 20131117 15 Web 3.0 :互联网的语义革命 :the power of the Semantic Web to transform your business (美) 大卫·西格尔著 科学出版社 2013 20131128 16 计算智能 :理论、技术与应用 丁永生编著 科学出版社 2004 20131128 17 PHP开发宝典 刘欣, 李慧等编著 机械工业出版社 2012 20131128 18 物联网关键技术 徐勇军, 刘禹, 王峰编著 电子工业出版社 2012 20131128 19 下一代互联网 吴建平, 李星主编 电子工业出版社 2012 20131128 20 物联网与云计算 张为民, 赵立君, 刘玮编著 电子工业出版社 2012 20131128 21 互联网之美 张浩编著 清华大学出版社 2013 20131128 22 QQ帮主 :马化腾其人 刘世英, 李良忠著 经济日报出版社 2010 20131202 23 马化腾谈创业与管理 谢森任著 海天出版社 2011 20131202 24 十亿美金的教训 林军, 唐宏梅著 浙江大学出版社 2011 20131202 25 企鹅凶猛 :马化腾的中国功夫 薛芳著 华文出版社 2009 20131202 26 中国互联网风云16年 武帅著 机械工业出版社 2011 20131202 27 矛盾与出路 :网络时代的文化价值观 :cultural values in the internet era 金民卿, 王佳菲, 梁孝著 经济科学出版社 2013 20131214 28 问道云计算 王鹏著 人民邮电出版社 2011 20131214 29 大数据时代 :生活、工作与思维的大变革 :a revolution that will transform how we live, (英) 维克托·迈尔-舍恩伯格, 肯尼思·库克耶著 浙江人民出版社 2013 20131214 30 云计算 :深刻改变未来 张为民 … [等] 编著 科学出版社 2009 20131214 31 基于认知与计算的事件语义学研究 刘茂福, 胡慧君著 科学出版社 2013 20131214 32 语义Web原理及应用 高志强 … [等] 编著 机械工业出版社 2009 20131214 33 语义网、社会网络计算与Web资源共享 王莉著 电子工业出版社 2011 20131214 34 语义网原理与技术 陆建江 … [等] 编著 科学出版社 2007 20131214 35 Semantic web for the working ontologist : modeling in RDF, RDFS and OWL = 实用语义网 : RDFS与O Allemang, Dean. Beijing : Posts &amp; Telecom Press, 2009. 2009 20131214 36 语义Web技术基础 Pascal Hitzler … [等] 著 清华大学出版社 2012 20131219 37 Head First PHP &amp; MySQL Lynn Beighley, Michael Morrison著 中国电力出版社 2010 20131219 38 云计算与分布式系统 :从并行处理到物联网 :from parallel processing to the internet o (美) Kai Hwang, Geoffrey C. Fox, Jack J. Dongarra著 机械工业出版社 2013 20131229 39 新概念英语语法新思维.:第四册 梅雪主编 中国石化出版社 2010 20131229 40 程序员的数学 (日) 结城浩著 人民邮电出版社 2012 20131229 41 新概念英语语法新思维.:第三册 新概念英语学习中心编 中国石化出版社 2010 20131229 42 精通Spring :Java轻量级架构开发实践 孟劼编著 人民邮电出版社 2006 20131229 43 Hadoop技术内幕,:深入解析Hadoop Common和HDFS架构设计与实现原理 :in-depth study of C 蔡斌, 陈湘萍著 机械工业出版社 2013 20131229 44 数据挖掘原理与算法 邵峰晶 … [等] 编著 科学出版社 2009 20140105 45 Lucene in action中文版 (美) Otis Gospodnetic, Erik Hatcher著 电子工业出版社 2007 20140105 46 语义Web技术基础 Pascal Hitzler … [等] 著 清华大学出版社 2012 20140219 47 Hadoop技术内幕,:深入解析Hadoop Common和HDFS架构设计与实现原理 :in-depth study of C 蔡斌, 陈湘萍著 机械工业出版社 2013 20140219 48 精通Spring :Java轻量级架构开发实践 孟劼编著 人民邮电出版社 2006 20140219 49 Lucene in action中文版 (美) Otis Gospodnetic, Erik Hatcher著 电子工业出版社 2007 20140219 50 基于认知与计算的事件语义学研究 刘茂福, 胡慧君著 科学出版社 2013 20140219 51 编译原理 :编译程序构造与实践 张幸儿编著 机械工业出版社 2008 20140325 52 编译原理 主编王磊, 胡元义 科学出版社 2009 20140325 53 基于认知与计算的事件语义学研究 刘茂福, 胡慧君著 科学出版社 2013 20140325 54 编译原理及实现 孙悦红编著 清华大学出版社 2011 20140325 55 编译原理及编译程序构造 薛联凤, 秦振松编著 东南大学出版社 2013 20140325 56 C++程序设计教程 钱能主编 清华大学出版社 1999 20140325 57 新概念英语词汇精典.:4 主编金利 北京语言大学出版社 2010 20140325 58 新概念英语词汇精典.:3 主编金利 北京语言大学出版社 2010 20140325 59 单词记忆大革命 :用耳朵记单词,:CET-6 金莉 世界图书出版公司 2007 20140325 60 精通Spring :Java轻量级架构开发实践 孟劼编著 人民邮电出版社 2006 20140414 61 Lucene in action中文版 (美) Otis Gospodnetic, Erik Hatcher著 电子工业出版社 2007 20140414 62 语义Web技术基础 Pascal Hitzler … [等] 著 清华大学出版社 2012 20140414 63 新概念英语词汇精典.:3 主编金利 北京语言大学出版社 2010 20140511 64 新概念英语词汇精典.:4 主编金利 北京语言大学出版社 2010 20140511 65 基于认知与计算的事件语义学研究 刘茂福, 胡慧君著 科学出版社 2013 20140511 66 Java web典型模块与项目实战大全 :49.5小时多媒体教学视频 常建功等编著 清华大学出版社 2011 20150316 67 英语思维是这样炼成的 王乐平著 华南理工大学出版社 2010 20150316 68 文明之光.:第二册 吴军著 人民邮电出版社 2014 20151018 69 文明之光.:第三册 吴军著 人民邮电出版社 2014 20151018]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 拉取海量数据报 OutOfMemoryError]]></title>
    <url>%2F2017%2F05%2F12%2FMySQL-pull-the-massive-data-out-of-memory-error%2F</url>
    <content type="text"><![CDATA[在用最基本的JDBC拉取数据的时候，由于拉取的是海量数据，所以程序跑了一段时间之后报java.lang.OutOfMemoryError: Java heap space，这个错误很简单，也很好解决，网上一搜一大把，只需要设置ResultSet获取数据模式为row-by-row，但是总结多数的解决方案是如下两种：① 以PreparedStatement为例，需要设置四个参数123preparedStatement = connection.prepareStatement(formatSql, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);preparedStatement.setFetchSize(Integer.MIN_VALUE);preparedStatement.setFetchDirection(ResultSet.FETCH_REVERSE); ② 同样以PreparedStatement为例，需要设置三个参数12preparedStatement = connection.prepareStatement(formatSql, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);preparedStatement.setFetchSize(Integer.MIN_VALUE); 这种解决方案是可以的，那么本文还有无存在的必要呢？当然有。这两种方案基本上都是参看MySQL官方说明来解决的，具体链接点我，内容摘录如下 By default, ResultSets are completely retrieved and stored in memory. In most cases this is the most efficient way to operate and, due to the design of the MySQL network protocol, is easier to implement. If you are working with ResultSets that have a large number of rows or large values and cannot allocate heap space in your JVM for the memory required, you can tell the driver to stream the results back one row at a time. To enable this functionality, create a Statement instance in the following manner: stmt = conn.createStatement(java.sql.ResultSet.TYPE_FORWARD_ONLY, java.sql.ResultSet.CONCUR_READ_ONLY);stmt.setFetchSize(Integer.MIN_VALUE); The combination of a forward-only, read-only result set, with a fetch size of Integer.MIN_VALUE serves as a signal to the driver to stream result sets row-by-row. After this, any result sets created with the statement will be retrieved row-by-row. There are some caveats with this approach. You must read all of the rows in the result set (or close it) before you can issue any other queries on the connection, or an exception will be thrown. The earliest the locks these statements hold can be released (whether they be MyISAM table-level locks or row-level locks in some other storage engine such as InnoDB) is when the statement completes. If the statement is within scope of a transaction, then locks are released when the transaction completes (which implies that the statement needs to complete first). As with most other databases, statements are not complete until all the results pending on the statement are read or the active result set for the statement is closed. Therefore, if using streaming results, process them as quickly as possible if you want to maintain concurrent access to the tables referenced by the statement producing the result set. 但是我想说，其实一个参数就足矣。只需要设置fetch size为Integer.MIN_VALUE即可。代码如下：12preparedStatement = connection.prepareStatement(formatSql);preparedStatement.setFetchSize(Integer.MIN_VALUE); 这样为什么可以呢？我们来看源码，点开prepareStatement的具体实现。 123456package com.mysql.jdbc;public class ConnectionImpl extends ConnectionPropertiesImpl implements MySQLConnection &#123; public PreparedStatement prepareStatement(String sql) throws SQLException &#123; return this.prepareStatement(sql, 1003, 1007); &#125;&#125; 可以看到即使你调用的是prepareStatement(formatSql)，但是在实现中调用的是prepareStatement(sql, 1003, 1007)，而ResultSet.TYPE_FORWARD_ONLY = 1003，ResultSet.CONCUR_READ_ONLY = 1007，所以不需要在调用的时候传递TYPE_FORWARD_ONLY和CONCUR_READ_ONLY。 再来看setFetchDirection的具体实现类12345678910111213package com.mysql.jdbc;public class StatementImpl implements Statement &#123; public void setFetchDirection(int direction) throws SQLException &#123; switch(direction) &#123; case 1000: case 1001: case 1002: return; default: throw SQLError.createSQLException(Messages.getString("Statement.5"), "S1009", this.getExceptionInterceptor()); &#125; &#125;&#125; 可知，在实现中，当direction值是1000、1001和1002时，其处理逻辑是一样的，那么这些值表示什么意思呢？在ResultSet类中可以查到123int FETCH_FORWARD = 1000;int FETCH_REVERSE = 1001;int FETCH_UNKNOWN = 1002; 所以再调用preparedStatement.setFetchDirection(ResultSet.FETCH_REVERSE);这一句其实完全没必要，因为不论你传递的是哪个值，其结果都是相同的，所以说，使用流式结果集获取海量数据一个参数足矣，不要迷信网上二手信息，同样不要迷信官网，只有源码最靠谱。 如果想继续深究，可以查看MySQL判断是否开启流式结果集的方法，实现如下，判断逻辑很简单123456package com.mysql.jdbc;public class StatementImpl implements Statement &#123; protected boolean createStreamingResultSet() &#123; return this.resultSetType == 1003 &amp;&amp; this.resultSetConcurrency == 1007 &amp;&amp; this.fetchSize == -2147483648; &#125;&#125;]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradle使用扩展属性管理依赖版本号]]></title>
    <url>%2F2017%2F05%2F09%2FGradle-uses-extended-attributes-to-manage-dependent-version-numbers%2F</url>
    <content type="text"><![CDATA[Maven预设变量使用过Maven的人应该都知道，我们在Maven项目中添加依赖的一般性做法。就是打开pom.xml文件，在&lt;dependencies&gt;节点下添加12345&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt; 包含坐标和版本号的内容，那么在Java类文件中，就可以引用Lucene包中的各种类了。但是要注意一点，这里面的版本号是以硬编码的形式存在，作为一个合格的软件开发者，要尽量在你的代码中避免硬编码的情况。为什么呢？比如我需要依赖其它的Lucene模块，那么pom.xml中添加内容如下：12345678910111213141516171819202122232425&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-highlighter&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queries&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt; 假设经年累月，项目需要升级，Lucene的新版本也已发布，那么是不是需要手动修改每一行&lt;version&gt;5.5.0&lt;/version&gt;呢？这还只是依赖几个模块的问题，假设你依赖成百上千个模块，其版本号都需要升级，是不是觉得想抽当初的自己呢？其实在Maven中这种情况很好解决、就是利用预设变量。 12345678910111213141516171819202122232425262728293031&lt;properties&gt; &lt;version.lucene&gt;6.0.0&lt;/version.lucene&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;$&#123;version.lucene&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;$&#123;version.lucene&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-highlighter&lt;/artifactId&gt; &lt;version&gt;$&#123;version.lucene&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;$&#123;version.lucene&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queries&lt;/artifactId&gt; &lt;version&gt;$&#123;version.lucene&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 那么以后再遇到项目升级的情况，只需要手动修改&lt;version.lucene&gt;6.0.0&lt;/version.lucene&gt;一行代码即可搞定，所有引用到该版本变量的依赖都自动升级，这样来管理依赖，是不是很哈皮呢？ Gradle单模块同样作为后起之秀的Gradle如何优雅地解决类似问题呢？硬编码的写法如下123dependencies &#123; compile "org.apache.lucene:lucene-core:5.5.0"&#125; 优雅的写法①如下，打开build.gradle文件123456ext &#123; luceneVersion = '6.5.0'&#125;dependencies &#123; compile "org.apache.lucene:lucene-core:$luceneVersion"&#125; 一定要注意包含$符号时，要用双引号，我就因用单引号在这上吃过亏。在Gradle中单引号和双引号都是合法的，但是略有不同。单引号中的内容严格对应Java中的String，不对$符号进行转义。双引号的内容则和脚本语言处理有点像，如果字符中有$号的话，则它会先对$表达式求值。在Gradle中，其实还有三引号的情形，这代表什么呢？三引号中的字符串支持任意换行，比如12345def multieLines = ''' begin line 1 line 2 line 3 end ''' 除了①，还有优雅的写法②，使用字典类型，修改build.gradle文件123456789101112ext &#123; javaSourceCompatibility = '1.8' libVersions = [ junit : '4.12', lucene: '6.5.0', guava : '20.0' ]&#125;dependencies &#123; compile "junit:junit:$libVersions.junit" compile "com.google.guava:guava:$libVersions.guava"&#125; Gradle多模块当在一个根项目下有多个子模块，那么一种简单的做法是在每个子模块中都定义ext代码块，声明需要使用到的版本号变量，这样做当然可以。但是当遇到需要升级版本号的情况时，需要手动修改所有的子模块，其实还有更优雅的解决方案。就是在根项目中定义ext代码块，打开根项目的build.gradle定义1234567ext &#123; luceneVersion = '6.5.0' libVersions = [ junit : '4.12', guava : '20.0' ]&#125; 然后在每个子模块的build.gradle中都可以直接引用之，方式如下123dependencies &#123; compile "junit:junit:$&#123;rootProject.libVersions.junit&#125;"&#125; 这样，当需要升级版本号的时候，只需要升级根项目中的变量即可，所用子模块的版本号会自动升级。 消除Gradle编译警告通过Gradle编译项目过程中，有时会报如下警告信息 注: xxx.java使用或覆盖了已过时的 API。注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。注: xxx.java使用了未经检查或不安全的操作。注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。 警告不是Error，虽然不影响编译，但是看着总是不舒服，所以想办法消除警告信息。根据StackOverflow上的问答，在build.gradle中添加如下配置即可消除警告。1234567allprojects &#123; gradle.projectsEvaluated &#123; tasks.withType(JavaCompile) &#123; options.compilerArgs &lt;&lt; "-Xlint:unchecked" &lt;&lt; "-Xlint:deprecation" &#125; &#125;&#125;]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人民的名义]]></title>
    <url>%2F2017%2F05%2F06%2FThe-name-of-the-people%2F</url>
    <content type="text"><![CDATA[一不小心，追了一个剧《人民的名义》，随着剧情一点一点地推进，突然开始很心疼，心疼的不是别人，也不是剧中的人物，因为那毕竟是虚构的，不是真实的人生，而是我们自己，这才是真真实实的生活与生命。看了剧中的情节，真是大开眼界，不论是祁同伟的一人得道，鸡犬升天，还是赵瑞龙的草菅人命亦或是丁义珍在饭局上左右逢源，逢场作戏，近似人生赢家。甚至一个处级干部赵德汉，都能狂贪几个亿，要知道，除了国家级领导人，下面就是部司厅局处科，小小一个处级干部，都能贪污几个亿？你知道天朝有多少处级干部吗？呵呵，我还真查了下，毛爷爷说，没有调查就没有发言权，参考链接在这里，处级干部约60万，一个人一亿的话，不敢想象，当然了，不能这么悲观，要相信党相信政府，我还是有这觉悟的。现如今大家都说日子难过，现在知道了吧？为什么我们日子难过了，在人民的名义中各种现象也只是社会真实情况的冰山一角吧？假设现实情况比电视剧中所要努力描绘的良好社会略槽糕一点点，那么日子难就很正常了，我想说，不正常才奇怪哩。 现在好多人都说房价高，那么房价为什么会高呢？只有两种情况，一种是你赚钱的速度赶不上房价的上涨速度，另一种是货币贬值了，房价虚高，只不过刚需接盘侠们是货币贬值的最后承受者，没有享受到任何一点点货币贬值带来的收益，要知道货币贬值对所有人来说并不是公平的，有人说，我手里的钱贬值了，那有钱人手里的钱一样也贬值了呀？只能说你too young，too simple，钱就像是流水，就像一条河，同样的一河水，在水的上游和下游能一样吗？同样货币这个贬值链，就像一条河水一样，也分上游下游，货币流到上游的人的手里的时候，并没有贬值，而是和以前价值一样，那么这些占据上游的都是哪些人呢？发大水的是央行，上游的人是各大国有银行、国企、国家各部委、权力掌控者等，中游是商业银行、民营企业、利益既得者等，经过层层盘剥，当这个贬值链流到下游的时候，又是哪些人呢？就是这个底层劳动人民、刚需接盘侠，之所以房价会高涨，是因为没有任何一个产业能够锁住这么海量的贬值货币，唯有将这些海量的货币存之于楼市，才能尽量减少对其他行业的伤害，试想通胀不是流向楼市，而是流向日常生活必须品，那个个吃不上饭可是天大的问题了，但是一部分人住不上房却不是致命的，即使这些住不上房的人闹腾也不用操心，因为那些有房的会代政府来管教他们，你说此话怎讲，君不见，房价略降，全国各地的售楼处都出现了业主打砸情况吗？这不就是有房产者、既得利益者在代国家管教这些无房产者吗？房价想降很难，一是政府不愿意，房价降了，海量的贬值货币往哪藏呢？二是有产者不愿意，买不起房，大家不会联合起来打砸售楼处，但是房价下降，已买房的人则会联合起来上书政府，要求政府救市，更有甚者，直接联合起来打砸售楼处，还有天理吗？法制社会存在这种流氓行径？没有任何理由，就是因为房价下降，就可以任意耍流氓了？呵呵，这些人没有得到严肃的处理，已经代表了政府的态度，睁一只眼闭一只眼，这些人就都没事了。 追剧的过程中，一把手在剧中出现了无数遍，我英文不好，不要欺负我，我想请问一把手英文怎么翻译呢？大英帝国、美帝国家人民的词典中，有一把手这个词吗？有海外待过的人，还望不吝赐教。民选政府与任命制有着本质的区别，民选出来的领导人要对人民负责，同理，任命制选出来的领导人只对任命他的人负责。这种本质的区别，注定了人民有着本质的不同地位，该得到什么样的对待都是自己争取而来，你笑着看资本主义国家人民游行、示威、罢工，那就不要忽视人家赢得的地位。有付出才有收获，老祖宗留下来的话，是有道理的。任命制在我看来只能听天由命了，就像古代皇帝一样，如果是个雄才大略、英明神武的皇帝，那么人民生活相对来说更幸福一点，如果碰到昏庸无能、荒淫无度的主，那人民的生活能不水深火热吗？这不就是靠天吃饭吗？和现在的任命制有啥区别？一把手喜欢你，就提拔你，至于你对人民来说是福是祸，又有几个一把手会关心呢？ 这部剧叫《人民的名义》，恕我愚钝，我不知道这里面哪一点代表了人民的名义，我只看到草根出身的处长在第一集就被干掉了，我只看到同样草根出身的祁同伟和高小琴最后同样被干掉了，而那个利用权势不断打压祁同伟的人呢？梁璐包括他的父亲最后毫发无损，这就是人民的名义吗？人民的孩子总是最先和最后被干掉吗？我分明看到的就是有权有势同样能逍遥法外，我不知道现实中有多少丁义珍，但是我敢保证现实中的丁义珍绝对不会在国外的餐馆刷盘子，处处美化、处处讨喜，编剧们都不觉得恶心吗？像沙瑞金和侯亮平在现实中毕竟是少数，再进一步讲，现实中有多少丁义珍、赵立春（副国级，国家领导人啊、领导人啊、领导人啊）、赵瑞龙、刘新建、陈清泉等等这些人呢？恐怕远不止电视中所表现的那些吧？ 在剧情最后部分，侯亮平劝降祁同伟，莫名的心疼起祁同伟来，那个和天下棋，愿用自己的命去胜天半子的人，也许在祁的心中是这么想的，你侯亮平算什么东西，也敢来劝降我？在祁的身边，虽然不乏师长，以及同一师门的师弟师妹，但是又有几个打心眼里瞧得起看得起他的呢？他想过靠自己的努力来出人头地，在大学里学习努力，成绩优异，并且还是校学生会主席，但是有什么用呢？毫无关系背景的他，眼睁睁的看着那些能力平平甚至低于他的的校友们分配到了大城市，而他却被掌控权力的梁璐的父亲下放到了穷山沟沟里，只因他没接受梁璐的追求，即便他没有放弃，主动请缨去做缉毒工作，因为这个工作更危险，更容易立功，凭借着过人的勇气和努力，他成为了缉毒英雄，但是结果呢？梁璐的父亲，凭借手握大权，借着权力以惜才为名义，阻止让他升迁，硬生生的将他继续困于穷山沟之中。最终，这个内心孤傲的人向现实屈服了，这才有了那在汉东大学的著名一跪，祁同伟啊是个被时代抛弃的人、是个被权力毁掉的人、是个草根出身不断向上爬却死在权力脚下的人，直到最后事发败露，他也没有屈服，用他的话说，这个世界上没有人能够审判他，即便可能被判无期徒刑，苟延残喘，但是并没有，他没有投降，没有屈服，而是选择自杀结束生命。 剧中还有一个现象被大家忽略了，那就是高育良和李达康的女儿都出国了，这就是官二代的命啊。而侯亮平和陈海这种所谓的中产呢？孩子只能挤在国内上些垃圾学校，甚至侯亮平妻子给孩子报的一个补习班，那老师居然是个电工，但是招生工作火热火热的，这和现代的学区房、各种补习班真是不谋而合。幸好这个老师被侯亮平一眼识破，谁又能说现实中存在诸多没有被识破的呢？如果现实中没有侯亮平去戳穿老师的嘴脸呢？岂不是依然可以大摇大摆的顶着补习班的名义继续招摇撞骗。如果现实中的所有官员子女都在国外接受教育，他们对人民又怎么会负责呢？从小接受资本主义国家的理念、文化、习俗、教育等等，怎么比得了土生土长的中国人对人民的了解程度呢？这些人谁又能保证其不在官爸爸富爸爸的扶持下走上统治人民的路子呢？当他们成长为了统治阶级，只要自我舒适，谁管你人民死活，大不了贪了几个亿全家移民罢了。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《蚁族：大学毕业生聚居村实录》]]></title>
    <url>%2F2017%2F04%2F25%2FRead-ant-tribe-college-graduates-live-in-the-village-record%2F</url>
    <content type="text"><![CDATA[古有科举，今有高考，然今日境却不与古日同。科举自隋唐始，终于晚清，这是古代读书人唯一的上升通道，虽然说比较困难，但是最起码还是一个可行的通道，能让人对未来满怀憧憬。但当今时代，时移世易，在这个权利和人情的社会，越是“苦读”的平民家庭出身的孩子，机会可能越少。许多靠近权力的机关和垄断行业里，越来越没有平民和贫苦人家孩子的缘分。父辈的权力和“人脉”会以某种方式“世袭”。“蚁族”们大多来自农村和县级市，家庭收入低，他们的父母处于社会中下层。十年寒窗苦读得以让他们接受高等教育，但毕业时他们发现，由于自己没有“硬”关系，只能又回到“村”里。他们不是不努力不是不上进的一群人，能够从数千万底层劳动人民中脱颖而出，本身就说明了他们还是有所追求，有所渴望的，但是现实给予了他们重重一击，这一击足以让他们将人生中种种不如意，归因于这个社会，从而使自己喝社会对立起来。 人们说，现在是“知识经济”的时代。然而，蚁族们，接受了高等教育，却无法实现知识致富，甚至对某些人来说，找到一份糊口的工作都比较困难，这到底是自身不够努力？还是社会无法提供施展的平台？他们在社会上究竟处于怎样的位置？他们比传统的农民工幸福吗？难道他们千里迢迢来到大城市只为了体味生活的磨难？蚁族们有太多的故事值得述说，每个人虽然微若蝼蚁，但是都怀有各自那小小的梦想与希望。鲁迅先生说，希望本无所谓有，也无所谓无，这就像地上的路，其实地上本没有路，走的人多了，也便成了路。虽然现实有各种困难，但是对于蚁族们来说，心有多大，舞台才有多大。 在这本书中记录了大量采访对象身上的故事，看了他们的故事有时候也能看到自己的影子吧。不是不努力，不努力的人确实有，但是努力了依然看不到希望的恐怕才是主流吧，不能简单的把原因归咎于这些才毕业或者说毕业没有多久，也没啥能力的学生身上，大学的垃圾教育不足以培养他们在社会上谋生的手段，这又能怪的了谁呢？大学一直都是易出难进，至少在中国是这样的，而美国与此恰恰相反，美国的模式是易进难出，任何人都可以去读名校，但是四年本科毕业率极低，至少比中国低20个百分点吧。中国的大学就像是一股洪流，不是能够轻易改变的，任何置身其中的人，哪怕你能看到问题所在，你依然无能为力，你能做的只是尽量提高对自己的要求而已，仅此别无他法，只能被这股洪流裹挟着，浪费掉那谈不上多重的生命。 其实这些蚁族中也不乏有毅力、雄心、长远规划的人，而这些人多数都是男人，总的来说，在采访的蚁族中，每月一两千、两三千且月月光的女生居多，他们从心里上就认为自己是弱者，自己无需为以后考虑，更无需负担买房的压力，所以他们反而是过得更舒适的一类人，而另一类人呢？工资有的能到五六千，但是依然只蜗居在这每月几百块的破旧房子里，而舍不得租住更好的但价格更贵的房子，他们多数都是在默默地为了以后做这长远的打算。他们年纪都不大，却有更远大的目标，他们有能力使自己过得更好，却甘愿蜗居在这条件极差的城中村里，他们不在乎眼前的生活是否安逸。在这个世界上，他们将事业放在重要的位置。在他们看来，美好的梦想永远值得他们努力奋斗。 农村孩子在城市中等待翻身，如同等待铁树开花一样艰难。他们以考大学的方式告别农村，却至今仍无法融入城市的主流文化。物竞天择，家庭的负担和过低的起点，使农村的孩子无法轻装上阵。走出了老家的农村，又走进了北京的农村，这仿佛是一种宿命，萦绕着他们本该绽放的人生。然而他们没有退缩，在我看来，这些蜗居在城中村的人，心中依然保留着一份幻想，期待靠自己的努力，不断打拼，最终能够在这个城市立稳脚跟。可惜，现实是残酷的，他们终将成为北京的弃儿，我又何尝不是呢？一个人奋斗在魔都，满以为凭自己的努力可以奋斗出属于自己的一片天，如今想想，痴念罢了。不管是既得利益者，还是有产阶层，抑或统治阶级，都不可能轻易的让你成为这个全国顶尖城市的原住民。 大家想必都还记得曾经非常火热的《奋斗》吧？这些刚毕业就开上奥迪或奥拓的年轻人是否就是我们想象中的奋斗？如果这就是80后的奋斗，那这些城中村居住着的大学生难道都在混生活混日子？《奋斗》中那些俊男靓女们整天无所事事，打台球、泡吧的都市青年们究竟在为什么奋斗？其实，真正的生活中，这些住在“聚居村”里的“蚁族”正以实际行动诠释着“奋斗”的真正含义。刚毕业的他们面对生活显得捉襟见肘，但是这些能直面现实、接受现实的年轻人具有强大的精神动力，他们有自己的理想，而且正在积蓄力量为实现这些理想而奋斗。现实的分层是残酷的，《奋斗》的导演们以及里面的俊男靓女们其实并不知道什么才是真正源于生活的奋斗，那种从社会的底层不断向上爬，去努力撬动途径的每一个阶层所遇到的阻力与抗拒究竟有多大，他们是完全无法体会的，这部剧虽然很火，但是仅仅是富家公子顽劣青年的自我标榜与吹捧而已，他们并不理解艺术源于生活，更高于生活的真正含义。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[水库论坛炒房多军欧神—欧成效]]></title>
    <url>%2F2017%2F04%2F16%2FReservoir-forum-elite-realtor-ouchengxiao%2F</url>
    <content type="text"><![CDATA[欧神简介：姓欧名成效。出生于丁巳年癸丑月癸酉日。上海人，疑似出生于公务员/教育系统的家庭，父母其一据说为正处级。欧神是神童，跳了两级，上海高考前一百，稀里糊涂下进入复旦物理系。那是他的迷惘期，毕业后进入某五百强快消品外企做管培生。然而，他觉得那样的人生毫无希望，毫无前途，他渴望成功。尝试过做外汇，失败告终。在温相（相，古称丞相，今乃总理。我特么很讨厌现在还用相这个字，放眼世界都是民主国家的天下，我天朝人民的思维还仍旧停留在封建时代）上台后的两年内，一次偶然的机遇，他接触到了上海的房地产。一开始只是为了短期获得利差，不料却一头栽了下去。 之前，我的粉丝里面，有人问我欧神现在房产有多少，我估计他起码有25~30套。但是后来看到他的回忆录，他第一次短期获利后，开始大量建仓。第一次大量建仓后，房价虚高，迫于父母压力，仍然抛售了500平米的上海房产，那可是2004-2005年！欧神在网络上，我们能够找到的，最早的作品，是在天涯论坛，是发表于2004年7月31日23点的：《人民币面临巨大的贬值压力》。 欧神在这篇文章内断言，人民币将对外升值，对内贬值。也因此，欧神认为，只有房地产，才是人民抵抗通货膨胀、资产增值的选择。然后，他发表了大量大量经济观察的文章。基本没有看走眼的。然而，当年下面，满满的都是愤青骂街留言。此时的欧神，才26岁。同时，欧神指出，炒房，是一条竞争几乎为零、不论资质人人皆可参与、只要下功夫迟早赚出一个亿的康庄大道。看准了大趋势，然后一头扎在炒房领域。反复炒反复炒反复炒。炒房是个非常孤独的事情，炒房客蛮少，躺着赚钱也缺乏社交。欧神即便再有钱，在经济关系中，不过是个中国最牛散户，兴许还剥削Ｎ多人以收租。有钱，有闲，就在网上开论坛，聊天。他系统性地把全部炒房心得，发表出来。他目前认为京沪还有上升空间。而且，非常看好重庆。2016年11月，他在清华第一次公开亮相，为某经济论坛发表演讲。 这是他的大致经历了。他的女儿，如果我没记错，出生于庚寅年的己土日。2010年。他不止一个孩子了。 在最新的一期水库论坛中，欧神坦言对于房产而言，普通人已经没有上车机会了，阶级固化基本完成了。这真是一个疯狂的时代。TG（T代表锤子，G代表镰刀，这是标识，只可意会，不可言传），ZF，吃相越来越难看了。他们要把一切不属于自己阶层的人，打成奴隶，永世不得翻身。就像西西弗斯一样，推着无用的巨石，永世处于那样的状态。而另一方面，那些没上车的人们，他们不比那些上车的聪明？不比他们勤奋？不比他们优秀？不比他们道德？阶级流动的渠道，说关闭就关闭。普通的屌丝阶层，甚至扩大到中产阶层，连知情权都没有，怎么回事都不知道。像苍蝇一样无头乱撞，什么都不懂，最后只能归咎自己命不好。此时此刻，我才感受到了欧神的价值。屌丝一生，都遇不到一个真正的富人，告诉他们真正的诀窍。只能自己瞎折腾一生，哪怕比欧神双商高资质好，都会如此。再聪明，八字再好，也不过是韭菜，只有被收割的命运。只不过，十年前，韭菜们还有系统性逃离的机会。人人那时，都有机会。除了90后。80后也有微弱的机会。现在当然也有，只不过阶层固化获得了决定性的胜利，看还有谁能钻过去吧。未来，八字估计都不顶用了。货币会一直超发，没有笼子的权利就是逃出瓶子的恶魔，人民控制不了货币超发，只能默默承受着通胀对你的财富的吞噬，想想七八十年代那些上一代人节衣缩食，辛苦攒下百十来块的养老钱，现在呢？也只够两顿饭钱吧？我有时会莫名地心疼这些人，钱真的攒不住，攒一辈子几年就给你通胀掉了，另一方面我又对大众的愚昧恨之入骨，他们不知道争取权利也不知道支持那些为他们争取权利的人，反而给那些为整个社会民主化进程呕心沥血的人泼以无情的冷水与嘲讽。在心里默默讥笑他们“你们这些傻叉，就知道民主就知道争取权利，看我们多聪明，躲在你们背后，你们争取到了，我们一样可以享受，你们争取不到，我还有嘲笑的对象，好爽啊”，这就是我天朝绝大多数人的心态吧。 欧神对金融系统有着深刻的认知，比如央行拿外汇去拯救国有银行的坏账，好多人感觉和自己没有关系，于是没有人出声，其实一方面坏账是指钱被贪官贪污或者携款潜逃国外，导致银行无法填补亏空，形成坏账，另一方面，外汇属于全体中国人民，用外汇储备去填补亏空，本质上就是用每一个中国人辛苦攒来的钱去填补亏空，这里面包括你的、我的、他的、所有人的，没人能逃得掉。为什么说是人民的钱，因为这背后一系列的操作最后都会导致物价上涨，由物价上涨来填补，而每一个人只要你在国内生存，就间接的变成由你消费膨胀的物价来填补坏账的亏空。 在2004年的时候，基于对金融系统深刻的认知，欧成效说“房价是太低，不是太高，远远地太低太低太低。房价上涨，是一个《挤泡沫》的过程，挤掉的是货币系统里面的泡沫”。我补充一句，坏账越多亏空越多贪官越多，最后导致的就是人民币里面的泡沫越多，而如果仅靠物价上涨是无法消化这么多泡沫的，而且老百姓也最不愿意，因为你每天都要吃饭喝水，你无法承受这么多泡沫所带来的物价上涨，所以归根结底，一个可靠的手段是房价上涨，用房价来消化货币系统中的泡沫，进而将房价维持在高位，也将超发的货币绑定在房价之上。 购房千万不要购买以房型作为卖点的楼盘，否则你很可能进去了就被套牢了，房型并没有统一的标准，如果是自住，只需要选择适合你自己的就好了，无非是朝向、通风、采光。对于上班族来说，一个4~6平米的客卫，才是最优质房型。强烈推荐买房应该买市中心，五六年房龄的低估房，原则上不买一手房。 如果汇率上涨，则受益者是人民币持有人，也即是十万万老百姓。你可以很轻松地去加勒比海海滩度假，去希腊看奥运会，甚至娶一个年轻漂亮的买来西亚太太。而如果汇率不涨，国内通胀的话。受益的是政府和银行。作为最大的债务人，几万亿的坏账将相应缩水，从而更容易偿还。政府还将获得大量的铸币权，以应付财政开支。 房地产事业，是一种典型的“剥削内贸补贴外贸”。用国内老百姓的负担，来补贴出口竞争力。甚至可以说，“房价越高，出口竞争力越强”。中国也即将走上日本的老路。低汇率，导致外汇储备急剧增加，引发通胀压力。而政府将通胀压力引向房地产，让国内百姓为此买单。外贸出口也始终不受损伤。这是欧神2005年说的话，以他的话来判断2005~2017这之间中国的经济走势来说的话，基本上都是正确的。 欧神说房价越高，居民购买力越强，因为他认为房价就像是零和博弈，有人付出钱买房，自然就有人靠卖房获取收入，那么你想过如果卖方都是政府呢？或者说卖方都是开发商呢？这种大量的金钱集中在一个人或者一个组织的身上，其购买力是远远不如将钱分散到千万人身上的，因为一个人消费能力再强，自然也无法消费一千个人或者一万个人所消费的东西，所以我并不赞成欧神的观点。相反我认为高房价大大的透支了刚需族中产阶级的购买力，注意我说的是刚需，那些名下有几套房的中产并不在我的范围之内。 美元并没有什么购买力。3万亿Dollar的外汇储备，也根本买不回“中国曾出口”的等价购买力。更何况，美国的信用，已经大大受到质疑，三万亿美元最终能拿回多少。金融界目前都是个大大的问号。如果美元收不回来，那么我们过去二三十年的出口，都扔到海里了。如果美元通货膨胀，贬值2/3，那么我们过去十几年的出口都扔到海里了。出口导向型的经济政策，其最终的输家只有一个，那就是农民，因为商人出口挣到外汇，央行把外汇换成人民币，而这人民币就是靠印刷机印出来的，农民手中的钱凭空损失了购买力，而他们并不自知。有人冲到我们的草原，抢夺我们的牛马，抢夺我们的羊羔。我们拼命反抗，因为我们知道他是强盗，抢我小羊的强盗。有人在电脑前拨动了几个数字，宣布几项政策。造成了数以十万亿的财富移动，造成了沿海几亿人的繁荣，造成了内陆几亿人的贫穷，而这幕后黑手、最大的强盗其实就是央行。我们不知道，因为我们不懂经济，不懂金融学。我们只知道日子是很苦很苦的，却从来不明白为什么。 治大国如烹小鲜。当远方的信使，骑着快马气喘吁吁地奔波了八百里跑过来和你说“宰相大人，不好了，出大事了”。你掏出手枪，砰地一声，把信使打死了。然后洋洋得意地和周围人说：“我什么也没听到，天下太平”。你这不是2B么。我们现在的内阁团，就是这么一群2B。欧神的这段话说的非常贴切，对于房价是十年九控，除了2009年（08年金融危机，余威延后，致09年房价略降），基本上是越控越涨，很简单，现在房价的90%成本是土地成本，土地成本不降，你想降房价？开玩笑，有哪个开发商愿意亏本建房卖给你呢？房价必然是土地成本+建筑成本+利润等的总和，所以说限购限贷只会使需求延后而已，对于减少土地供应，则更是傻逼政策，未来没地建房了，你还指望房价下降？不涨上天才怪呢！ 文章里有一个小段子，我感觉不错，摘录如下“一个中国人刚到USA时，美国人对他说：‘美国是一个法制社会，你明白吗？’。那个人说：‘明白，公民都要守法’。美国官员说：‘No，法制社会的意思是，政府要守法’”。禁不住一声仰天长叹！ 另外一个段子是这样的，话说重庆的中心是解放碑，解放碑就是重庆中央广场的一个石碑，连接放射性的五条大道，其地位类似于上海的人民广场。段子是这么说的，当年江姐被捕，就是因为蒲志高看见她在朝天门码头上船，穿西装而且自己扛皮箱，于是蒲志高推测她必然是想省钱的G党特务兼屌丝无疑。哈哈，笑了半天。 在任何一个社会，任何一个时代，“觉醒”的人都是少数的，哪怕启蒙时代也不例外。具体来说，就是无论社会怎样变革，哪怕再过100年。社会上看“综艺”节目的人数，仍将大大多于《科学新发现》。阿姨们会为“中国达人秀”上两个镜头感动得热泪盈眶，却丝毫不考虑新出的法规对公平的践踏。用中国一句很有传统智慧的话说：“肉食者鄙”。对于“觉醒”的人群，估计占总人口的0.1%，以中国目前的人口规模，则是100W人。这个数字包含了中国目前所有的“上层”精英。包括所有有权有势的人。这个社会其实就是0.1%的人，在奴役着99.9%的人，从诛心的角度讲，则是0.1%的人在愚弄着99.9%的人。比如现在火遍全国的学区房，其实这些接盘侠们只能算是中产里的富裕者而已，完全算不上精英或者高层，精英和高层是不会让他的后代在国内接受这种洗脑式的垃圾教育的，他们的后代早已驰骋在国外的世界顶级名校中了。 如果你要买房，那么记住一句话准没错，“政府呼吁买房时，一般都是房价最低的时候，买房的事得听政府的”。听党的话跟党走，政策放松时马上入场买，涨个一年半，政府要调控了马上卖，高抛低吸，虽然不一定能卖在最顶峰，但肯定比你追涨杀跌要强百倍。 参考文献[1] 欧神文集（仅有部分，内容不全，全文请自行去下载PDF）]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提高中文输入效率—自然双拼输入法]]></title>
    <url>%2F2017%2F04%2F06%2FImproving-chinese-input-efficiency-natural-shuangpin-input-method%2F</url>
    <content type="text"><![CDATA[缘起每个人的大脑都有其自己的舒适区，比如说从家到公司开车上班，大脑会优先选择走熟悉的路线，而不是每天变个花样走陌生的路线，所以大脑其实是有惰性的，它总是会优先选择自己最熟悉、最不费力、也最简单的策略。长此以往，如果你不主动跳出舒适区的话，大脑自己是无法成长的。这一点小马（马克·扎克伯格，不是马云爸爸哦）也已经认识到了，不得不说，牛人就是牛人，尽管衣食无忧，此生不必奋斗了，但是依然在不断进步，让自己变得更加优秀。小马自己说“每年我都要给自己设立一个挑战，去学习新的东西，让自己在工作之外得到成长”。所以他特别为自己制定了每一年的挑战计划，列表如下： 2009年挑战每天戴领带上班 2010年挑战学习汉语 2011年挑战只吃自己亲手屠宰的动物 2012年挑战每天写代码 2013年挑战每天跟除脸书员工之外的不同的人见面 2014年挑战每天写封感谢信 2015年挑战每个月读两本书 2016年挑战开发一款私人专属人工智能助手以及全年跑步587公里（365英里） 2017年挑战走遍美国每一个州，听取民众的声音 成功的人不可怕，可怕的是成功以后还不断努力的人，比如小马，所以我们应该无条件的向优秀的人学习。为此，我也开始学习一项可能大多数人一辈子都不会学习的技能，那就是双拼输入法。为什么选择双拼呢？因为大多数人用的都是全拼，切换输入法的成本其实是很高的，而双拼效率要胜过全拼，粗略比较一下双方的击键次数，滕王阁序863字。用全拼的字符数（不计空格）2387个。用双拼的字符数是1580个。双拼的字符数是全拼的66%左右。使用辅助码后，双拼的重码差不多是10%到20%，常用的语境中，可以实现盲打。 双拼方案双拼的详细信息参看双拼-维基百科，流行的双拼方案有 自然码双拼 小鹤双拼 拼音加加双拼 微软拼音2003双拼 紫光拼音双拼 智能ABC双拼 我选择的是自然双拼方案，主要是因为这是目前最流行最通用的双拼方案，很多输入法软件的默认双拼方案就是自然码。不过据说小鹤双拼的方案用起来最舒服，如果不介意的话，那么选择应用最广泛的自然码双拼方案即可。 自然双拼自然双拼是有官网的，不过比较丑陋而已，官网点我。根据官网自然码简介如下 自然码是一款非常成熟且定位在高端用户群的输入法，具有超强的整句错误定位和修正功能，兼容非常多的操作习惯，可选择各种扩充专业词库，并可挂接黑马整句输入引擎（偏重医学方面） 自然码利用压缩韵母和偏旁部首发音，以拼音为基础的高效编码方案。在非常简单的情况下，达到了远远超过普通拼音的最佳境界。自然码双拼部分已经是目前事实上的标准方案，广泛用于各种输入法中，其中与自然码非常相似的微软双拼也是自然码授权的修改版 自然码方案的理念是让用户只要记住20多个压缩韵母就能快速准确输入。免除全拼记忆汉字和词组顺序、记忆大量简码，输入速度上不去的麻烦。让用户在使用中轻松自然，随心所欲 在全拼中，每个字都需要声母和韵母组成，但声母和韵母所需要输入的字母个数是不一定的，从一个到三个不等，按键时就需要进行多次输入才能组成一个声母或韵母。而双拼对其进行规范化，无论是声母还是韵母，都各自集合在一个按键上，即把声母中zh、ch、sh和非单字母韵母（ong、iong、uang等）进行重新编排，使每个声母或者韵母都对应一个按键。 这样一来双拼相对于全拼的简便则凸显出来，特别是对于an、ao、un、ang、ong、ing、uang、iang等这些长的拼音组合以及单字输入这种需求来说，比如说「量」全拼用liang，双拼只需要LL，两个键。 以自然双拼的键位设置为例，如果我想输入「少数派」三个字，其键盘上： 按键「U」对应的声母「sh」，对应韵母「u」 按键「K」对应的声母「k」，对应韵母「ao」 按键「P」对应的声母「p」，对应的韵母「un/ün」 按键「L」对应的声母「l」，对应的韵母「ai」 那么，输入「少数派」就变得很简单了： 敲击「U」「K」，就能输入「sh」「ao」 敲击「U」「U」，就能输入「sh」「u」 敲击「P」「L」，就能输入「p」「ai」 所以只需要打印一份自然双拼键位图放在身旁即可，照着键位图练习一段时间基本上就可以记住全部键位了，实现超快盲打。 终极总结按照笔者自己的经历，学习双拼大概经历四个阶段： 用双拼是不是傻。 好想念用全拼时快捷如风的日子。 敲键盘好有感觉而且速度还不错唉。 用全拼是不是傻。 当许多年以后，来到一个陌生的电脑前，看见他们星移电掣地在键盘前忙碌地来回敲打，不时还冲你莞尔一笑：「我打字快伐？」，这时你只是报之神秘的一笑，视而不语，让笑意消散在风中。]]></content>
      <categories>
        <category>Skill</category>
      </categories>
      <tags>
        <tag>Input Method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多说已死，切换Hexo博客评论插件到Disqus]]></title>
    <url>%2F2017%2F03%2F23%2FTransfer-comments-from-duoshuo-to-disqus%2F</url>
    <content type="text"><![CDATA[本站点之前的评论插件一直用的都是多说，作为一款免费的第三方社会化评论插件，总体来说，多说做的还算可以，唯独其号称智能的防垃圾评论系统，就像空气人一样，完全无用，导致多说垃圾评论泛滥，令人作呕。恰逢最近多说宣称要进行业务转型，自然评论系统也要关闭，国内的目前比较好的评论系统只有畅言不错，但是畅言需要备案，而我不愿意备案，无奈只能选Disqus了，所以将本站点的多说评论转成Disqus了。 因为Disqus在国内被墙，所以使用Disqus需要自带翻墙功能或者说需要自带科学上网功能，否则无法加载评论框，自然也就无法评论了，这是我天朝一特色，除了这非常蛋疼的一点，Disqus做得非常好。切换评论系统，首要任务是将评论数据转移到新的系统中，这样基本上就大功告成了。转移评论数据参考多说评论迁移至Disqus，亲测有效，已经成功转移，需要注意一点，在从多说导出数据的时候，选择工具，导出数据，一定要勾选如下两个选项 包含文章数据 包含评论数据 这样在使用脚本解析的时候，才不会报错。 启用Disqus，需要编辑站点和主题的_config.yml文件，添加disqus_shortname字段（先搜索，如果有就不用），设置如下 disqus_shortname: your-disqus-shortname 如需取消某个页面的评论，在md文件的front-matter中增加 comments: false 关于Next.Mist主题启用Disqus评论详情可以参考这里。 同样之前使用的多说分享，以及多说热评等都将被停用，分享推荐使用百度分享，但是Next主题并不支持百度分享，证据在这里，所以只能使用JiaThis分享代替，但是JiaThis的侧栏式分享有BUG，会铺满整个屏幕，不推荐使用，经过亲自实验，最后选择了图标式，分享图标会显示在评论框的上面，除了图标比较丑外，够用。 点JiaThis™图标式代码获取JiaThis分享图标代码，添加到D:/hexo/themes/next/layout/_partials/share/jiathis.swig中即可，然后在D:/hexo/themes/next/_config.yml中启用 123# Sharejiathis: trueadd_this_id: 国外的分享插件，填你自己的add_this_id，需注册]]></content>
      <categories>
        <category>Git/GitHub</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向log4j2 API编程而不是slf4j]]></title>
    <url>%2F2017%2F03%2F08%2FProgramming-to-the-log4j2-API-instead-of-slf4j%2F</url>
    <content type="text"><![CDATA[很高兴，阿里开源了其内部的Java开发手册，简单点说这是一本Java开发规范，比方说以前我一直在纠结工具类的命名到底是以utils结尾还是以util结尾，那同样地，工具类的包名是以utils结尾还是以util结尾呢？在这本电子书里就给出了很好的说明。再比如定义数组的时候，我们可以这样String strs[] = new String[5];也可以这样String[] strs = new String[5];，到底哪种方式更好呢？显然是后一种，后一种明确的指定了我们所定义的变量是String[]类型。也许你会说，这些都是小问题并不影响我开发，是的，问题不大，但是规范漂亮的代码看起来难道不是更加的赏心悦目吗？把每一次阅读代码的过程看做是品味一杯醇厚的咖啡不是让人觉得更加惬意吗？当然规范的代码带来的好处远不止如此，比如两个竞争性的开源项目，性能特性等差别不大，其中一个是你主导的，那么这时候，如何能让自己的开源项目获得更多的star呢？显然代码规范漂亮简洁的项目肯定能获得更加的流行，再比如当你接手离职人员的代码的时候，看着那一坨坨写的像翔一样的代码，你是不是很想骂娘呢？甚至问候一下他们家的女性成员呢？ 在看《阿里巴巴Java开发手册》过程中，其在日志规约部分提到 应用中不可直接使用日志系统（Log4j、Logback）中的API，而应依赖使用日志框架SLF4J中的API，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一 很遗憾，这里面并没有指明对于Log4j2日志系统，是否需要使用门面日志框架呢？而我正在使用的就是Log4j2。有关Log4j2的介绍请参考这里和这里，现在的Log4j2已经是Java中最优秀的日志框架了，那么我们是否还需要留有余地，以便日后更换日志框架呢？因为使用slf4j的主要一个目的就是可以方便的更换底层的具体日志框架，而如果没有更换日志框架的必要的话，那么自然也就没有使用slf4j的必要了。 当然了，更多的情况是为了兼容性考虑，比如旧有的项目一直用的都是slf4j，那么这时如果想要结合log4j2使用的话，上层需要面向slf4j API编程，而底层日志框架指定log4j2，需要添加如下依赖： 12345678910111213141516171819202122232425&lt;!-- log配置：Log4j2的核心依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 桥接：告诉Slf4j使用Log4j2 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.8&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 面向slf4j API编程 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.24&lt;/version&gt;&lt;/dependency&gt; 可以看出，这样需要依赖4个Jar包，而实际上log4j2核心的Jar包只有2个。如果log4j2已经足够完美，并且我们也不需要切换底层日志框架的话，是不是直接面向log4j2的API编程更好呢？秉着一向追求完美的习惯，于是去stackoverflow上逛了一下，发现已有类似问题。Is it worth to use slf4j with log4j2中就说了，推荐直接面向log4j2 API编程，理由如下： Message API Lambdas for lazy logging Log any Object instead of just Strings Garbage-free: avoid creating varargs or creating Strings where possible CloseableThreadContext automatically removes items from the MDC when you’re finished with them 而且就目前来说，对于log4j2中的诸多特性，slf4j并不支持（See 10 Log4j2 API features not available in SLF4J for more details），最重要的是log4j2包含了一个log4j-to-slf4j模块，该模块可以在任何时候将任何面向log4j2 API编程的代码转向任何具体的slf4j的实现框架，其调用流程简单描述如下： 综上所述，现在，你可以直接面向log4j2 API编程了。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Log4j2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《暗时间》]]></title>
    <url>%2F2017%2F03%2F04%2FRead-dark-time%2F</url>
    <content type="text"><![CDATA[作者简介：刘未鹏，南京大学计算机系硕士毕业，现就职于微软亚洲研究院创新工程中心，有自己的Blog Mind Hacks。域名 MindHacks.cn 的含义： Mind Hacks 是一本书 Mind Hacks 是一系列思维工具 Mind Hacks 有一个漫长的前生——一个有着近6年历史的技术博客，在CSDN上有超过一百二十万的访问量，详见《前世档案|C++的罗浮宫》 Mind Hacks 的博客作者创建了TopLanguage：https://groups.google.com/group/pongba 善于利用思维时间的人，可以无形中比别人多出很多时间，从而实际意义上能比别人多活很多年。善于利用思维时间的人则能够在重要的事情上时时主动提醒自己，将临时的的记忆变成硬编码的行为习惯。 我：以前一直为自己寻找借口，就是事情太多，记不住，有些事情难免忘记，这次看到了牛人的解决办法，不仅为之一振，是的，既然会忘记，就说明还没有成为行为习惯，比如刚开始学会开车，可能处处小心，走过每一个路口都要仔细观望一番，但是当某条路走了成百上千次之后，你几乎可以凭借本能而不是大脑开车了，什么时候加速什么时候刹车什么时候打转向灯等等，不需要大脑参与其中任何一次的决策，这样也不会出现任何问题，这就是行为习惯。顺便多说一句，所谓的硬编码就是在程序编程中将某些功能写死，使得以后很难扩展或是应付变化，这在编程中是极其不推荐的，远离硬编码的首要原则是尽可能地使自己的代码能够迎接各种变化的挑战。 程序员行业是最适合自学的行业，网络是程序员的天堂，需要的资源、工具，比课堂上的多出何止百倍，如果说还有一个学科，并不需要传统的教育就可以成才，估计非程序员莫属了。 我：从我自身实战的角度来说，我的大多数专业知识都是靠自学得来，不要指望在大学里能够学到多么专业多么精深的知识，或许在国内顶尖高校这是可能的，但是对于国内剩下的几千所一般性大学来说，这几乎是百分之百不可能的。大多数的高校老师都是理论派，念念书读读PPT，这堂课就过去了，几乎没有实操的现场演示，所以对于在一般大学的学生来说，我觉得倒不如去网络上下载大量的培训视频自学技术来得实在。生活中，另外一些常见的现象是，一些人总是会这样说：“可以把某某安装程序发我一份吗？”，“可以给我推荐某某方面的编程书籍吗？”几乎都不用预测，这种类型的人多数都是不适合程序员行业的，他们属于衣来伸手，饭来张口类型，从不会自我督促自我前进，哪怕他们学会太多，但是却无法应付一点点他们没有学过的领域所带来的挑战，他们永远达不到从无知走向无师自通的境界。 对于经验知识的学习来说，光是看着别人做或者听着别人说还不够，往往到了自己就想不起来，结果就是你虽然学到了知识，它却不会在恰当的时候从你的大脑中蹦出来，属于“死知识”。 我：只这一句话就道出了我为什么没有成为得到的深度订阅用户的原因，这其中的一个原因就在于经验性的知识并不能使你受益多少。比如李笑来靠着比特币实现财务自由，他把经验告诉你了，有用吗？你还有机会吗？你再次从头来过，在比特币还未大行其道的时候，你敢斥巨资投入吗？下一次碰到像比特币这样的机会，可能你还会错过，即使你保证说自己不会错过了，但是当这种机会出现的时候，你也抓不住，因为你看不清前景，你不知道它最后会变得这般美好。 人太容易为各种各样的事情分心，要集中注意力做一件事情使非常难的，而正因为难，少有人做到，那些做到的，都变成了牛人。专注力为什么会对学习效率造成这么大的影响。这来源于两个方面，一是专注于一件事情能让表层意识全功率运作，这个是显式的效率。第二点，也是更重要的，它还能够使你的潜意识进入一种专注于这件事情的状态。 我：特别是现在的诱惑越来越多，时间的碎片化也越来越严重，而想要集中精力利用大块的时间去学习已经很难获得了，因为在工作中你容易被太多事情打扰，进而分散你的注意力。最近看到冯大辉的无码科技招聘，里面一条说的特别好，尽量保持办公室安静，即便有需要交谈的地方尽量也不要打扰到别人，还有你最好不要有抖腿的习惯，否则一排的桌子都会被你带动的。 利用碎片时间：任何一点时间都可以用于阅读。举个例子，我每天从家里出发到公司的路上一般有10分钟左右，我发现可以读2~5页书（不是小说类，而是知识类的书）。即便往少了算每天两页，两百页的书也就3个月就读完了。点滴的时间汇聚起来就是一个“长尾”，想一想，每天有多少个这样的4~5分钟。 我：作者达到这个级别，我真想说，不想成功都难啊！生活中大量的点滴时间都被我们忽略了，刷个微博、看个新闻、扫个朋友圈、聊个天、吐个槽等等，可以说，你只要做好了非常基本的几点，其实你很容易就超越了生活中大部分的人。 Satisficing原则：有一次，在豆瓣上看到某人日志里面提到一个讲座，后面写了一些感想，但讲座的链接没有给出，于是我第一反应就是留言问他要链接，但是实际上呢？只要把讲座关键词扔到Google上就行了。留言要链接可以说是人的第一反应，而且这也满足可行原则。如果这个日记是不允许留言的，并且我不认识这个作者，我可能会立即想到去Google了。 我：我感觉吧，其实大脑像人一样，也是个懒蛋，当大脑不费吹灰之力想到了一个可行解的时候，它就不会再去进一步思考最优解了，生活中的某些事情，其实只要多加思考，是存在最优解的。 人最重要的能力之一就是能否从别人的错误中学习，往往是这类人能够迅速走在别人的前面，在别人跌倒的地方跳过去。而不是将别人趟过的泥潭再趟一遍。 我：人生有崖，如何能够最快最短的学习到足够的经验知识就能快速的成功，其实不仅知识可以学习，经验同样可以学习，同样的一件案例，有人能够挖掘到很多的经验而有的人两手空空，毫无所获。但是也不要只做一个思想上的巨人行动上的矮子，经验知识要能在恰当的时刻从你的脑海中蹦出，还需要不断的实践，否则大家都坐地谈天而不付出行动，这样是无法成功的。 看书并记住书中的东西只是记忆，并没有涉及推理，只有靠推理才能深入理解一个事物，看到别人看不到的地方，这部分推理的过程就是你的思维时间，也是人一生中占据一个显著比例的“暗时间”，你走路、买菜、洗脸、坐公车、逛街、出游、吃饭、睡觉，所有这些时间都可以成为“暗时间”，你可以充分利用这些时间进行思考，反刍和消化平时看和读的东西，让你的认识能够脱离照本宣科的局面。这段时间看起来微不足道，但日积月累将会产生庞大的效应。 要从一个“记忆”学习者向“思考”学习者转变，现代，经常性的日常信息多得看都看不完，所以能够深入去思索的人就更少的可怜了，这就需要信息筛选与鉴别能力，要建立起个人的层层过滤网，就像鼻孔中的鼻毛一样，把浑浊的空气经过过滤网的层层筛选到最后变成洁净的空气一样，洁净的空气可以供养给肺部，同样有效洁净的信息流供养给大脑，并可以将从垃圾信息中节约出来的时间用于大脑的自我重塑——思考。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《人类简史》]]></title>
    <url>%2F2017%2F02%2F12%2FRead-a-brief-history-of-humankind%2F</url>
    <content type="text"><![CDATA[作者简介：尤瓦尔·赫拉利，1976年生，牛津大学历史学博士，现为耶路撒冷希伯来大学的历史系教授，青年怪才，全球瞩目的新锐历史学家。他擅长世界历史和宏观历史进程研究。在学术领域和大众出版领域都有很大的兴趣。他的《人类简史》一书让他一举成名，成为以色列超级畅销书，目前这本书已授20多个国家版权，在历史学之外，人类学、生态学、基因学等领域的知识信手拈来，根据图书改变的课程上传YOUTUBE后风靡全球，拥有大批青年粉丝。写书，视频课程之外，他还开设有专栏。 人类经过数百万年才发展出以四肢行走、头部相对较小的骨架，而要将这种骨架调整成直立，可说是一大挑战，而且还得撑住一个超大的头盖骨，更是难上加难。于是，为了能望远、能有灵活的双手，现在人类只得面对背痛、颈脖僵硬的苦恼代价。这点对妇女来说造成的负担更大。直立的步行方式需要让臀部变窄，于是产道宽度受限，而且别忘了婴儿的头还越来越大。于是，分娩死亡成了女性的一大风险。而如果早点儿生产，婴儿的大脑和头部都还比较小，也比较柔软，这位母亲就更有机会渡过难关，未来也可能再生下更多孩子。 我：逻辑思维的罗振宇讲过这个话题，人类的婴儿其实在母体内只是完成了最基本的器官发育，许多重要的器官以及后续的发育成熟都是在母体外完成的。这其实是一个权衡的过程，想要大的脑容量又想要直立行走，只能将预产期提前，在还未发育完全的情况下，就让孩子出生，在出生之后继续完成发育，这样才能取得一个相对人类历史来说短暂的平衡。 在现代的富裕社会，平均每周的工时是40~45小时，发展中国家则是60~80小时；但如果是狩猎采集者，就算住在最贫瘠的地区，平均每周也只需要工作35~45小时。他们大概只需要每三天打猎一次，每天采集3~6小时。一般时期，这样就足以养活整个部落。而很有可能大多数的远古采集者住的都是比卡拉哈里沙漠更肥沃的地方，所以取得食物和原物料所需的时间还要更少。最重要的是，这些采集者可没什么家事负担。他们不用洗碗，不用吸地毯，不用擦地板，不用还尿布，也没账单得付。 我：现代社会的发展越来越变态，人已经不为自己而活，而是为了其它所有和自身无关的社会机构政府组织而活，总的来说现在其实是一种倒退，因为要养活的人实在太多，再也无法回到采集社会了。记得美国有段年轻人拍的视频，讲的就是少部分的精英奴役了世界上大部分的人，他们控制纸币，让我们为纸币而活，进而又生产出大批量可有可无的商品，诱惑你去花费纸币，纸币花完了再出卖自己的劳力，给别人奴役自己的机会。 现在如果是在血汗工厂工作，每天早上大约七点就得出门，走过饱受污染的街道，进到工厂用同一种方式不停操作同一台机器，时间长达10小时，叫人心灵整个麻木。等到晚上七点回家，还得再洗碗、洗衣服。而在三万年前，如果是个采集者，可能是在大约早上八点离开部落，在附近的森林和草地上晃晃，采采蘑菇、挖挖根茎、抓抓青蛙，偶尔还得躲一下老虎。但等到中午过后，他们就可以回到部落煮午餐。接下来还有大把时间，可以聊聊八卦、讲讲故事，跟孩子玩，或者就是放松放松。 我：想着这种惬意的生活方式其实不就是很多人穷毕生之力去追逐的吗？古人早已过上了好日子，在我们看不上他们的日子的同时，他们更加看不上我们的日子。 农业革命所带来的非但不是轻松生活的新时代，反而让农民过着比采集者更辛苦、更不满足的生活。狩猎采集者的生活其实更为丰富多彩。普遍来说，农民的工作要比采集者更辛苦，而到头来饮食还要更糟。农业革命可以说是史上最大的一桩骗局。谁该负责？这背后的主谋，既不是国王，不是牧师，也不是商人。真正的主要嫌疑人，就是那极少数的植物物种，其中包括小麦、稻米和马铃薯。人类以为自己驯化了植物，但其实是植物驯化了智人。 我：想想确实是这样，人类的生活正变得越来越复杂，所以我们以为掌控在我们手里的东西其实都是我们所依赖的，我们正变得越来越臃肿越来越脆弱，从一个简单的个体变成了一个依赖系统，这些依赖系统中任何一环出了差错，都会对人类产生影响，甚至付出生命的代价。 奢侈品史上常有这样的情况，就是原本的奢侈品往往最后会成为必需品，而且带来新的义务。等到习惯某种奢侈品，就开始认为这是天经地义。接着就是一种依赖最后，生活中就再也不能没有这种奢侈品了。让我们用现代大家都熟悉的例子来解释。在过去的几十年里，我们有许多本该会让生活轻松省时又如意的发明，像是洗衣机、吸尘器、洗碗机、电话、手机、计算机、电子邮件等等。在以前，寄信是件麻烦事，得亲手动笔、写信封、贴邮票，还得在走到邮筒那里去寄。想得到回信，可能得等上几天、几星期，甚至是几个月。至于现在，可以随手就寄一封电子邮件，传送到地球的另一边，而且如果收件人在线，可能只要一分钟就能收到回信了。我确实省下了所有麻烦和时间，但生活真的更轻松了吗？ 我：好多你以为的必须其实并不是真正的必须，就像各种结婚条件一样，房子车子票子只是要的人多了，你才把罕见错以为必须了。 释迦牟尼认为，人遇到事情通常会产生欲念，而欲念总是会造成不满。遇到不喜欢的事，就想躲开；遇到喜欢的事，就想维持并增加这份愉悦。但正因为如此，人心就永远不满、永远不安。这点在碰上不悦的时候格外明显，像是感觉疼痛的时候，只要疼痛持续，我们就一直感到不满，用尽办法想要解决。然而，就算是遇上欢乐的事，我们也从不会真正满足，而是一直担心这种欢乐终将结束或是无法再持续或增强。有些人多年来一直在寻找爱情，但等到真的找着了爱情，却还是不满足。有的开始整天担心对方可能会离开；有的又觉得自己太过屈就，应该再找更好的人。 我：释迦牟尼说的很对，这个理论不仅适用于我们普通的日常生活，也适用于感情之间。现在所谓的剩男剩女，其实不就是源于内心的不满吗？欲壑难填，对现有的男人或者女人不满意，想要寻找更好的男人或者女人，然而她自己却不说自己配不配得上的问题。 混沌系统分成两级，一级混沌指的是“不会因为预测而改变”。例如天气，就属于一级混沌系统。虽然天气也是受到无数因素影响，但我们可以建立计算模型，不断加入越来越多因素，让天气预报也越来越准确。至于二级混沌系统，指的是“会受到预测的影响而改变”，因此就永远无法准确预测。例如市场就属于二级混沌系统，假设我们开发出了一个计算机程序，能够完全准确预测明天的油价，情况会如何？可以想见，油价会立刻因应这个预测而波动，最后也就不可能符合预测。例如，假设目前石油价格是每桶90美元，而这个绝对准确的程序预测明天会涨到100美元，商人就会立刻抢进，好在预期的涨价中获利。但结果就是油价会在今天就涨到100美元，而不是明天。那明天究竟会如何？这件事就没人知道了。 我：生活中这种混沌系统太多了，大部分的事情我们都可以套用这个公式。第一类我们称为零反馈事件，比如从众、审美疲劳。你一般不会因为知道自己从了众或别人告诉你在从众而不这样做。第二类是强反馈事件，最典型的是投资策略，如果大伙都知道今年的暴雨会袭击咖啡豆园地，这星巴克就没法炒了，所有的零和博弈都有类似问题，好比某部落举行篝火晚会，一只香嫩的烤全羊就在“口口相传”中消失了。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《精要主义》]]></title>
    <url>%2F2017%2F01%2F20%2FRead-essentialism%2F</url>
    <content type="text"><![CDATA[生之智慧，在于摒弃不必要之事。——林语堂 作者简介：THIS公司创始人，该公司致力于帮助个人和企业将80%的时间用在正确的事情上，免受琐事的困扰。公司客户包括苹果、谷歌、Facebook、皮克斯、Adobe、Twitter等多家知名公司。备受欢迎的撰稿人，文章经常发表在《快公司》《财富》《赫芬顿邮报》《公司》等杂志上。他还是《哈佛商业评论》受欢迎的专栏作家之一。成功的演讲者，足迹遍布澳大利亚、加拿大、英国、日本等多个国家，曾受邀在西南偏南大会上发表演讲，曾在瑞士达沃斯世界经济论坛上采访过阿尔•戈尔，曾受挪威王储的邀请在创新年会上发表演讲。他的演讲内容聚焦于创新、专注力、领导力、精要主义。 最直白的一个问题，什么是精要主义？专注精要事务，可以通向成功；但成功会带来太多的选择和机会，其结果就是最初通向成功的那个专注点土崩瓦解。成功变成了失败的催化剂，它会让人陷入吉姆·柯林斯所称的“盲目地追求更多”的境地。要想走出这种困境，出路就是自律地追求“更少，但更好”。百折不挠、坚持不懈地追求精要事务，排除非精要的一切，建立一个让执行毫不费力的行为体系。 现代这个社会，大家正越来越关注于健康的生活方式，也有越来越多的人加入到了运动的大军中来，但是坚持下来的极少，究其原因，总是抱怨没有时间，然而这是真的吗？当你抱怨没有时间的时候，却抱着手机刷着微博、朋友圈，不知不觉两小时过去了，而这两小时用来锻炼是绰绰有余的。就像精要主义里面说的，锻炼本身不是什么难事，每天决定是否锻炼才是真正让人头疼的。解决办法就是把锻炼发展成一种体系或者常规，使之彻底融入到我们的日常生活之中。 精要主义者和非精要主义者在思考、做法、收获等诸多方面存在着区别，而这种差别不仅导致了穷忙族的诞生，也导致了越忙反而越没有收获。很多时候，包括我自己都把忙视为通向成功之路，然而并不是，在这本书中就提到了，我们生活的世界中，几乎所有的一切都是毫无价值的，只有极少量事物具有非凡的价值，这是我们无法挣脱的现实。精要主义就是要打破这种用忙碌衡量成功的浅见。 简单来说，精要主义做的是减法而不是加法，当然它也不提倡为了少做而少做，而是主张只做必做之事，尽可能做出最明智的时间和精力投资，从而达到个人贡献峰值。精要主义就是对自己的生活进行设计规划，而不是事事依循默认设置。若要最大限度成就真正重要之事，切忌贪多求全，事事应允。其实在生活中，你无需完成更多的事情，你只需要尽力做好对的事情即可。 书里提到了一个日常生活中常见的现象，被作者称之为“成功的悖论”，主要可以概括为以下4个可预见的阶段： 第一阶段，如果我们真的有清晰的目标，它有助于我们取得成功 第二阶段，当我们成功时，就会被冠以“可以去找的人”的称号，成为“老好人”，只要你需要，就总能找到这个人。这样，我们就能得到更多的选项和机会 当我们拥有了更多的选项和机会，实际上也就被要求付出更多的时间和精力，这样便会导致精力的分散。我们被千头万绪的事情牵扯得越来越无法集中精力 因为精力分散，我们无法实现原本可以实现的个人贡献峰值。成功所带来的后果最终瓦解了最初指引我们走向成功的那个真正清晰的目标 这个日常生活中常见的现象是不是似曾相识呢？想要做的更好，付出更多，取悦更多的人，必然需要付出更多的精力，而精力的分散，可能会导致本来处理相当好的事情也无法兼顾，从而不仅无法应付新的事情也丢失了已有的优势。 人之所以为人，就在于选择的能力。——马德琳·恩格尔 精要主义是一种思维方式，不简单的是一种做事方法，要抓住精要主义的本质，需要接受三条核心真理：“我选择我要做的事”，“只有小部分事情是重要的”以及“我能做任何事但不是所有事”。国外科学家做了一个实验解释了“习得性无助”这一现象，简单点说，患有这种障碍的人就是没有发现自己才是拥有选择权的决定者。精要主义者必须高度重视选择能力，将之视为一种战无不胜的力量，它的存在独立于其他任何事物、任何人及任何力量。忽略了选择的能力，就等于选择了无助，这就是习得性无助现象的本质。 所谓战略，就是如何作出选择及取舍。它意味着通过慎重选择来实现不同。——迈克尔·波特 尽管有时取舍会给人带来痛苦，但却代表着一个意义重大的机会。通过强迫自己权衡两者并战略性地选择那个对自己最有利的，就能极大地增加实现自己目标的机会。精要主义者的一大悖论就是相比非精要主义者，精要主义者实际上会探索更多的选择项。精要主义者在专注于某项事情之前，先广泛地探索和评估各种选项，他们最开始时探索更多的选项，就能保证以后作出的选择是正确的。 生活中总是有这样一些人，和他们维护关系要比跟其他人费劲，这些人总是把他们自己的问题当成我们的问题。这就是界限范围不清的典型例子，一定要果断地和这些人划清界限范围，让他们知道我们没有义务去处理任何属于他们自己的事情，这样无休止的被他们纠缠，只会导致我们精疲力竭，最后还会因为没有很好的达到他们的要求，而破坏本来属于我们还不错的关系。无论从何种方面来说，对于这些人要么拒绝要么远离，也不要因为失去了这些人而懊恼，因为这无形中为你节约了大量的精力。不管是谁想要榨取你的时间和精力，来为他自己的目的服务，唯一的解决办法就是划清界限。 给我6个小时砍倒一棵树，我会先花4个小时来磨快斧子。——亚伯拉罕·林肯 在本书的结尾，作者指出了现在我们正在面临的问题，那就是这是一个比以往任何时候都过度互联的时代，智能终端的束缚捆绑和超量信息的入侵干扰已经把人们切分成了越来越细碎的存在，在如何保持自我的完整性以抵御在机械、琐碎和无效的囹圄中一步步滑向无足轻重的危机？这个问题吴军博士在他的智能时代中也提到了，在现在这样一个比以往任何时候都告诉法发展的时代，稍不留神，就会被世界远远地抛在身后，吴军也说在智能时代只有2%的人能够进入到世界的前列，剩下的都会被淘汰，而抱怨并没有任何卵用。人只要还有点儿向前的动力和扑腾的能力，就不会甘于成为庸碌之辈，因此很多人为了最大限度地实现价值而东奔西突。但是任何成功和对成功的追逐都不能脱离具体的、现实的社会条件，因此认清现实无疑是找到出路的首要条件。 因为无法在万事万物中择其精要而为之，还要对各种外部期待和要求俯首帖耳、唯命是从，因此满世界都是一个个方向不明、轻重不辨、气喘吁吁的累人或累人联合体——企业。盲目地追求更多，让本已经负重过度的生活和事业雪上加霜，进而与价值实现的目标南辕北辙。 也许出身际遇天注定，祸福兴衰如天之风云般不可测，修成圣贤、臻于翘楚对于万众而言最终可能只是个终不可及的幻影，但在人力所能左右的疆域里，于抉择取舍之间时刻倾听内心志向之声，于千头万绪中明辨轻重缓急，于万事万物中斩断一切有碍进步之事，藏大局于胸，不为一时一事所缚，进退有度，涵养心智，活在当下，尽享途中之乐，这或许是人人皆可为之的。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《创新者的窘境》]]></title>
    <url>%2F2017%2F01%2F04%2FRead-the-innovator-s-dilemma%2F</url>
    <content type="text"><![CDATA[作者简介： 克莱顿•克里斯坦森：哈佛商学院教授，因其在企业创新方面的深入研究和独到见解，被尊称为“创新大师”。1997年，当《创新者的窘境》英文版出版时，克莱顿•克里斯坦森只是哈佛商学院的助理教授。而此书一出，就确立了他在创新技术管理领域的权威地位 。 就算我们把每件事情都做对了仍有可能错失城池，面对新技术和新市场，往往导致失败的恰好是完美无瑕的管理。 非常值得推荐的一本书，该书并不研究一些管理落后的企业衰败的原因或者并不占据行业主导地位的企业衰败的原因，而主要谈的是为何成功的企业、管理优秀的企业、占据行业龙头的企业、同时也在不断创新的企业最后衰败的原因。前者其最终衰败看来是必然，如若从此点切入，此书必然难成大器；而从后者这一非常困难的角度切入，注定了该书起点不低。另外书中主要以硬盘驱动器这一行业作为研究案例，因为在现实世界中只有该产业急剧发展，不断涌现出大批优秀企业并不断被新的后来者所替代，其周期之短是绝佳的研究案例。整本书逻辑缜密、结构清晰、层层推进，读来不仅心生赞叹。 为什么大企业会失败大企业为什么会失败？实际上，成熟企业在应对各种类型的延续性创新时可以做到锐意进取、积极创新、认真听取客户意见，但它们似乎无法成功解决的问题是为新产品找到新的应用领域和新的市场，这些是企业在刚刚进入市场时所普遍具备，但是在时过境迁后又明显丧失了的一种能力。这些领先企业似乎被它们的客户牵绊住了手脚，从而在破坏性技术出现时给了具有攻击性的新兴企业颠覆主流行业领先企业的可乘之机。 价值网和创新推动力除新技术和创新机构所具有的必备能力外，面临破坏性技术创新的企业必须分析创新对其相关价值网的影响。关键问题是创新活动尚未明确的性能属性在新兴企业已经建立的价值网中是否能得到重视；为了实现创新的价值，是否必须进入其它价值网，或建立新的价值网；市场和技术轨线是否可能最终交汇，从而使无法解决消费者当前需求的技术最终能够解决他们在未来的需求。 机械挖掘机行业的破坏性技术变革更努力地工作、更聪明地管理、更积极地投资、更认真地听取客户的建议，这些都是应对新型延续性技术所带来的问题的解决之道。但这些良好的管理范式在应对破坏性技术时却完全失效，而且在很多情况下甚至还会不利于问题的解决 。 回不去的低端市场一个企业要完成推出新产品这样一个浩大的工程，其后勤、人力和推动力都必须跟上新产品推进的过程。因此，成熟企业并不仅仅受到客户需求的制约，还受到它们参与竞争价值网所固有的财务结构和企业文化的制约——这个制约因素能够湮没任何及时投资下一轮破坏性技术浪潮的理性的声音。 管理破坏性技术变革尽管破坏性技术能够改变基本特征完全不同的各个行业的发展趋势，但在遭遇破坏性技术时，这些行业导致成败的因素都是一致的。成功的企业希望集中资源来开展能够满足消费者需求的活动，因为这些活动能带来更高的利润，在技术上更具可行性，而且能帮助它们保持在重要市场上的竞争力。这些企业也建立了一整套流程来帮助它们实现这些目标，但寄希望于这套流程同样能够成功地培育出破坏性技术无异于古人手缚羽翼、挥动翅膀的飞行痴梦。这些期望违背了成功的企业运作方式和性能评估机制的基本趋势。 把开发破坏性技术的职责赋予存在客户需求的机构在面对消费者明确说“不”的破坏性技术时，管理者应该创建一个独立的机构，使这个机构直接面对确实需要这种技术的新兴消费者群体。有无数实例证明，大多数公司在开发破坏性技术的同时，力图保持它们在主流市场的竞争力，这条道路基本上是一条死胡同；它们在其中一个市场上的市场地位将被削弱，除非企业成立两个彼此独立的机构（从属于相应的价值网）来吸引不同的目标客户。 如何使机构与市场的规模相匹配在延续性变革中，专注于扩展传统技术的性能，并选择在新的延续性技术出现时充当追随者的企业也能继续保持其领先地位和竞争优势。但在破坏性技术变革中，情况则大不相同。在破坏性技术刚刚出现时就率先进入这些新兴市场的企业将赢得巨大的汇报，并建立起明显的先发优势。 发现新的新兴市场没有人——不论是我们，还是我们的消费者——能够在真正使用之前了解破坏性产品是否能够投入使用、怎样使用，或者使用量有多大。一些面临这种不确定性的管理人员更倾向于在其他公司切实找到相关市场之后再行进入。但考虑到破坏性技术的领导者能够建立起巨大的先发优势，面临破坏性技术创新的管理者应走出实验室和跟踪调研小组调研，利用发现驱动型方法进入市场，直接了解有关新消费者和新应用领域的知识。 如何评估机构的能力与缺陷当机构遭遇变革时，管理者首先必须要确定他们是否具备成功所需的资源。然后，他们需要再问一个不同的问题：机构是否具备能成功所需的流程和价值观？对于成熟企业来说，创新之所以总是看起来困难重重，其原因就在于它们聘用了能力很强的人，并将设计初衷与他们肩负的使命不相匹配的流程和价值观强加给他们。在这个日新月异的时代，应对变革的能力已成为事关企业成败的关键一环，确保能者适得其所亦是企业肩负的一项重要责任。 产品性能、市场需求和生命周期在性能过度供给给破坏性技术带来威胁或机遇时，还会导致产品市场的竞争基础发生根本性变化。消费者选择产品或服务时所遵循的各种标准的排序将发生变化，从而标志着产品生命周期从一个阶段过渡到另一个阶段。换句话说，企业提供的性能轨线和市场要求的性能轨线的交汇，是引发产品生命周期从一个阶段过渡到另一个阶段的根本原因。 管理破坏性技术变革：案例研究能力不足、官僚作风、傲慢自大、管理队伍老化、规划不合理和投资短视显然是导致许多企业最终失败的主要原因。但我们已经知道，即使是最优秀的管理者也会受到某些法则的制约，而且这些法则会加大破坏性创新的难度。当优秀的管理者不能理解，或者试图抗拒这些法则的力量时，他们领导下的企业距离失败也就为期不远了。 创新者的窘境：概要《创新者的窘境》一书阐述了破坏性技术取代原有技术的过程，以及管理良好的企业内部所蕴含的强大力量是如何阻碍它们开发破坏性技术的。克里斯坦森教授构建了一个包含四项破坏性技术原则的框架，并以此来解释为什么开发成熟技术上最为有效的管理方法反而会阻碍对破坏性技术的开发。最后，他提出，管理者可利用这些原则，使他们所在的企业能够更加有效地开发代表未来市场发展趋势的新技术。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《拆掉思维里的墙》]]></title>
    <url>%2F2017%2F01%2F02%2FRead-remove-the-wall-of-thinking%2F</url>
    <content type="text"><![CDATA[作者简介：古典，新精英生涯总裁、美国生涯教练国际认证CBCC中国首席导师、GCDF全球职业规划师培训师、CCTV《科技博览》、北京台《天天阅读汇》、凤凰台《一虎一席谈》邀请职业发展专家《中国教育报》《新前程》、新浪教育、中华英才网等媒体职业规划专栏作家。 国内不超过10个同时拥有全球职业规划师（GCDF）、高级职业指导师、注册心理咨询师与企业教练4个认证的生涯发展专家，被业内认为是“中国职业规划界的新一代领军人物”。大学就读土木工程，玩乐队，练散打，骑单车从长沙流浪到北京。想出国却稀里糊涂进入新东方，历任GRE首席词汇讲师、集团培训师，新东方教育与职业发展协会会长。 “从绝望中寻找希望，人生终将辉煌！这是我当年被新东方吸引的原因。但今天的青年，他们不绝望，更多是迷茫。人生终将辉煌，但哪一种是他们的辉煌？我的回答是：成长，长成为自己的样子！”2007年，创办新精英生涯，希望帮助30%的青年人，成长，长成为自己的样子。 Three passions, simple but overwhelmingly strong, have governed my life: the longing for love, the search for knowledge, and unbearable pity for the suffering of mankind.——罗素 关于婚姻的这一段说的非常好，“很多人想通过婚姻找到一种安全感，可现在你会看到很多离婚的例子，都是因为想要找到安全感，而无法得到安全感。安全感绝对不是来自婚姻，如果你是想要安全感而进入婚姻的话，这个婚姻80%会出现问题。因为对方不是一个港口，他不是一个固定的东西，而是一个活着的人，他还会去接触不同的人，他还会在兴趣上改变。所以，你想要在婚姻上得到的东西，你一定要有能力自己给自己，如果不能，这样的婚姻基本上都是失败的。” 在我们是自己生命的巫师一节中，提到我们的心智模式决定了我们能看到什么世界，更加好玩的是，这个自建的“真实世界”，又反过来印证这个模式给我们看。如果我们认为葡萄是酸的，葡萄果然就是酸的；父母觉得孩子坏得都不像自己生的，孩子果然坏得超乎你想象；你觉得“男人没有一个好东西”，那么你就能遇到一系列流着坏水的男人。其实一方面我们生活在这个世界，一方面我们看到的世界也是我们心里的投射，心里阴暗的人看这个世界，到处都是阴暗的；心里光明的人看这个世界，也充满了光明。 一个好的心智模式非常重要，它不仅塑造了你的性格，也能决定你用什么心态去拥抱这个世界。就像现在的独生子女从小就被父母、爷爷奶奶环绕着，他就很容易产生这样的推论：不仅这个家庭，而且这个世界都是以我为中心的。带着这种自我为中心的心智模式进入社会，往往需要吃几次大亏才能调整过来。如果你生在独生子女家庭，那么你要学会自我调整自己的心智模式，如果你只生一个孩子，千万不要让家庭成员都围绕着他一个人转，从小就会导致他不能以正确的心智去看待这个世界。 现代社会的三种毒药：消费主义、性自由和成功学。消费主义以品牌为噱头，以时尚为药效，恋物成瘾。性自由以人性为噱头、以性爱为药效，纵欲成瘾。成功学以速学为噱头，以名利为药效，误导急于走捷径成为人上人的年轻人投身其中，投机成瘾。 对于爱情这种东西，世人在其面前顿时显得智商为零，尤以现在的女孩子为重，“找一个爱我的、包容我的、疼我的、有房有车的……”欲壑难填啊，对终生伴侣的选择上，这些女孩子总是提到对伴侣的要求，对自己的要求却只字未提。好多真正的对的伴侣可能就在她们的一次次要求之中错过了，而这些错过了的将是终生都无法再次得到的。关于什么是爱情，苏格拉底早就说过，看到那个麦田了吗？从里面摘出一颗最大最好的麦穗。但只能摘一次，而且不能回头，这就是我们大多数人面临的爱情境况。很多人摘了一颗麦穗之后，不断的发现好的在后面，另一些人一直不摘，总想着后面会有更好地麦穗等着自己，结果在快走到麦田尽头的时候，才匆忙摘了一颗普通的麦穗。 中国的父母很容易有这样的思维方式，把自己缺失的东西放大，强加于他们的儿女身上。尤其是独生子女的家庭，儿女占用了所有的资源，所以也应该承担他们所有的希望。当资源付出到一定程度，这样一场对儿女爱的绑架就开始布局——如果你不按照我的计划发展，我就要伤心，就要在内心压抑偷偷饮泣。我这一辈子把你养大，现在过得这么累，全都是因为你。这样的父母是悲剧的，遇到这样父母的孩子更是悲剧的，如果处理不当，那么你的人上基本上就是由你的父母安排了，如果处理得当，那必然也是要付出极大精力，这时候父母已经从你的事业助推器变为你事业上的阻碍了，他们已经成为了你事业的第一个拦路虎。 从心理学上来说，一个人缺什么，就会投射到身边人的身上，他会觉得身边的其他人也觉得自己缺。于是他就会不断地表达说自己其实不缺，一不小心就过了。这个结果就是，他不断地表达的东西就是自己最缺乏的——如果你想看一个人缺什么，你就看他不断强调什么就好了。比如有人说，你是一个笨蛋，往往不聪明且心虚的人马上反驳，你才是笨蛋！真正的智者会微笑着回答：是的，所有人都是愚者。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016年终总结]]></title>
    <url>%2F2016%2F12%2F31%2F2016-year-end-summary%2F</url>
    <content type="text"><![CDATA[时间真快，一年又到了收官的时刻了，这一年也是一个多事之秋，从橙黄大战（摩拜 VS. ofo）再次挑起O2O领域又一大战；到王石大战宝能系，来来回回，拉锯战打的不亦乐乎，请全国人民看了个宫斗剧；再到王健林定了个小目标，比如“先挣它一个亿”，逗乐了全国人民；都快年底了，乐视又出来秀一把，先是资金链断裂，后是商学院土豪同学注资几个亿，让乐视打了个翻身仗，倒了也能爬起来；整个国家大事太多，说也说不完，不过都是别人的事，和屌丝们关系其实不大。马上新的一年就要开启了，一时不知从何写起，毕竟每一个独立的个体都有一段精彩的人生，那么作为个人总结就仅仅记述我自己罢了。 这一年逻辑思维可以说是大火特火，用罗振宇的话说，知识服务的时代到来了。从2016年5月28日，“得到”上线3个订阅专栏产品开始，陆续推出了共15个专栏产品，截止12月3日，189天的时间，订阅专栏产品营收突破1个亿，订阅数量近80万份【包括1元/年《罗辑思维》29万份】，相当于订阅产品平均每天收入53万，这个成绩让很多媒体人和知识工作者都羡慕不已。当然我也羡慕不已，罗胖凭此轻松实现财务自由，而这正是我一生的追求。由此我猛然想到，其实每一个程序员都是一个隐形的富豪，只不过他们被彻底的忽悠了，我越来越同意比尔·盖茨的观点，软件不能随意开源。原因何在？任何劳动成果都应该有所收益，这样才能不断地激励人去继续创造成果，而开源软件呢？作为程序员的智力劳动成果，被无偿开放给全世界，无需付费，任何人都能够看到开源软件的源代码，而作为这个智力劳动成果的程序员却没有任何收益，当然了那种虚名除外。试想对于一个读了多年书拿到计算机学位的人来说，付出的时间、精力、金钱必然无数，做出来的产品却要开放源代码供别人使用，这从任何一个角度来说都是一个不合逻辑的事情，偏偏这个世界整天都在鼓吹开源，只有当年的比尔·盖茨与全世界叫板，软件必须付费，这才有了今天的微软帝国，倘若当年盖茨经受不住忽悠，将Windows免费分发给全世界使用，恐怕微软撑不过三年就要关门大吉了。所以作为程序员的一份子，每一个写程序的人都不应该盲目去推崇开源，更不要以此作为道德制高点去歧视打压任何不开源的开发者。最后我的观点是不要盲目的鼓吹开源，也不绝对地反对开源，对于大的基础性的项目，开源有时可以促进一个产业的急速发展，这也是应当能看到的开源带来的好处。在这里重申一下开源软件的定义“Free is not Free”（自由而非免费），但是多数国人却都把开源当作免费了。 把思维拉回来，再来说说知识服务这事，好多人以为花钱就能买到知识，比如得到的诸多订阅用户们。你们只是买到了知识的阅读权，但是这个知识却不属于你们，即便你读过订阅的内容，可能过段时间，当你碰到同样的问题，你依然不会解决；碰到同样的困境，你依然不知道该用哪种知识解决。更有甚者，在付费订阅了诸多得到产品之后，很大一部分人是没有坚持学习的，多数人图个新鲜度以及心理安慰的作用，觉得自己付费订阅，就每天都在进步了。多数人在使用几天后，再也不记得他订阅了付费产品，自然付费订阅的效果是零，这也是知识服务的一大特点，即无需售后，知识卖掉了，钱到手了，这笔买卖就结束了。当然了，这种用户是一种极端，他们仅仅停留在付费买心安的阶段，另外还有一种极端，在我的朋友圈就出现了这种人，在得到上看了谁的文章，阅读了谁的专栏，订阅了谁的栏目，都要长篇大论，指点江山，貌似这都是在他的运筹帷幄之中一样。天天只会坐地意淫，以为自己又涨了多少本事，殊不知，在我看来，你只是涨了吹牛的本事，千万要以此为戒，莫以为说得出和做得到是一回事，你纵然谈天说地，但从不脚踏实地迈出一步，我可以说，你是无法成功的。阅读、吸收知识纵然是一方面，更重要的是迈出你的步子，开始行动，“种一颗树的最好时机是十年前，其次，是现在”。 这一年，又大了一岁，感情生活却并不顺心，慢慢地觉得古人说的先成家后立业，其实是非常有道理的。任何一个人的精力都是有限的，这必然导致你的注意力是最为宝贵的资产，谈恋爱消耗了你的大量注意力，但是不一定有成果。所以一旦成家之后，内室已定后方安稳，就可以收心集中你的注意力，专注于事业了。现在空巢青年越来越多，这不是一个好现象，在什么阶段就应该做什么事，最佳的生育年龄在20-30之间，倘若大家都在30-40岁生育，其后代的质量并非最佳。这种空巢青年是随着独生子女的增多而渐渐增多的，以后剩女会越来越多，剩男也会被动地变得越来越多，每个人的妥协及包容度不变下降，导致对方很小的缺点都难以忍受，离婚分手变得像吃饭一样简单，毫无仪式感。并且伴随着性观念的开放，传统的好女孩越来越少，你不可能说性观念开放的女孩比性观念保守的女孩更好，我实在不知道理由在哪？根据劣币驱逐良币原理，坏的社会风气也会驱逐好的社会风气，因为在坏的社会风气之下，假设你能够获得倾向于分配给自己更多的收益，自然也就没人去追逐良好的社会风气了。都说大学附近是宾馆的天堂，此话不假，每晚不知道多少花季少年花季少女在此偷吃禁果。曾经在一本书中，读过一个研究成果，说明了非洲人的智力低下与他们的乱性有关，书中列举了大量的早已灭亡的古国或民族，都有过乱性的历史，这些早期取得辉煌发展成就的民族，在后期却变成了智力低下的民族，乱交及乱性，一定会导致后代的智商降低，这是有科学佐证的。所以男人啊，如果有可能，一定要找个保守的好女孩。一个好女孩如果能把一个完整的自己留给她的老公，我觉得这就是她最贵重的嫁妆。另一个对比的例子就是犹太人，性观念非常保守，除了文化水平普遍较高、家庭教养好之外，这也与其宗教信仰有关，犹太人一般极力排斥堕胎，必然导致他们对自己的性行为更负责，也导致他们的后代在智商上占有更大的优势。智商这东西确实是个神奇的东西，比如说你们俩都是面条师傅，智商高低对你们影响不大，但是当上升到造原子弹的高度，没有高智商是肯定玩不转的，这也解释了唯有犹太人仅靠十万人就建立了以色列，仅靠一小撮军队就赢得了几次与整个阿拉伯联军的战争，没有他们的高智商，不知道以色列被灭国多少次了。 这一年也是微信公众号大爆发的一年，爆发的公众号导致了过度的营销，过度的营销导致了不断的去迎合趣味低俗的用户，从而导致鸡汤文和脑残文章越来越多，这也说明大多数人的智商都是非常低的，这是真的，你不要偷笑，汉族人的智商在世界上处于一个相对滑落的阶段。不知道哪个大牛说的，一个不会阅读的民族是没有未来的，这让我突然脑补了俄罗斯人忍受饥饿，依然在安静排队领面包的情形，要是在天朝，呵呵，你懂得。我这里说的阅读自然不是那些鸡汤问和脑残文，而是说的书籍，这也是我不断阅读的原因所在。比如一篇很火的鸡汤文说中国男人配不上中国女人，我在想要脑残到何种地步的人才会有这种想法呢？这其实在进化学上早就被解释过了，古代女人一旦生育之后，在养育阶段是没有猎食能力的，必须靠男人，而男人的猎食能力取决于体格而不是容貌，所以几万年的进化方向是，女人变的越来越漂亮，吸引男人将打到的猎物带回家，这样女人才能存活下去，而男人的进化方向是完全无需改进任何容貌上的进化。所以女人比男人好看，这是天生的，不存在配不配的上的问题，这是进化决定了的。但是正是这种噱头多、槽点多、喷点多且不需要动脑子还能引起男女撕逼的文章大行其道，吸引了诸多用户的目光，不得不说，这个时代，恶趣低俗的文化已经得到了越来越多的流行。 这一年工作还算舒心，主要是活不多，自由时间较多，可以尽可能干我想干的事情。公司技术氛围一般，大牛不多，重业务轻技术。人员素质也不是特高，最最重要的是，不给配大屏显示器，不给配Mac，这肯定不是技术驱动型公司的作风。目前公司的显示器是21寸的。要知道大屏显示器可以极大的提高开发人员的效率，随便一点效率的提升，都值回你的显示器价格，浪费在多窗口间切换，这就是对智力的浪费。如果你开公司，请记住，一定要尽可能给技术人员配上大屏显示器，因为他会用工作效率的百倍提升来回报你所付出的那点成本。从毕业入职到现在，在公司做了一点业务开发，没什么可说的，没有通用性，换了一个公司代码就用不上了。唯一拿得出手的就是做了个基于Redis的Lucene索引存储系统，将Lucene索引文件直接对接Redis数据库，简单的测试了下性能介于RAMDirectory（基于内存的索引存储）和FSDirectory（基于文件系统的索引存储）之间，还算可以接受。该项目RedisDirectory已经推送到Github上了，希望能给有此需求的人带来一些帮助，当然如若你能赞助我一点，那将是极好的。 这一年理财能力也在稳步提升中，越来越觉得多数人在金融知识方面的欠缺。个人理财平台也从最早的余额宝到后来的理财通，又转战到京东金融，再到陆金所，再到其它P2P，现在到了现货黄金，当然投资有风险，入市需谨慎。如果有机会的话，我个人是很想读个经济学或金融学硕士，过去十多年其实是一个资产大泡沫的年代，说的简单点，就是手里有资产的人收益最大，包括各种实物资产，另外一个就是通胀完全跑赢基准利率，也就是说，过去十年谁欠银行钱最多，谁的收益最大，想想吧，十年前如果你借个十万块，那可是真当钱花啊，在今天你再还掉十万块，是不是轻松地一逼啊。所以货币只要保持这个发行速度，那些贷款上百万买房的人，等十年之后其实房贷是非常轻松的，这部分人也比没有实物资产的人能获得更多通胀所带来的利益。 这一年我开始健身了，办了五年的会员卡，说长不长，说短不短，无论是上至特权阶层，下至平民百姓，只有身体才是最重要的，最近北京又闹雾霾了，只想简单的感谢一下雾霾，你是唯一一个让帝都和全国人民处于平等地位的东西了。身体是跟随自己一辈子的，所以尽量不要亏欠自己的身体，否则将来年纪大了，躺在床上动弹不得，后悔药也买不到的。既然办了卡，就要坚持下去，三天打鱼两天晒网是万万要不得的。同时也发现了诸多本末倒置的人，健身衣买了几千块钱的，运动一下就要吃蛋白、健肌粉、鸡胸肉，关键是你倒是坚持运动啊，我啥也不吃正常饮食，普普通通百来块的篮球运动衫一样可以练出肌肉，其实你只要做对一件事就够了，那就是准时出现在健身房。 这一年对跨界有了更好的认识，从个人角度来说，一定要多去结识不同行业的不同的人，千万不要让自己的朋友圈都是同一行业的人，只有跨界才会产生提升效率的催化剂。为什么程序员多屌丝，这与他们的出身有很大关系，比如身边有些屌丝，吃个饭也纠结这个贵些那个便宜些，这是对注意力的极大浪费，虽然我算不上土豪，但是在吃这种生活必须品上不要太过计较，只要安全卫生即可。如果你真想节约，可以在衣着及电子产品上稍微节约一些，因为衣着随潮流而动，风格变换很快，同样电子产品更新换代更快。一个良好的出身和一个富裕的家庭，对一个人的成功真的很重要，这些屌丝可能金钱能力提升了，但是思维没有提升，还停留在那个物质匮乏的年代，记得李笑来说过，最划算的买卖是用金钱换时间，凡是能用钱解决的问题就不要去花时间。远离屌丝，因为他会拉低你的生活品味，同时会消耗你的注意力，每个不断进取的人都要对自己的思维进行不断迭代，提升自己，一个人包括思想也是能实现自我净化自我更新的。目前在跨界交友上我做的并不好，这也是以后需要重点改进的一个方向，程序员你们懂的，天天和电脑打交道，没时间没精力也没机会去跨界交友，目前我也没有找到什么好的办法去认识不同行业的不同的人。 这一年我还没有成家，希望自己能早日成家，好安心立业，对于那些要房要车才结婚的女孩，我就不说了，你们赶紧去找有房有车的吧，就别浪费我时间了。在上海这个中国最大的超级都市，房子的梦离我依然很遥远，户口虽然落在了上海，但是我随时可能迁走，如果这里真的容不下我，何必对此眷恋呢？小沈阳说过，人的一生可短暂了，眼睛一睁一闭，一天过去了，眼睛一闭不睁一辈子过去了，能留在上海固然很好，但是如果实在没那个水平，也就不要硬撑了，让自己过得轻松些舒服些也是一种值得追求赞扬的生活方式。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《技巧：如何用一年时间获得十年的经验》]]></title>
    <url>%2F2016%2F12%2F30%2FRead-skills-how-to-get-a-decade-of-experience-in-a-year%2F</url>
    <content type="text"><![CDATA[花了一天的时间看完了《技巧：如何用一年时间获得十年的经验》，很励志的一本书，里面有很多小故事非常好，特别适合处在人生低谷的人去阅读。这是一个体重曾经达到260斤的大胖子写的，也是一个在互联网行业浸淫了十几年的老人物了，看这些人写的书，其实是非常划算的，他把十几年积累的经验写下来，而你只要花很少的时间就能够体味一段别样的人生，这不是无形中延长了你的生命吗？你并不需要去真实的走过他们所走过的路，但是你依然可以品味到那段人生，就像是为以后你自己的路增添了一些路灯，使你可以更顺畅平坦地走下去。 从这里学到了一个简单提高英语听力的方法，就是去听英文的Podcast(播客)，比较简单的有English as a second language Podcast(英语作为第二语言播客)，然后是ATP(Accidental Tech Podcast，偶然科技播客)，如果有能力完全听懂，那么继续听EconTalk(经济对话)、PlanetMoney(星球货币)、This American Life(美国生活)等等。语言的习得能力是一个渐进的过程，只要抱着日拱一卒，不期速成的心态就终会有收获。 发现优秀的人都是喜欢阅读的人，这也更加坚定了我持续不断阅读的信心，总的来说，我很认同郝培强的说法，读书可以改造我们的思维，给我们新的思维模型；另外在现有的思维模型下，读书可以给我们数据，让我们对现有模型更精通，更确信。看书不仅是一个追求数据增长的过程，最重要的是追求模型增长。这个世界没有绝对真理，所有的信息都散落在世界的各个角落。我们慢慢地读书学习成长的过程，就是一个不断汲取这些东西的一个过程，随着你越来越逼近这个世界的真相，你就会越来越有能力，所以核心还是怎么看待这个世界的问题。 在这个碎片化的时代，为什么越来越多的人沉迷于微博、微信、游戏、直播等等，在这本书中也谈到了这个问题，最根本的原因不是因为沉迷，所有沉迷于一种坏习惯的人都是因为没有真正值得做事情，如果真正有这样一种事情让他全情投入，那么根本不会沉迷于这么多无聊的事情之中而无法自拔。对事情没有兴趣、没有追求、没有激情……，那么自然而然就会陷入无聊之中，而一旦陷入无聊，必然会沉迷在其它的东西里面。 其实随着自媒体的壮大，任何人都可以发声，并且可以将之向外传播，从而导致现在的信息过剩也可以说是噪音过剩，但是在我看来，真正有价值的信息还没有达到过剩的地步。在这些过剩的信息之中大部分的都是垃圾信息而已，尤以腾讯新闻、百度新闻等为最，充斥着大量虚假不实的报道。如果你不加辨别，过剩的信息必然导致你吸收的垃圾信息过剩，而你真正需要的信息其实并没有吸收多少。很多人一说起段子来，个个都是段子高手，但是碰到长文，没有几个人有耐心看完的，就更别提读书了，这个社会，真正能够静下心来读书的人越来越少，而能够真正完整塑造你的知识体系的方法就是读书。我也很认同郝培强提到的观点就是——现在看书的人很少，你只要看书，就可以脱颖而出。通过读书获取完整的知识体系，而体系化的知识可以倒逼大脑进化，从而实现大脑的不断成长。如果你每天都花大量的时间看几万条的微博和朋友圈，倒不如找一本薄薄的100页的好书带来的成长更大。 最后附上本书心障一节的收尾，同时也作为本篇读书笔记的收尾。 寻找和突破心障的方法是寻找一种对美好世界和美好人生的渐进解，首先我们承认对这个世界的终极一无所知，但是我们知道近一点儿比远一点儿更好，我们不知道完美世界的图景是什么，但是我们可以一点儿一点儿努力去接近它。我们不知道目标在哪里，但是从渐进解出发，我们永远可以找到一个方向，一个清晰的方向，它可以告诉我们，我们一直在前进，一直没有停息。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《自控力》]]></title>
    <url>%2F2016%2F12%2F24%2FRead-the-willpower-instinct%2F</url>
    <content type="text"><![CDATA[作者简介： 凯利•麦格尼格尔教授（Kelly McGonigal, Ph.D.）是斯坦福大学备受赞誉的心理学家，也是医学健康促进项目的健康教育家。她为专业人士和普通大众开设的心理学课程，包括“意志力科学”（The Science of Willpower）和“在压力下好好生活”（Living Well with Stress），都是斯坦福大学继续教育学院历史上最受欢迎的课程。她还为《今日心理学》（Psychology Today）杂志网站开设了“意志力科学”博客。她目前居住在加利福尼亚州的帕洛阿尔托市。 在职业生涯还未开始之时，职业病悄然袭来。遂办了五年的健身卡，虽然自认自控力较强的我，慢慢地也有懈怠的时候，好在一周三次，基本都准时出现在健身房中了。以前在没坚持健身之前，总觉得坚持下来应该不是难事，想想那些绝大多数坚持不下来的人，心中总是不齿与他们为伍。后来随着时间渐久，偶有疲倦之时，便心生退意，心想都坚持那么久了，不去一天也算不了什么，但是有了第一次就会有第二次，所以万万不可对自己放松警惕，我也成为了健身房中为数不多准时出现在健身房中的人，而那些在健身房中只有一面之缘的过客，往往见了几次之后就再也碰不到了，不是我们时间错开了，我想只是他们不再坚持健身罢了。希望读完此书，能够了解有关自控力的未知始末，在看透自控力的基础之上，也能进一步加强我的自控能力。 自控力主要讲述了什么是自控力？自控力如何发生作用？为何自控力如此重要？这本书用了大量的示例及实验去解释我们生活中可能遇到，但是却不知道其原因的事情，虽然很多实验都及其专业，但是读来略微枯燥，作为非专业人士，我不需要知道这么详细的实验及说明，我只想要知道结果，姑且认为我是一个结果导向主义者吧，因为这样最节约时间，效率也最高。 为了成功做到自控，你必须知道自己为何失败。 现代社会，意志力不但区分了人和动物，也区分了每一个人。 每个人的意志力都是与生俱来的，但有些人的意志力更强。 神经学家发现，如果你经常让大脑冥想，它不仅会变得擅长冥想，还会提升你的自控力，提升你集中注意力、管理压力、克制冲动和认识自我的能力。一段时间之后，你的大脑就会变成调试良好的意志力机器。 意志力实际上是“我要做”、“我不要”和“我想要”这三种力量。它们协同努力，让我们变成更好的自己。 科学家说，提高意志力最有效的方式就是锻炼。如果你觉得锻炼太累了，或是没有时间锻炼，那么不妨将锻炼当作恢复体能和意志力的方法。 长期睡眠不足会影响意志力。大脑中掌控意志力的主要部分是前额皮质，睡眠不足间接影响到前额皮质，前额皮质受损就会失去对大脑其他区域的控制。好在这些反应是可逆的，如果睡眠不足的人补上一个好觉，他的前额皮质就会恢复如初。 如果你明知道自己能获得更多的睡眠，却没法早点入睡，那就不要想睡觉这件事，想一想你到底对什么说了“我想要”。这个意志力法则同样适用于你想逃避或拖延的事——当你不知道自己想做什么的时候，你或许需要知道自己不想做什么。 因为自控需要大量的能量，很多科学家都认为，长时间的自控就像慢性压力一样，会削弱免疫系统的功能，增大患病的概率。 正如慢性压力会影响健康一样，试图控制所有的思想、情绪和行为也是一剂毒药，会给你带去过重的生理负担。 意志力是种生理本能，它和压力一样，通过不断进化来保护我们不受自身伤害。 如果你想彻底改变旧习惯，最好先找一种简单的方法来训练自控力，提高意志力，而不是设定一个过高的目标。 有足够的证据表明，日常所需的自控力会消耗意志力，而我们需要这些意志力来抵抗日常的诱惑。 自控力就像肌肉一样有极限。自控力用得太多会疲惫，但坚持训练能增强自控力。 不要陷入“道德许可”陷阱，比如你去了健身房就会允许自己多吃点，比如你捐了点善款之后，就更容易允许自己去商场进行一次大采购。 另一种典型的陷阱是“向明天赊账”，比如今天我先抽根烟，但从明天起戒烟；我今天先不去健身了，但我保证明天会去；我先买上一些节日礼物，但之后三个月绝不购物。 当我们将意志力挑战看成衡量道德水平的标准时，善行就会允许我们做坏事。为了能更好地自控，我们需要忘掉美德，关注目标和价值观。比如，因为乘坐飞机头等舱而对环保感到愧疚的旅客会多付航空公司一些钱，让航空公司到南美洲种几棵树。这使得他们觉得自己有了“绿色许可”，进而可以更加心安理得地选择头等舱。 我们的大脑错把奖励的承诺当做快乐的保证，所以，我们会从不可能带来满足的事物中寻找满足感。 情绪低落会使人屈服于诱惑，摆脱罪恶感会让你变得更强大。 失败的时候，请原谅自己。面对自己的挫折，持同情自我的态度，以免罪恶感让你再次放弃抗争。 乐观的悲观主义者更有可能成功。预测你什么时候，会怎样受到诱惑和违背承诺，想象一个不让自己放弃抗争的具体方法。 我们无法明确地预知未来，这为我们带来了诱惑，让我们拖延着不做某事。 自控力可以传染，如果一个人戒烟了，那么他家人和朋友戒烟的概率也会增加。 自控受到社会认同的影响，这使得意志力和诱惑都具有传染性。 “讽刺性反弹”可以解释了现代人的很多失败案例：失眠患者越想入睡，就发现自己越清醒；减肥的人拒绝碳水化合物，却梦到了沃登面包和澳洲坚果曲奇；忧心忡忡的人试图摆脱焦虑，却一次又一次陷入对灾难的幻想。压抑人的本能时，就会产生这种讽刺性反弹效应。 试图压抑自己的想法、情绪和欲望，只会产生相反的效果，让你更容易去想、去感受、去做你原本最想逃避的事。 驾驭冲动。当冲动一直存在时，与这些生理上的感觉共处，像驾驭海浪一样驾驭它，不要试图摆脱它，但也不要将冲动付诸行动。 如果说真的有自控力秘诀，那么从科学的角度来说确实有一个，那就是集中注意力。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《腾讯传》]]></title>
    <url>%2F2016%2F12%2F21%2FRead-tencent-biography%2F</url>
    <content type="text"><![CDATA[种一棵树最好的时机是十年前，其次，就是现在。 作者简介：吴晓波：著名财经作家，“吴晓波频道”、蓝狮子出版创始人，常年从事中国企业史和公司案例研究。著有《大败局》I和II、《激荡三十年》《跌荡一百年》《浩荡两千年》《历代经济变革得失》等广具影响力的财经书籍，著作两次入选《亚洲周刊》年度最佳图书。 在马化腾14岁生日的时候，他向家里索要一台专业级、80mm口径的天文望远镜，那要花他父亲将近4个月的工资。虽然父母刚开始不同意，最终还是满足了他的愿望。为了培养儿子的科学兴趣，马家在马化腾很小的时候就订阅了《我们爱科学》等科普杂志。 我：突然觉得任何事情好像在冥冥之中自有天定，试想在那个年代，能掏出那么多钱为孩子买玩具的家庭能有多少？又能有多少父母在那个年代就开始为孩子订阅科学杂志的？在86年马化腾就亲自观测了哈雷彗星，而我直到九几年也不知道哈雷彗星的存在，所以每个人能做多大的事，有一部分在你出生的时候就决定了，这部分人在当今的天朝占有很大的比例。另外剩下的一部分不是占了天时，就是占了地利，单单想靠自己的努力工作，循规蹈矩的去获得财富，在如今的中国是办不到的。你能看到的成功人士并没有向你展示他获得成功的全部因素，一部分是家庭因素，一部分是上一代人的先天优势，当然也有一部分灰色的产业。所以即便是你知道成功者的套路，你也很难成功，因为你不具备他成功的条件。在小马如此年纪，家人就细心栽培，即便不创办腾讯，小马也能干出其它大事来。记得知乎上有个匿名回答，获得诸多好评，他说“孩子在真正接触社会之前，是站在父母的肩膀上看这个世界的，父母能够站多高，很大程度上决定了孩子的眼界、知识面。富贵人家的孩子，最宝贵的财富就在于，他们的父母比你的父母知道的更多，能够在人生的关键时刻给他们更多的指导，让他们少走弯路甚至走捷径，而穷人家的孩子，没有这些指导，只能自己慢慢扑腾，慢慢摸索，也许碰的头破血流之后成了，也许头破血流之后死了。”国外也有诸多实验，论及父母的经济条件对孩子的影响，在BBC《人生七年》这部纪录片里，上流社会的John和Andrew，他们年仅7岁时，就已经明确知道自己会上顶级的私立高中，然后读牛津大学，再然后进入政坛，他们7岁时就已经养成了阅读《金融时报》、《观察家》的习惯。上流社会的孩子从小就具备精英意志，他们按照精英的方式生活和要求自己。49年之后，56岁的John成为了企业家并致力于慈善事业，Andrew成为了律所合伙人，他们的孩子继续接受着精英教育。而底层社会的孩子，从小就没什么梦想，长大后靠出卖劳动力为生，他们的儿女也重复着父辈的生活方式。 有两位出生于1973年的斯坦福校友上门找到杨致远，想要把自己的一个搜索技术以100万美元的价格卖给雅虎，杨致远优雅地拒绝了他们。9月7日，拉里·佩奇和谢尔盖·布林被迫在加州郊区的一个车库内孤独创业，他们把公司取名为Google。 我：杨致远的这个失误，成为了他一生的梗，这个梗充分的证明了，即使你运气好，先行一步，但是远见却更加重要，看看如今的Google和雅虎，从当年的最受追捧和籍籍无名，到如今的角色互换，不得不说这是一个多么惨痛的教训。 在腾讯的历史上，乃至中国互联网史上，QQ秀都堪称一款革命性的收费产品，它可以被视为互联网产业的一次“东方式应用创新”。腾讯不是这一创新的发起者，可是它却凭借这一创新获得真正商业上的成功。 我：可以说真正彻底解决了腾讯盈利能力的业务就是QQ秀，这比腾讯之前推出的任何一项收费服务所带来的利润都要高得多。这款从韩国sayclub借鉴而来的产品，不仅在异国他乡获得流行，更是彻底改变了中国互联网的格局。腾讯从一家互联网二线梯队中的公司，逐渐成长为中国互联网行业中的佼佼者。尽管后来有诸多大佬在IM领域对QQ发起围剿，但都以失败告终，其最终极的原因在于“当人们在一个世界——无论是现实的还是虚拟的——里完成了自我身份的认定后，迁徙将是一个非常困难的任务”。（注：移动梦网业务也带来大量利润，但该业务是由中国移动主导，不能归属于腾讯） 马化腾把腾讯的渐进式创新解释为“小步快跑，试错迭代”。在他看来，也许每一次产品的更新都不是完美的，但是如果坚持每天发现、修正一两个小问题，不到一年基本就把作品打磨出来了，自己也就很有产品的感觉了。 我：读到这里一股熟悉的味道扑面而来，这不就是《精益创业》里面谈到的吗？原来小马哥早已把理论应用于实战了。 对于中国移动不断绞杀移动梦网的合作伙伴，就像：“一个地主圈了一块特别肥沃的地，一开始招募了一群佃农，自带耕牛和农具来开发。土地被耕耘出来了，地主不乐意再跟别人分享果实，就想办法把佃农们通通赶走，自己添置了大量的耕牛和农具，自得其利。后来发生的变化是，突然出现了拖拉机。那些被赶走的佃农们用新的机器和工具开出了更多的地，结出了更多的果实。 我：我不能再表达更深一步的赞同了。对于中国这些可恶的国企和垄断公司，只会民脂民膏，在创新及为社会创造价值方面毫无建树，就日常生活而言，只有去国有银行要等几个小时，只有去国有电信营业厅要等几个小时，只有去国有医院要等几个小时，这种例子不胜枚举，如今中国的各种问题都能从这些懒惰的国企身上找到影子。 在北京腾讯会所，凯文·凯利对马化腾说，“失控”不是指混乱无序、低效率甚至自我毁灭的状态，蚂蚁群、蜜蜂群这样由巨量个体构成的组织体，能够呈现出高度的秩序和效率，不是因为蚁王、蜂王的控制，而是得自于一种自下而上的大规模协作，以及在协作中“涌现”的众愚成智、大智若愚的“集群智能”。 我：感觉这个正是腾讯目前在践行的信条，例如微信在成立之初从没有一个人从上到下的去推动它，甚至计划它，在腾讯内部有两个微信项目在并行存在，都是自下而上的自发组织开发，只不过张小龙的微信早了一步，笑到了最后。我非常欣赏这种组织的公司，因为它能创造无限可能。而任何其它的公司只要是从上到下去运作的，可能也会有一些成功的项目，但是绝对不会创造出有无限愿景的产品。 在对话的最后，马化腾问凯文·凯利:“在您看来，谁将会成为腾讯未来的敌人？”“哎，这是一个至少价值1亿美元的问题。”KK笑了起来，他的回答仍然是经典的“失控式”的，“在互联网世界，即将消灭你的那个人，从来不会出现在一份既定的名单中”。 我：虽然不希望腾讯倒闭，但是我真的很期待这么一个公司的出现，那将是一个绝对更加精彩的崭新的互联网世界。 在一次交流中，马化腾很感慨地讲过一段话，他说：“不管已经出现了多少大公司，人类依然处在互联网时代的黎明时分，微微的晨光还照不亮太远的路。互联网真是个神奇的东西，在它的推动下，整个人类社会都变成了一个妙趣无穷的实验室。我们这一代人，每个人都是这个伟大实验的设计师和参与者，这个实验值得我们屏气凝神，心怀敬畏，全情投入。” 我：这也是我自始至终坚定地选择互联网公司的原因，从第一份工作开始，就决定只去互联网公司。虽然我默默无闻，籍籍无名，但是就像一头捕猎的猎豹一样，潜伏在草丛中，时刻警惕着，一旦时机成熟，纵身一跃……]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《鞋狗》]]></title>
    <url>%2F2016%2F12%2F19%2FRead-shoe-dog%2F</url>
    <content type="text"><![CDATA[作者简介：菲尔·奈特(Phil Knight) 耐克创始人，全球最具影响力的企业家之一；毕业于俄勒冈大学，在斯坦福大学获得MBA学位； 1964—2004年担任耐克公司总裁，2004—2016年担任耐克公司董事长； 源于24岁时的一个疯狂想法，他用向父亲借来的50美元创业，一手将耐克打造成一个年收益超过300亿美元的企业帝国； 他所缔造的耐克标志，已经不仅仅是一个商标，更是一个优雅和伟大的文化标志，是在全世界每个角落都能被立即认出的为数不多的标志之一。 Let everyone else call your idea crazy……just keep going. Don’t stop. Don’t even think about stopping until you get there, and don’t give much thought to where “there” is.——菲儿·奈特2014年于斯坦福大学商学院 缘起之所以读这本书，是因为看了比尔·盖茨的荐书单，作为世界首富，盖茨每年都会向人们推荐他这一年来读过，且认为相当不错的书。目前，盖茨在《2016年，我推荐这些书给你》中写道 直到现在，阅读依然是我最喜欢用来了解一个新课题的方法。从儿童时代开始，我就平均一周读一本书。即使忙到不可开交，我也尽量挤出多一些时间阅读。——比尔·盖茨 我想，作为世界首富，你应该没有盖茨忙吧？即使这样他依然保持着一周一本书的习惯，请问我们这些屌丝还有什么理由不读书呢？都说人丑就要多读书，我说，人穷更要多读书。在今年他推荐的五本书中，另外四本是《弦理论》、《基因》、《强势领袖的神话》和《电网》。 该书获得多方赞誉，其中写推荐语的就有：联想集团创始人柳传志、创新工场董事长兼首席执行官李开复、真格基金创始人徐小平、真格基金联合创始人王强、信中利资本集团董事长汪潮涌、秦朔朋友圈新媒体平台及中国商业文明中心发起人秦朔、第一财经CEO周健工、《李翔商业内参》出品人李翔、和同资本合伙人张涛、新浪高级副总裁魏江雷、跑哪儿科技联合创始人田同生等等诸多大腕联合推荐。 第一部分你瞧，在我们这里，得拼命奔跑才能保持在原地。要是想到别的地方，那就得再快一倍才行。——刘易斯·卡罗尔，《爱丽丝梦游仙境》 在剑道中，只有在心中不再被我和你，不再被对手和他的剑，不再被自己的剑和使剑的方法所困扰时，才能达到最完美的状态……一切都是空虚：你自己、挥舞的剑和舞剑的胳膊，即便是空虚的想法都不再存在。 奈特在提到他的启蒙教练鲍尔曼同时也是他的合伙人的时候，有段话说的非常好，鲍尔曼说“人们有种错误的想法，那就是只有杰出的奥运会运动员才称得上是运动员，但他觉得每个人都是运动员。只要你身体无碍，就可以运动”。是的，身体是我们唯一容易改变并且会跟随我们一生的东西，有一个好身体非常的重要，不管你是打算做一番伟业还是平淡一生。这就像是一笔回报丰厚的投资，只要每天适量的运动，就能保持你拥有一个健康的体魄，在你以后的生活和事业上，给你提供不断的动力与支持，还有比这更合算的买卖吗？ 第二部分从代销鬼冢虎到推出自己的品牌NIKE，这中间经历的困难重重，但是都没有打垮这个年轻的朝气蓬勃的NIKE公司，直至一个里程碑式的产品出现，彻底奠定了NIKE的王者风范——“气垫鞋”。在年轻的NIKE创办之前，如今依然耳熟能详的品牌都早已存在，例如阿迪达斯、匡威、彪马等，所以与它们相比，NIKE是一个绝对的后来者，而如何从一个后来者到不断超越前辈，进而成为世界之冠，才是一个绝佳的精彩的学习典范。 有一个非常精彩的片段是，开始有人仿制NIKE鞋了，也就是常说的高仿品。在没有任何技术人员的指导下，这个仿制品的细节和做工都堪称完美。于是奈特果断的写信给仿制品老板谋求合作，让他们成为了NIKE的代工商。这不失为一个处理仿制品的方法。命运总是青睐勇者，以及诸如此类的事物。 在年近古稀的时候，奈特的长子因为潜水而意外身亡，使得耐克公司的接班人问题变得更加迫在眉睫，挑选合适的继任者一直是大公司掌门人的共同挑战，岂止是公司，上升到国家亦然如此。历史的轨迹总是惊人的巧合，当毛爷爷在锻炼他的接班人的时候发生了意外，当奈特在想或许可以传位于儿子的时候也发生了意外，虽然要为逝者悲伤，但是这也为世界的多元提供了更加多样性的选择。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Log4j2实现不同线程不同级别日志输出到不同的文件中]]></title>
    <url>%2F2016%2F12%2F18%2FLog4j2-to-achieve-different-levels-of-different-threads-log-output-to-a-different-file%2F</url>
    <content type="text"><![CDATA[Java日志框架作为一个Java程序员，肯定离不开日志框架，现在最优秀的Java日志框架是Log4j2，没有之一。根据官方的测试表明，在多线程环境下，Log4j2的异步日志表现更加优秀。在异步日志中，Log4j2使用独立的线程去执行I/O操作，可以极大地提升应用程序的性能。 在官方的测试中，下图比较了Sync、Async Appenders和Loggers all async三者的性能。其中Loggers all async表现最为出色，而且线程数越多，Loggers all async性能越好。 除了对Log4j2自身的不同模式做对比以外，官方还做了Log4j2/Log4j1/Logback的对比，如下图所示 其中，Loggers all async是基于LMAX Disruptor实现的。 使用Log4j2需要哪些JAR使用Log4j2最少需要两个JAR，分别是log4j-api-2.x和log4j-core-2.x，其它JAR包根据应用程序需要添加。 配置文件位置默认的，Log4j2在classpath下寻找名为log4j2.xml的配置文件。也可以使用system property指定配置文件的全路径。-Dlog4j.configurationFile=path/to/log4j2.xml，在Java代码中指定路径如下所示123456789101112import org.apache.logging.log4j.LogManager;import org.apache.logging.log4j.core.LoggerContext;import java.io.File;public class Demo &#123; public static void main(String[] args) &#123; LoggerContext loggerContext = (LoggerContext) LogManager.getContext(false); File file = new File("path/to/a/different/log4j2.xml"); loggerContext.setConfigLocation(file.toURI()); &#125;&#125; 一般的，不需要手动关闭Log4j2，如果想手动在代码中关闭Log4j2如下所示 123456789import org.apache.logging.log4j.LogManager;import org.apache.logging.log4j.core.LoggerContext;import org.apache.logging.log4j.core.config.Configurator;public class Demo &#123; public static void main(String[] args) &#123; Configurator.shutdown((LoggerContext) LogManager.getContext()); &#125;&#125; 有关Log4j2的内容很多，不能一一列出，如果在开发中遇到任何问题，推荐去官方文档中寻找解决方案。 不同的线程输出日志到不同的文件中方法一使用ThreadContext在多线程编程中，如果不做特殊的设置，那么多个线程的日志会输出到同一个日志文件中，这样在查阅日志的时候，会带来诸多不便。很自然地，我们想到了让不同的线程输出日志到不同的文件中，这样不是更好吗？在翻阅官方文档过程中，找到了FAQ（Frequently Asked Questions），其中有个问题How do I dynamically write to separate log files?正是我们所需要的。根据提示步步推进可以顺利解决该问题。其中log4j2.xml配置如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration status="OFF"&gt; &lt;Appenders&gt; &lt;Routing name="Routing"&gt; &lt;Routes pattern="$$&#123;ctx:ROUTINGKEY&#125;"&gt; &lt;!-- This route is chosen if ThreadContext has value 'special' for key ROUTINGKEY. --&gt; &lt;Route key="special"&gt; &lt;RollingFile name="Rolling-$&#123;ctx:ROUTINGKEY&#125;" fileName="logs/special-$&#123;ctx:ROUTINGKEY&#125;.log" filePattern="./logs/$&#123;date:yyyy-MM&#125;/$&#123;ctx:ROUTINGKEY&#125;-special-%d&#123;yyyy-MM-dd&#125;-%i.log.gz"&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;%d&#123;ISO8601&#125; [%t] %p %c&#123;3&#125; - %m%n&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy interval="6" modulate="true"/&gt; &lt;SizeBasedTriggeringPolicy size="10 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/Route&gt; &lt;!-- This route is chosen if ThreadContext has no value for key ROUTINGKEY. --&gt; &lt;Route key="$$&#123;ctx:ROUTINGKEY&#125;"&gt; &lt;RollingFile name="Rolling-default" fileName="logs/default.log" filePattern="./logs/$&#123;date:yyyy-MM&#125;/default-%d&#123;yyyy-MM-dd&#125;-%i.log.gz"&gt; &lt;PatternLayout&gt; &lt;pattern&gt;%d&#123;ISO8601&#125; [%t] %p %c&#123;3&#125; - %m%n&lt;/pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy interval="6" modulate="true"/&gt; &lt;SizeBasedTriggeringPolicy size="10 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/Route&gt; &lt;!-- This route is chosen if ThreadContext has a value for ROUTINGKEY (other than the value 'special' which had its own route above). The value dynamically determines the name of the log file. --&gt; &lt;Route&gt; &lt;RollingFile name="Rolling-$&#123;ctx:ROUTINGKEY&#125;" fileName="logs/other-$&#123;ctx:ROUTINGKEY&#125;.log" filePattern="./logs/$&#123;date:yyyy-MM&#125;/$&#123;ctx:ROUTINGKEY&#125;-other-%d&#123;yyyy-MM-dd&#125;-%i.log.gz"&gt; &lt;PatternLayout&gt; &lt;pattern&gt;%d&#123;ISO8601&#125; [%t] %p %c&#123;3&#125; - %m%n&lt;/pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy interval="6" modulate="true"/&gt; &lt;SizeBasedTriggeringPolicy size="10 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/Route&gt; &lt;/Routes&gt; &lt;/Routing&gt; &lt;!--很直白，Console指定了结果输出到控制台--&gt; &lt;Console name="ConsolePrint" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %t %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;!-- 级别顺序（低到高）：TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL --&gt; &lt;Root level="DEBUG" includeLocation="true"&gt; &lt;!--AppenderRef中的ref值必须是在前面定义的appenders--&gt; &lt;AppenderRef ref="Routing"/&gt; &lt;AppenderRef ref="ConsolePrint"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 测试类如下所示12345678910111213141516171819202122import lombok.extern.log4j.Log4j2;import org.apache.logging.log4j.ThreadContext;@Log4j2public class TestLog &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; ThreadContext.put("ROUTINGKEY", Thread.currentThread().getName()); log.info("info"); log.debug("debug"); log.error("error"); ThreadContext.remove("ROUTINGKEY"); &#125;).start(); new Thread(() -&gt; &#123; ThreadContext.put("ROUTINGKEY", Thread.currentThread().getName()); log.info("info"); log.debug("debug"); log.error("error"); ThreadContext.remove("ROUTINGKEY"); &#125;).start(); &#125;&#125; 运行测试类，会得到如下两个日志文件，other-Thread-1.log和other-Thread-2.log，每个日志文件对应着一个线程。该程序使用Gradle构建，依赖的JAR包如下：12345dependencies &#123; compile 'org.projectlombok:lombok:1.16.10' compile 'org.apache.logging.log4j:log4j-core:2.6' compile 'org.apache.logging.log4j:log4j-api:2.6'&#125; 需要注意的一点是，每次在使用log对象之前，需要先设置ThreadContext.put(&quot;ROUTINGKEY&quot;, Thread.currentThread().getName());，设置的key和log4j2.xml配置文件中的key要一致，而value可以是任意值，参考配置文件即可理解。 有没有发现，每次使用log对象，还需要添加额外的代码，这不是恶心他妈给恶心开门——恶心到家了吗？有没有更加优雅地解决办法呢？且看下节。 方法二实现StrLookup修改log4j2.xml配置文件如下123456789101112131415161718192021222324252627282930313233&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration status="OFF"&gt; &lt;Appenders&gt; &lt;Routing name="Routing"&gt; &lt;Routes pattern="$$&#123;thread:threadName&#125;"&gt; &lt;Route&gt; &lt;RollingFile name="logFile-$&#123;thread:threadName&#125;" fileName="logs/concurrent-$&#123;thread:threadName&#125;.log" filePattern="logs/concurrent-$&#123;thread:threadName&#125;-%d&#123;MM-dd-yyyy&#125;-%i.log"&gt; &lt;PatternLayout pattern="%d %-5p [%t] %C&#123;2&#125; - %m%n"/&gt; &lt;Policies&gt; &lt;SizeBasedTriggeringPolicy size="50 MB"/&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max="100"/&gt; &lt;/RollingFile&gt; &lt;/Route&gt; &lt;/Routes&gt; &lt;/Routing&gt; &lt;Async name="async" bufferSize="1000" includeLocation="true"&gt; &lt;AppenderRef ref="Routing"/&gt; &lt;/Async&gt; &lt;!--很直白，Console指定了结果输出到控制台--&gt; &lt;Console name="ConsolePrint" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %t %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level="info" includeLocation="true"&gt; &lt;AppenderRef ref="async"/&gt; &lt;AppenderRef ref="ConsolePrint"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 实现StrLookup中的lookup方法，代码如下：123456789101112131415161718import org.apache.logging.log4j.core.LogEvent;import org.apache.logging.log4j.core.config.plugins.Plugin;import org.apache.logging.log4j.core.lookup.StrLookup;@Plugin(name = "thread", category = StrLookup.CATEGORY)public class ThreadLookup implements StrLookup &#123; @Override public String lookup(String key) &#123; return Thread.currentThread().getName(); &#125; @Override public String lookup(LogEvent event, String key) &#123; return event.getThreadName() == null ? Thread.currentThread().getName() : event.getThreadName(); &#125;&#125; 编写测试类，代码如下：1234567891011121314151617import lombok.extern.log4j.Log4j2;@Log4j2public class TestLog &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; log.info("info"); log.debug("debug"); log.error("error"); &#125;).start(); new Thread(() -&gt; &#123; log.info("info"); log.debug("debug"); log.error("error"); &#125;).start(); &#125;&#125; 该测试类同样会得到两个日志文件，每个线程分别对应一个，但是在使用log对象之前不再需要设置ThreadContext.put(&quot;ROUTINGKEY&quot;, Thread.currentThread().getName());，是不是清爽多了。 根据官方的性能测试我们知道，Loggers all async的性能最高，但是方法一使用的是Sync模式（因为Appender默认是synchronous的），方法二使用的是Async Appender模式，那么如何更进一步让所有的Loggers都是Asynchronous的，让我们的配置更完美呢？想要使用Loggers all async还需要做一步设置，如果是Maven或Gradle项目，需要在src/main/resources目录下添加log4j2.component.properties配置文件，根据官网说明，其中内容为 1Log4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector 同时还需要在classpath下添加依赖的disruptor JAR包com.lmax:disruptor:3.3.6。当配置使用了AsyncLoggerContextSelector之后，所有的Loggers就都是异步的了。有方法证明使用了Loggers all async吗，答案是有，默认的location不会传递给Loggers all async的I/O线程，所以如果不设置includeLocation=true的话，打印出来的日志中location信息是“?”，例如“2016-12-16 16:38:47,285 INFO [Thread-3] ? - info”，如果设置includeLocation=”true”之后，打印出“2016-12-16 16:39:14,899 INFO [Thread-3] TestLog - info”，Gradle构建依赖如下：123456dependencies &#123; compile 'org.projectlombok:lombok:1.16.10' compile 'org.apache.logging.log4j:log4j-core:2.6' compile 'org.apache.logging.log4j:log4j-api:2.6' compile 'com.lmax:disruptor:3.3.6'&#125; 不同级别的日志输出到不同的文件中通常情况下，用到的日志级别主要是info/debug/error三个，而如果不做特殊配置，这三者信息是写到一个日志文件中的，当我们需要不同级别的日志输出到不同的文件中时，需要如何做呢？log4j2.xml配置信息如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration status="OFF"&gt; &lt;Properties&gt; &lt;Property name="logFilePath"&gt;logs&lt;/Property&gt; &lt;Property name="logFileName"&gt;testLog&lt;/Property&gt; &lt;/Properties&gt; &lt;Appenders&gt; &lt;!--很直白，Console指定了结果输出到控制台--&gt; &lt;Console name="ConsolePrint" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %t %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;/Console&gt; &lt;!--&lt;File&gt;输出结果到指定文件&lt;/File&gt;--&gt; &lt;!--&lt;RollingFile&gt;同样输出结果到指定文件，但是使用buffer，速度会快点&lt;/RollingFile&gt;--&gt; &lt;!--filePattern：表示当日志到达指定的大小或者时间，产生新日志时，旧日志的命名路径。--&gt; &lt;!--PatternLayout：和log4j一样，指定输出日志的格式，append表示是否追加内容，值默认为true--&gt; &lt;RollingFile name="RollingFileDebug" fileName="$&#123;logFilePath&#125;/$&#123;logFileName&#125;-debug.log" filePattern="$&#123;logFilePath&#125;/$$&#123;date:yyyy-MM&#125;/$&#123;logFileName&#125;-%d&#123;yyyy-MM-dd&#125;_%i.log.gz"&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;!--注意，如果有多个ThresholdFilter，那么Filters标签是必须的--&gt; &lt;Filters&gt; &lt;!--首先需要过滤不符合的日志级别，把不需要的首先DENY掉，然后在ACCEPT需要的日志级别，次序不能颠倒--&gt; &lt;!--INFO及以上级别拒绝输出--&gt; &lt;ThresholdFilter level="INFO" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;!--只输出DEBUG级别信息--&gt; &lt;ThresholdFilter level="DEBUG" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;Policies&gt; &lt;!--时间策略，每隔24小时产生新的日志文件--&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;!--大小策略，每到30M时产生新的日志文件--&gt; &lt;SizeBasedTriggeringPolicy size="30MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;RollingFile name="RollingFileInfo" fileName="$&#123;logFilePath&#125;/$&#123;logFileName&#125;-info.log" filePattern="$&#123;logFilePath&#125;/$$&#123;date:yyyy-MM&#125;/$&#123;logFileName&#125;-%d&#123;yyyy-MM-dd&#125;_%i.log.gz"&gt; &lt;Filters&gt; &lt;!--onMatch:Action to take when the filter matches. The default value is NEUTRAL--&gt; &lt;!--onMismatch: Action to take when the filter does not match. The default value is DENY--&gt; &lt;!--级别在ERROR之上的都拒绝输出--&gt; &lt;!--在组合过滤器中，接受使用NEUTRAL（中立），被第一个过滤器接受的日志信息，会继续用后面的过滤器进行过滤，只有符合所有过滤器条件的日志信息，才会被最终写入日志文件--&gt; &lt;ThresholdFilter level="ERROR" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;ThresholdFilter level="INFO" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="30MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;RollingFile name="RollingFileError" fileName="$&#123;logFilePath&#125;/$&#123;logFileName&#125;-error.log" filePattern="$&#123;logFilePath&#125;/$$&#123;date:yyyy-MM&#125;/$&#123;logFileName&#125;-%d&#123;yyyy-MM-dd&#125;_%i.log.gz"&gt; &lt;Filters&gt; &lt;ThresholdFilter level="ERROR" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="30MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;!--logger用于定义log的level以及所采用的appender，如果无需自定义，可以使用root解决，root标签是log的默认输出形式--&gt; &lt;!-- 级别顺序（低到高）：TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL --&gt; &lt;Root level="DEBUG" includeLocation="true"&gt; &lt;!-- 只要是级别比ERROR高的，包括ERROR就输出到控制台 --&gt; &lt;!--appender-ref中的值必须是在前面定义的appenders--&gt; &lt;Appender-ref level="ERROR" ref="ConsolePrint"/&gt; &lt;Appender-ref ref="RollingFileDebug"/&gt; &lt;Appender-ref ref="RollingFileInfo"/&gt; &lt;Appender-ref ref="RollingFileError"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; src\main\resources\log4j2.component.properties内容不变，如下1Log4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector 测试代码如下 1234567891011121314151617import lombok.extern.log4j.Log4j2;@Log4j2public class TestLog &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; log.info("info"); log.debug("debug"); log.error("error"); &#125;).start(); new Thread(() -&gt; &#123; log.info("info"); log.debug("debug"); log.error("error"); &#125;).start(); &#125;&#125; 该程序会生成三份日志文件【testLog-debug.log，testLog-error.log，testLog-info.log】，如果你足够细心的话，就会发现线程1和线程2的info|debug|error信息都输出到一个info|debug|error日志文件中了。如何解决这个问题呢？换句话说，我想得到 Thread 1 Thread 1的info日志 Thread 1的debug日志 Thread 1的error日志 Thread 2 Thread 2的info日志 Thread 2的debug日志 Thread 2的error日志 六个日志文件，要如何实现呢？这正是下一节要讲述的内容。 不同线程不同级别的日志输出到不同的文件中要实现该功能，还要从RoutingAppender身上做文章。RoutingAppender主要用来评估LogEvents，然后将它们路由到下级Appender。目标Appender可以是先前配置的并且可以由其名称引用的Appender，或者可以根据需要动态地创建Appender。RoutingAppender应该在其引用的任何Appenders之后配置，以确保它可以正确关闭。 RoutingAppender中的name属性用来指定该Appender的名字，它可以包含多个Routes子节点，用来标识选择Appender的条件，而Routes只有一个属性“pattern”，该pattern用来评估所有注册的Lookups，并且其结果用于选择路由。在Routes下可以有多个Route，每个Route都必须配置一个key，如果这个key匹配“pattern”的评估结果，那么这个Route就被选中。同时每个Route都必须引用一个Appender，如果这个Route包含一个ref属性，那么这个Route将引用一个在配置中定义的Appender，如果这个Route包含一个Appender的定义，那么这个Appender将会根据RoutingAppender的上下文创建并被重用。 废话说多了，直接上配置才简洁明了。log4j2.xml配置如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!--status的含义为是否记录log4j2本身的event信息，默认是OFF--&gt;&lt;Configuration status="OFF"&gt; &lt;Properties&gt; &lt;!--自定义一些常量，之后使用$&#123;变量名&#125;引用--&gt; &lt;Property name="logFilePath"&gt;logs&lt;/Property&gt; &lt;Property name="logFileName"&gt;testLog&lt;/Property&gt; &lt;/Properties&gt; &lt;Appenders&gt; &lt;!--很直白，Console指定了结果输出到控制台--&gt; &lt;Console name="ConsolePrint" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %t %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;/Console&gt; &lt;!--&lt;File&gt;输出结果到指定文件&lt;/File&gt;--&gt; &lt;!--&lt;RollingFile&gt;同样输出结果到指定文件，但是使用buffer，速度会快点&lt;/RollingFile&gt;--&gt; &lt;!--filePattern：表示当日志到达指定的大小或者时间，产生新日志时，旧日志的命名路径。--&gt; &lt;!--PatternLayout：和log4j一样，指定输出日志的格式，append表示是否追加内容，值默认为true--&gt; &lt;Routing name="RollingFileDebug_$&#123;thread:threadName&#125;"&gt; &lt;Routes pattern="$$&#123;thread:threadName&#125;"&gt; &lt;Route&gt; &lt;RollingFile name="RollingFileDebug_$&#123;thread:threadName&#125;" fileName="$&#123;logFilePath&#125;/$&#123;logFileName&#125;_$&#123;thread:threadName&#125;_debug.log" filePattern="$&#123;logFilePath&#125;/$$&#123;date:yyyy-MM&#125;/$&#123;logFileName&#125;-%d&#123;yyyy-MM-dd&#125;-$&#123;thread:threadName&#125;-debug_%i.log.gz"&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;!--注意，如果有多个ThresholdFilter，那么Filters标签是必须的--&gt; &lt;Filters&gt; &lt;!--首先需要过滤不符合的日志级别，把不需要的首先DENY掉，然后在ACCEPT需要的日志级别，次序不能颠倒--&gt; &lt;!--INFO及以上级别拒绝输出--&gt; &lt;ThresholdFilter level="INFO" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;!--只输出DEBUG级别信息--&gt; &lt;ThresholdFilter level="DEBUG" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;Policies&gt; &lt;!--时间策略，每隔24小时产生新的日志文件--&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;!--大小策略，每到30M时产生新的日志文件--&gt; &lt;SizeBasedTriggeringPolicy size="30MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/Route&gt; &lt;/Routes&gt; &lt;/Routing&gt; &lt;Routing name="RollingFileInfo_$&#123;thread:threadName&#125;"&gt; &lt;Routes pattern="$$&#123;thread:threadName&#125;"&gt; &lt;Route&gt; &lt;RollingFile name="RollingFileInfo_$&#123;thread:threadName&#125;" fileName="$&#123;logFilePath&#125;/$&#123;logFileName&#125;_$&#123;thread:threadName&#125;_info.log" filePattern="$&#123;logFilePath&#125;/$$&#123;date:yyyy-MM&#125;/$&#123;logFileName&#125;-%d&#123;yyyy-MM-dd&#125;-$&#123;thread:threadName&#125;-info_%i.log.gz"&gt; &lt;Filters&gt; &lt;!--onMatch: Action to take when the filter matches. The default value is NEUTRAL--&gt; &lt;!--onMismatch: Action to take when the filter does not match. The default value is DENY--&gt; &lt;!--级别在ERROR之上的都拒绝输出--&gt; &lt;!--在组合过滤器中，接受使用NEUTRAL（中立），被第一个过滤器接受的日志信息，会继续用后面的过滤器进行过滤，只有符合所有过滤器条件的日志信息，才会被最终写入日志文件--&gt; &lt;ThresholdFilter level="ERROR" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;ThresholdFilter level="INFO" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="30MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/Route&gt; &lt;/Routes&gt; &lt;/Routing&gt; &lt;Routing name="RollingFileError_$&#123;thread:threadName&#125;"&gt; &lt;Routes pattern="$$&#123;thread:threadName&#125;"&gt; &lt;Route&gt; &lt;RollingFile name="RollingFileError_$&#123;thread:threadName&#125;" fileName="$&#123;logFilePath&#125;/$&#123;logFileName&#125;_$&#123;thread:threadName&#125;_error.log" filePattern="$&#123;logFilePath&#125;/$$&#123;date:yyyy-MM&#125;/$&#123;logFileName&#125;-%d&#123;yyyy-MM-dd&#125;-$&#123;thread:threadName&#125;-error_%i.log.gz"&gt; &lt;Filters&gt; &lt;ThresholdFilter level="ERROR" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="%d&#123;yyyy.MM.dd HH:mm:ss z&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="30MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/Route&gt; &lt;/Routes&gt; &lt;/Routing&gt; &lt;!--bufferSize整数，指定可以排队的events最大数量，如果使用BlockingQueue，这个数字必须是2的幂次--&gt; &lt;!--includeLocation默认值是FALSE，如果指定为TRUE，会降低性能，但是推荐设置为TRUE，否则不打印位置行信息--&gt; &lt;Async name="async" bufferSize="262144" includeLocation="true"&gt; &lt;AppenderRef ref="RollingFileDebug_$&#123;thread:threadName&#125;"/&gt; &lt;AppenderRef ref="RollingFileInfo_$&#123;thread:threadName&#125;"/&gt; &lt;AppenderRef ref="RollingFileError_$&#123;thread:threadName&#125;"/&gt; &lt;!-- 只要是级别比ERROR高的，包括ERROR就输出到控制台 --&gt; &lt;AppenderRef ref="ConsolePrint" level="ERROR"/&gt; &lt;/Async&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;!--Logger用于定义log的level以及所采用的appender，如果无需自定义，可以使用root解决，root标签是log的默认输出形式--&gt; &lt;!-- 级别顺序（低到高）：TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL --&gt; &lt;Root level="DEBUG" includeLocation="true"&gt; &lt;!--AppenderRef中的ref值必须是在前面定义的appenders--&gt; &lt;AppenderRef ref="async"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; log4j2.component.properties和ThreadLookup类不变，依赖的JAR包和上一节一样。测试类如下 1234567891011121314151617import lombok.extern.log4j.Log4j2;@Log4j2public class TestLog &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; log.info("info"); log.debug("debug"); log.error("error"); &#125;).start(); new Thread(() -&gt; &#123; log.info("info"); log.debug("debug"); log.error("error"); &#125;).start(); &#125;&#125; 该程序会输出六个日志文件，分别是 testLog_Thread-2_debug.log testLog_Thread-2_error.log testLog_Thread-2_info.log testLog_Thread-3_debug.log testLog_Thread-3_error.log testLog_Thread-3_info.log 至此，就实现了不同线程不同级别的日志输出到不同文件中的功能。 如何启用All Loggers Asynchronous为了使得所有的Loggers都是异步的，除了添加一个新的配置文件，就是log4j2.component.properties外，还有其它方式吗？有的，仅列举如下 例如【IntelliJ IDEA】中使用Gradle构建项目，那么可以在Settings | Build, Execution, Deployment | Build Tools | Gradle | Gradle VM options中填入 1-DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector 另一种就是在前面提到的ThreadLookup类中，添加静态代码块 123static &#123; System.setProperty("Log4jContextSelector", "org.apache.logging.log4j.core.async.AsyncLoggerContextSelector");&#125; 根据参考手册，有一点需要注意的就是，要使用&lt;Root&gt;或&lt;Logger&gt;标签，而不是&lt;asyncRoot&gt;和&lt;asyncLogger&gt;，原文如下： When AsyncLoggerContextSelector is used to make all loggers asynchronous, make sure to use normal &lt;root&gt; and &lt;logger&gt; elements in the configuration. The AsyncLoggerContextSelector will ensure that all loggers are asynchronous, using a mechanism that is different from what happens when you configure &lt;asyncRoot&gt; or &lt;asyncLogger&gt;. The latter elements are intended for mixing async with sync loggers. 混合使用Synchronous和Asynchronous Loggers需要disruptor-3.0.0.jar或更高版本的jar包，不需要设置系统属性Log4jContextSelector，在配置中可以混合使用Synchronous和asynchronous loggers，使用&lt;AsyncRoot&gt;或者&lt;AsyncLogger&gt;去指定需要异步的Loggers，&lt;AsyncLogger&gt;元素还可以包含&lt;Root&gt;和&lt;Logger&gt;用于同步的Loggers。注意如果使用的是&lt;AsyncRoot&gt;或者&lt;AsyncLogger&gt;，那么就无需设置系统属性Log4jContextSelector了。 一个混合了同步和异步的Loggers配置如下：1234567891011121314151617181920212223&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!-- No need to set system property "Log4jContextSelector" to any value when using &lt;asyncLogger&gt; or &lt;asyncRoot&gt;. --&gt;&lt;Configuration status="WARN"&gt; &lt;Appenders&gt; &lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --&gt; &lt;RandomAccessFile name="RandomAccessFile" fileName="asyncWithLocation.log" immediateFlush="false" append="false"&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;%d %p %class&#123;1.&#125; [%t] %location %m %ex%n&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;/RandomAccessFile&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;!-- pattern layout actually uses location, so we need to include it --&gt; &lt;AsyncLogger name="com.foo.Bar" level="trace" includeLocation="true"&gt; &lt;AppenderRef ref="RandomAccessFile"/&gt; &lt;/AsyncLogger&gt; &lt;Root level="info" includeLocation="true"&gt; &lt;AppenderRef ref="RandomAccessFile"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 参考文献[1] Different log files for multiple threads using log4j2[2] log4j2: Location for setting Log4jContextSelector system property for asynchronous logging[3] Making All Loggers Asynchronous[4] AsyncAppender[5] Mixing Synchronous and Asynchronous Loggers[6] How to verify log4j2 is logging asynchronously via LMAX disruptor?[7] Log4j2 synchronous logging[8] Log4J2 sync loggers faster than mixed async/sync loggers[9] Difference between Asynclogger and AsyncAppender in Log4j2]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Log4j2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《程序员的呐喊》]]></title>
    <url>%2F2016%2F12%2F10%2FRead-a-programmer-s-rantings%2F</url>
    <content type="text"><![CDATA[作者简介：Steve Yegge是一名程序员，也是博主，写了很多关于编程语言，生产力，和软件文化的文章。他拥有华盛顿大学计算机科学本科学位，20年的业界经验，开发领域涉及嵌入式操作系统、可扩展的电子商务系统、移动设备应用、提升软件生产力的工具等。他曾供职于亚马逊和Google等公司。 徐旭铭，编了十几年程，翻译过几本书，现在在亚马逊当码农。工作和兴趣都是写代码，喜欢看上去很麻烦的问题。住在西雅图，闲暇时喜欢看美剧。 这本书真是太有意思了，里面充斥了作者对各种语言以及开发现象和编程风格的吐槽，但是读来并不让人反感，也不觉得偏激，真是太有才了，真羡慕可以把吐槽文也写得这么棒的人。在书中，作者着重推荐了《重构》注意是Refactoring，我刚开始心里想到的是Rework，其实并不是。这是两本完全不同的书，但是我推荐你把它们都读一读，必然会受益匪浅。书中作者的一句话非常有意思，他说，“在读《重构》之前，他感觉自己这么多年来都像是脱了裤子上班一样，其他人是不是早就读过这本书了？我会不会是唯一不知道的人？”。我想，对于读书都这么如饥似渴的人，并且生怕落后于别人的人，在读到一本好书时的那种焦灼感，怎么可能会一直默默无闻呢？另外说一句，这本书翻译的也很棒，实属不易。 到今天，重构已经衍生出一整个产业链了，它就是一面旗帜。Java IDE的粉丝们都在为之摇旗呐喊。重构工具就好像摆在商店里的产品一样。翻开菜单从里面挑一个出来，连地球都能撬起来。 在谈到语言的宗教情况时，这个现象确实很严重，不管是在国内还是在国外，都不缺少狂热的语言宗教分子。当然了这里面的语言指的是计算机世界里的编程语言，而不是我们日常中所说的语言。每个语言狂热分子都把自己所使用的语言视为正统宗教，容不得别人指手画脚，哪怕这些都是善意的行为，在他们眼里，也是对他们语言宗教的亵渎。 好在当你会的语言越来越多的时候，也就相当于你在不断的改变宗教信仰。每次改变信仰，下一个对你的影响力就越弱，因为你在语言宗教的世界里变得越来越成熟，并且也不那么极端与狂热了，你可以站在一个宗教之外去审视当初自己沉溺其中的宗教。在各个宗教之间切换的时候，使你变得原来越睿智了，再也不屑于为了语言去与别人发生争执了。 每当遇到一些事物你觉得很好很强大，别人却批评抱怨的时候，你总会忍不住希望他们赶紧离开，好继续坐井观天，这时千万别忘了这一点：之所以我们今天能冲破黑暗时代，生活在一个相对光明的启蒙时代，正是因为我们敢于挑战自己最宝贵的信仰。所以质疑是件好事。 最重要的是作者指出了结对编程、极限编程、敏捷开发等都是扯犊子的玩意，这些都是软件咨询师搞出来挣钱的套路。其实这也是被逼无奈，如果像以前那样以完成一个项目之后才付款，但是在这过程之中，客户不断的变更需求，大多数软件咨询师以及负责开发项目的人都是挣不到钱的，因为这种项目成功的概率太小了，所以聪明的咨询师们搞出了各种概念并按阶段付款。但是并不能一概而论，有坏的敏捷开发，同时也有好的敏捷开发，在文中作者举了Google的例子作为好的敏捷，不妨看看列出的如下几点： 经理也至少有一半时间在写代码，所以他们更像是技术主管 工程师可以在任何时候换组，或者换项目，不会有人质疑你。只要你一句话，第二天就会有人来帮你把东西都搬到新的组那边去 Google的理念是不去告诉工程师要做什么，他们也是这样严格执行的 鼓励工程师花工作时间中的20%的时间去做任何想做的事情，而不是日常工作 很少开会。我估计一个工程师平均一周大概开3次会吧，这还包括了和主管一对一沟通 安静的环境。工程师或单独，或三五一组，都非常安静地专注于自己的工作 没有甘特图，或是日期-任务-负责人的表格，或是任何看得到的项目管理工具 就算真的碰到项目吃紧的时候，大家还是会去吃午饭和晚饭，都是免费的哦，除非自愿，没人会长时间的工作 感觉作者已经完全被Google收服了，是诚心的。在文中他说，在Google，每周都有新花样，新福利，新变化，新的调研，来问我们怎么才能让大家在Google过得更舒服。当然并不仅仅只是这些，在Google里驱使大家去做正确的事的动因是感激，这超过了所有的因素，甚至比所有的因素加在一起还要多。你会忍不住想要做到更好，因为Google对你的照顾无微不至，让人觉得好像欠了它似的。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 索引文件格式]]></title>
    <url>%2F2016%2F12%2F05%2FLucene-6-0-index-file-format%2F</url>
    <content type="text"><![CDATA[定义在Lucene中基本的概念包括：index、document、field和term。一个index包含一个documents的序列 一个document是一个fields的序列 一个field是一个命名的terms序列 一个term是一个bytes的序列 在两个不同fields中的相同bytes序列被认为是不同的term。因此，term表示为一对：命名field的字符串，以及field内的bytes。 倒排索引谈到倒排索引，那么首先看看正排是什么样子的呢？假设文档1包含【中文、英文、日文】，文档2包含【英文、日文、韩文】，文档3包含【韩文，中文】那么根据文档去查找内容的话 文档1-&gt;【中文、英文、日文】 文档2-&gt;【英文、日文、韩文】 文档3-&gt;【韩文，中文】 反过来，根据内容去查找文档 中文-&gt;【文档1、文档3】 英文-&gt;【文档1、文档2】 日文-&gt;【文档1、文档2】 韩文-&gt;【文档2、文档3】 就是倒排索引，而Lucene擅长的也正在于此。 Fields的类型 TextField：索引并分词，不包含词向量，多用于文本的正文 StringField：索引但不分词，整个String作为单个标记索引，例如可用于“国家名称”或“ID”等，或者其它任何你想要用来排序的字段 IntPoint：int型用于精确/范围查询的索引 LongPoint：long型用于精确/范围查询的索引 FloatPoint：float型用于精确/范围查询的索引 DoublePoint：double型用于精确/范围查询的索引 SortedDocValuesField：存储每个文档BytesRef值的字段，索引用于排序，如果需要存储值，需要再用StoredField实例 SortedSetDocValuesField：存储每个文档一组BytesRef值的字段，索引用于faceting/grouping/joining，如果需要存储值，需要再用StoredField实例 NumericDocValuesField：存储每个文档long值的字段，用于scoring/sorting/值检索 SortedNumericDocValuesField：存储每个文档一组long值的字段，用于scoring/sorting/值检索 StoredField：用于在汇总结果中检索的仅存储值 段（Segments）Lucene的索引可能是由多个子索引或Segments组成。每个Segment是一个完全独立地索引，可以单独用于搜索。索引涉及 为新添加的documents创建新的segments 合并已经存在的segments 搜索可能涉及多个segments或/和多个索引，每个索引可能由一组segments组成。 文档编号Lucene通过一个整型的文档编号指向每个文档，第一个被加入索引的文档编号为0，后续加入的文档编号依次递增。注意文档编号是可能发生变化的，所以在Lucene外部存储这些值时需要格外小心。 索引结构概述每个segment索引包括信息 Segment info：包含有关segment的元数据，例如文档编号，使用的文件 Field names：包含索引中使用的字段名称集合 Stored Field values：对于每个document，它包含属性-值对的列表，其中属性是字段名称。这些用于存储有关文档的辅助信息，例如其标题、url或访问数据库的标识符 Term dictionary：包含所有文档的所有索引字段中使用的所有terms的字典。字典还包括包含term的文档编号，以及指向term的频率和接近度的指针 Term Frequency data：对于字典中的每个term，包含该term的所有文档的数量以及该term在该文档中的频率，除非省略频率（IndexOptions.DOCS） Term Proximity data：对于字典中的每个term，term在每个文档中出现的位置。注意，如果所有文档中的所有字段都省略位置数据，则不会存在 Normalization factors：对于每个文档中的每个字段，存储一个值，该值将乘以该字段上的匹配的分数 Term Vectors：对于每个文档中的每个字段，可以存储term vector，term vector由term文本和term频率组成 Per-document values：与存储的值类似，这些也以文档编号作为key，但通常旨在被加载到主存储器中以用于快速访问。存储的值通常用于汇总来自搜索的结果，而每个文档值对于诸如评分因子是有用的 Live documents：一个可选文件，指示哪些文档是活动的 Point values：可选的文件对，记录索引字段尺寸，以实现快速数字范围过滤和大数值（例如BigInteger、BigDecimal（1D）、地理形状交集（2D，3D）） 文件命名属于一个段的所有文件具有相同的名称和不同的扩展名。当使用复合索引文件，这些文件（除了段信息文件、锁文件和已删除的文档文件）将压缩成单个.cfs文件。当任何索引文件被保存到目录时，它被赋予一个从未被使用过的文件名字。 文件扩展名摘要 名称 文件扩展名 简短描述 Segments File segments_N 保存了一个提交点（a commit point）的信息 Lock File write.lock 防止多个IndexWriter同时写到一份索引文件中 Segment Info .si 保存了索引段的元数据信息 Compound File .cfs，.cfe 一个可选的虚拟文件，把所有索引信息都存储到复合索引文件中 Fields .fnm 保存fields的相关信息 Field Index .fdx 保存指向field data的指针 Field Data .fdt 文档存储的字段的值 Term Dictionary .tim term词典，存储term信息 Term Index .tip 到Term Dictionary的索引 Frequencies .doc 由包含每个term以及频率的docs列表组成 Positions .pos 存储出现在索引中的term的位置信息 Payloads .pay 存储额外的per-position元数据信息，例如字符偏移和用户payloads Norms .nvd，.nvm .nvm文件保存索引字段加权因子的元数据，.nvd文件保存索引字段加权数据 Per-Document Values .dvd，.dvm .dvm文件保存索引文档评分因子的元数据，.dvd文件保存索引文档评分数据 Term Vector Index .tvx 将偏移存储到文档数据文件中 Term Vector Documents .tvd 包含有term vectors的每个文档信息 Term Vector Fields .tvf 字段级别有关term vectors的信息 Live Documents .liv 哪些是有效文件的信息 Point values .dii，.dim 保留索引点，如果有的话 锁文件默认情况下，存储在索引目录中的锁文件名为“write.lock”。如果锁目录与索引目录不同，则锁文件将命名为“XXXX-write.lock”，其中XXXX是从索引目录的完整路径导出的唯一前缀。此锁文件确保每次只有一个写入程序在修改索引。 英文文档参见：https://lucene.apache.org/core/6_0_0/core/org/apache/lucene/codecs/lucene60/package-summary.html]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《成为技术领导者》]]></title>
    <url>%2F2016%2F11%2F29%2FRead-becoming-a-technical-leader%2F</url>
    <content type="text"><![CDATA[作者简介：杰拉尔德·温伯格，软件领域最著名的专家之一，美国计算机名人堂代表人物，Weinbrg &amp; Weinberg顾问公司（位于美国内布拉斯加州首席林肯市）的负责人。温伯格精力旺盛、思想活跃，从20世纪70年代开始，他总共撰写了30多本书籍和数以百计的论文。在西方国家乃至全球，温伯格拥有大量忠实的读者群，这些“追星族”阅读了温伯格的每本重要著作，他们甚至建有专门的组织和网站，讨论和交流大师的重要思想。可以说，温伯格近年来的每本新书都是在万众瞩目中推出的。译者，朱于军，1999年于北京邮电大学获得电子与信息系统博士学位，发表论文二十余篇。其后加入朗讯贝尔实验室从事通信网络设备的研发。 究竟什么是领导？首先，传统上我们识别一个团队的领导者主要是靠： 让团队成员指出他们认为谁在指导整个团队时最具影响力； 要求旁观者指出最具影响力的成员，或者记录下有效的、有影响力的行为次数。 这种方法存在着缺陷，因为该模型主要是建立在团队成员或者旁观者的主观意见上，尤其是他们观察“有效的、有影响力的行为”的能力。作者给出了领导的一种有机定义：领导就是营造一个使人们工作更有意义且效率更高的环境的过程。 领导风格的模型在列举了自己打弹子球的例子中，作者指导了很多想要打好弹子球的孩子们，但是却总有一些孩子，无论你说多少遍，他们依然无法掌握技巧，他们并不缺少动力，因为他们都想打的更好，但是最核心的原因则是，他们不能很好地组织起自己的精力来学会那种需要非常努力的事务。推此即彼，多数人的不成功，一方面可以归咎于天赋、出身、环境等等，然而他们却忽略了最核心的原因，就是他们并没有真正地把精力花在想要做成的事务上。你可以扪心自问，有哪些事你真正组织了自己的精力并努力为之却做不成的呢？ 通过介绍了自己打弹子球的例子引出了领导的MOI模型 M：激励（motivation）——有奖品或有困难，这样才对相关人员有推动力或吸引力 O：组织（organization）——利用现有的组织结构使想法变成现实 I：想法（ideas）或创意（innovation）——种子，是要变成的图景 领导也可能意味着阻止发生变化。如果要预防某种变化的发生，必须对周围的环境实施以下三者之一 M：消除激励（kill the motivation）——使人们感到变化不会被赏识；为他们做所有事情，使他们觉得自己没必要做什么；对人们做的可能出于个人爱好的任何事情泼冷水 O：造成混乱（foster chaos）——鼓励激烈的竞争以致于使合作变得不可想象；使资源略低于必需的最小限度；消除共享的有价值的信息，或用大量无意义的词和纸淹没它们 I：限制想法的交流（suppress the flow of ideas）——如果你能责备别人，就不倾听；首先给出你的意见，并且要用最大的声音；惩罚那些提出建议的人；不让人们共同工作；最重要的是要容忍没有笑声的情形 我们每一个人、某些方面可能比别的部分发展的更好，但是每一个人都可以通过加强他的最弱部分来提高自己，从而成为一个领导者。所有那些一直非常成功的领导者通过鼓励人们重视创意或用更好的工作方法来提高工作效率。成功的技术领导者通常都是采用一种通用的，我们称之为解决问题型的领导风格，他们将注意力集中在创新过程上，并且主要通过以下三种方式实现：理解问题、管理想法的交流、保证质量。 要成为一个解决问题型领导者，你不需要有类似宗教上的某种突然转变。你只需去检查缺少策略的目标/手段的组合，然后充实它们，并且每次只考虑一个。 解决问题型领导风格 理解问题 仔细阅读任务规范 鼓励团队成员仔细阅读任务规范 参考问题的初始定义以消除争论 从客户那里澄清任务规范并获得额外的信息 在工作进行了一段时间，并对某些需求的含义有了更好的认识后，我们应该再回顾一下任务规范 管理想法的交流 为团队提供一个聪明的想法 鼓励借鉴有用的旧想法 认真完善团队成员提出的想法 放弃自己的想法并支持团队采纳的想法，但只有当每个成员都充分了解你的想法时才予以放弃 尽管时间压力很大，仍然不要吝惜花时间听其他人解释他们的想法 检验别人提出的想法 为了保持想法的交流，不要轻易否定团队成员的想法 如果你不得不否定一个想法，那么一定要明确，你所否定的只是这个想法而不是提出这个想法的人 在给出你的想法之前要先对它进行检验 当时间和人力吃紧时，不要再去考虑新的想法而应该专注于现有的想法 鼓励队员们放弃以前曾经成功过，但并不适用于现在的情况的想法 如果一个已被否定的想法对问题的其他部分有价值，就应该重新采纳它 控制质量 在进行项目的同时检查质量 在确定解决方案的同时设计出质量检查工具和过程 测量实现进度，与时间表进行对比并随时准备调整方案过程 在项目进行中重新评估你所预测的前景及其可行性 在把想法付诸实践之前，先征求客户的意见 即使想法失效，也要恢复士气 领导者是如何成长起来的领导者是改变的领导者——改变他人，改变工作小组以及改变组织。最重要的是，领导者是自身改变的领导者。成长为一个领导者，有以下几个关键点 孰能生巧：练习只是缓慢成长阶段的一部分。毫无疑问，练习有助于水平的提高，但这不是全部。孰能生巧，但当你开始感到自己真正在进步时，就要开始寻找某种观念上的突破。换句话说就是，花些时间去掌握技巧，但不要忘记寻找更好的打法。 向前飞跃：大部分的进步来自于从一个高原阶段到另一个高原阶段的突然飞跃，高原阶段中取得的缓慢进步对于从一个高原阶段飞跃到另一个高原阶段是至关重要的。 坠入低谷：每当试图提高时，可能都要先经历一次小小的退步之后才能取得大的提高。高原阶段是存在的，但你不是在跳跃而是在攀登。为了攀登到新的高度，你必须离开原有的立足点，舍弃你擅长的东西，并且还有可能滑落到低估。如果你不舍得放弃你擅长的东西，那么你会继续慢慢地进步，但永远达不到新的高度。 在现实社会中成长：不论你取得的成就有多大，多辉煌，你永远不会忘记身处低谷时体会到的痛苦。没有对美好事物的向往，这些痛苦会在你开始攀登之前就把你拉回原地。所以，在你开始下一次攀登之前，要想到站在顶峰时的感觉。 我做不到，因为……事实上，那些有潜质成为领导者的人要远远多于被任命为领导者的人。在设计良好的工作团队中领导才能来自于团队的每个成员，而非仅仅是被任命的领导者，所以，没必要也不应该等待一个任命的到来。成为领导者的技术明星们面临的最难的一个选择就是可能要远离最新的技术。同时必然要牺牲一部分技术资本以改善人际交往能力。他们已经超越了他们的技术领域，而今以更广泛的的方式把握技术。他们可以与来自众多领域的技术专家进行交谈，并能轻松地分辨出其中的精华与糟粕。这种技术能力正是他们的职业所需要的。 技术领导者拥有极大的权力，但他们既不是魔鬼也不是神。他们是一群碰巧掌握了有效解决问题方法的普通人，这种方法以通过创新来领导的能力为基础。其他人即使不具备任何技术背景也能通过掌握这种方法的基本要领而使自己或其他人的效率更高。 创新道路上的三大障碍不能像别人一样客观地看待自己是我们前进道路上的首要障碍，绝大部分有志于成为解决问题型领导者的人也面临着这个问题，他们需要借助他人的帮助才可以跨越这道障碍。自蔽，看不到自己的行为，所以也就无法改变。 第二个障碍是没问题综合症，这个症状的表现是，当信息进入耳朵后，大脑却反馈出一个与其本意毫不相关的老套的答复。就像一个人正在讲述一件很令人气愤的事情，而另一个人却冷冰冰地说了一句“没问题”那样。想进行自检的话，你必须能听到自己说“没问题”！。或者至少能意识到，在确认自己理解了其他人的问题之前，就给出了问题解决的答案。 第三大障碍是唯一解决方案信仰，对理论心理学中心教条的信仰，阻止你去发现其他可选择的解决方案，即使那些方案是你不需要任何人帮助就可以发现的。 这些顽固的障碍形成了一个封闭的系统，即除即生，难以尽去。 开发自知之明的工具成为一个技术领导者的第一步要从写日记开始，记日记的方法不同于书本和讲座的一个很大的好处是：其中的所有事情都和你有关。每个人的学习内容都各有不同，因此我没法告诉你，你能学到些什么，不过我保证你确实会学到东西。我觉得大家不要局限于传统的纸质日记，现在的知识管理工具这么多，足够用了，日记的最大好处是条理性以及减轻大脑的带宽。有些琐碎的事情既然已经记录下来，大脑就不必为此分配额外的带宽去记忆它。 开发创新能力当一个小组中的两个成员为谁的想法更好而争执不下时，头脑敏捷的领导者能设法找到由前两个想法结合而产生的第三种想法。 远见成为一名领导者意味着需要完成从关注自己的想法到关注别人的想法的转变。事情的本身无关紧要，重要的是你对事情的反应。假如只是由于成功就会导致失败，那么每个人都会失败。成功的道路通常都是曲折不平的。没有人每次都会成功。从来没有经历过失败的人不会成为领导者。领导者之所以成为领导者是因为他们知道应对失败的方法。 成功的创新者拥有一把神秘的钥匙，这把神秘的钥匙就是有独到的远见，这种远见是在一般部分的基础上吸纳了独到的成分。一般部分就是生活中周而复始的任务。第二部分就是个人化的独到见解，它体现在领导者对好想法的执着上。换句话说，必定存在值得做的一些事情，但是必定也存在着只有我才能有所贡献的独特部分。这就是实现远见的关键。没有个人的远见，领导技巧就不再有用。 激励他人的首要障碍创新的首要障碍是自蔽：看不到你自己。激励的首要障碍是另一种蒙蔽：无法像其他人看你那样看自己。 激励他人的第二大障碍 如果生存有问题，那么除了把人置于优先地位之外别无选择 如果工作的技术含量不是很高，领导者不需要有很强的能力，也能够利用人的敬畏心理来领导 技术背景很强的人会将任何任务转换成某项技术任务，从而避免从事他们所不愿意做的事情 不会有人愿意追随不关心人的领导者，除非他们别无选择 如果你没有什么可以发表的，却假装有，那么你对人们再多的关心也无法留住你的听众 注重任务的领导者往往会过高估计他们自己的成就 在我们所做的工作中，几乎没有什么真正重要到足以使工作者认为牺牲未来是值得的 如果工作很复杂，没有哪个领导者能绝对保证计划不会“误入歧途” 为了成为一个成功的解决问题型领导者，你必须将每个人的人性放在第一位 如果你是领导者，人才是你的工作。其他事情都不值得做]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene的索引文件锁原理]]></title>
    <url>%2F2016%2F11%2F23%2FLucene-index-file-lock-principle%2F</url>
    <content type="text"><![CDATA[环境Lucene 6.0.0Java “1.8.0_111”OS Windows 7 Ultimate 线程安全在Lucene中，打开一个IndexWrite之后，就会自动在索引目录中生成write.lock文件，这个文件中并不会有内容，不管是在索引打开期间还是在索引关闭之后，其大小都为0KB，并且在IndexWriter关闭之后，并不会删除该文件。如果同时打开多个IndexWriter的话，后打开的IndexWriter就会抛出LockObtainFailedException异常。这是个很重要的保护机制，因为若针对同一索引打开两个writer的话，会导致索引损坏。所以Lucene中的锁主要针对并发写的情况，在写的过程中并不会影响到并发读操作。总结如下： 任意数量的IndexReader类都可以同时打开一个索引，与其是否同属于一个JVM无关，在单个JVM中，利用资源和发挥效率的最好办法是多线程共享单个IndexReader实例。 一旦建立起IndexWriter对象，系统即会分配一个锁给它，该锁只有当IndexWriter对象被关闭时才会释放。 对于一个索引来说，一次只能打开一个IndexWriter对象，但是对IndexReader并无限制，IndexReader对象甚至可以在IndexWriter对象正在修改索引时打开。每个IndexReader对象将向索引展示自己被打开的时间点。该对象只有在IndexWriter对象提交修改或自己被重新打开后才能获知索引修改情况。 任意多个线程都可以共享同一个IndexReader和IndexWriter，这些类不仅是线程安全的而且是线程有好的，说其是线程有好的意思是，它们能够很好的扩展到新增线程中，因为这些类中的同步代码数并不多。 利用远程文件系统访问索引一般地，我们总是希望可以实现分布式搜索，在这方面现成的有Elasticsearch和Solr可以选用。而如果基于Lucene进行开发，如何实现呢？最简单的方案是用一台专用计算机保存和修改本地索引，然后用其他计算机通过远程文件访问来搜索该索引。该方案性能会比搜索本机索引要差得多。 一个改进的策略是通过将远程文件系统挂载至各计算机会提高一点效果。最好的效果是将索引复制到各台计算机自己的文件系统，然后再进行搜索。一个注意点是在将本地文件系统同步到远程文件系统的时候，假设远程文件系统正在被搜索，此时不能立即删除正在被使用的索引文件，此问题可以通过利用索引文件的代数解决，即搜索端用上一代索引，同步系统同步这一代索引并保留上一代索引至下一次同步时删除。另一点是如果在远程文件系统上存在定时打开的索引线程怎么办呢？为了获取最新的索引状态，通常会由一个后台线程去定时打开索引切换IndexReader，那么此时如果将本地文件系统索引同步到远程文件系统的时候一样有问题，比如segments_*文件同步过去了，定时线程打开segments_*并开始去查找其它文件，但是其它文件还未同步过去。可以通过先同步其它索引文件（非segments_*文件），再同步segments_*文件，然后更改索引代数，最后再去删除除本次和上次之外的所有无效索引文件解决。 索引锁机制首先抛出几个问题再来详解。Lucene是如何知道一份索引已被其它IndexWriter打开呢？也许你会说通过write.lock文件，但是该文件里并未存有任何信息，并在IndexWriter关闭之后也不会删除该文件，如何通过write.lock文件来辨别呢？另外在程序异常退出或是JVM异常关闭都会导致锁被释放，再次启动程序和JVM之后，程序都可以正常地获取锁，这又是如何实现的呢？ 在Lucene中，提供了一个顶层的抽象类LockFactory，它有三个实现类和一个子抽象类FSLockFactory，而FSLockFactory有两个实现类，分别是SimpleFSLockFactory和NativeFSLockFactory，总结如下 锁类名 描述 SingleInstanceLockFactory RAMDirectory的默认锁策略，为单个进程内实例准备，意味着所有的加锁操作都通过这个实例 VerifyingLockFactory 用来包装其它的LockFactory，以验证每次获取锁/释放锁的操作是正确的 NoLockFactory 完全关闭锁机制，单例模式，你需要使用INSTANCE SimpleFSLockFactory 使用Java的File.createNewFile API，它的主要缺点是在JVM崩溃时会导致遗留一个write.lock文件，在下次使用IndexWriter之前必须手动清除，比较适合NFS使用 NativeFSLockFactory 依赖java.nio.*进行加锁，FSDirectory的默认锁，不适合NFS使用，该类的最大好处是JVM异常退出的话，由OS负责移除write.lock，OS并不真的删除该文件，但是会释放该文件上的所有引用，确保下次可以重新获取锁 如果选择自行实现锁机制的话，要确认该机制能正确运行。有一个简单的调试工具LockStressTest，该类可以与LockVerifyServer和VerifyingLockFactory联合使用，以确认你自己实现的锁机制能正常运行。 值的说明的是，所有的这些类中都提供了一个获取锁的方法，以及一个与之对应的静态内部类XXXLock，在XXXLock中提供了检查锁是否有效的方法ensureValid方法和close方法。close方法主要用来关闭一些流以及释放与之相关的资源。 加锁源码解析在第一次实例化IndexWriter的过程中，Lucene首先会进行加锁操作，如果加锁失败的话，是不会进行下一步操作的。一个常见的实例化代码如下12IndexWriter indexWriter = new IndexWriter(FSDirectory.open(Paths.get(indexPath)), new IndexWriterConfig(new WhitespaceAnalyzer())); 打断点开始逐步调试，程序首先进入FSDirectory的open方法123public static FSDirectory open(Path path) throws IOException &#123; return open(path, FSLockFactory.getDefault());&#125; 在open中调用FSLockFactory的getDefault方法123public static final FSLockFactory getDefault() &#123; return NativeFSLockFactory.INSTANCE;&#125; 由源码也可以知道，FSDirectory使用NativeFSLockFactory作为其默认锁策略，接着判断如果是64位的JRE并且平台支持取消映射文件，则直接返回MMapDirectory123456789public static FSDirectory open(Path path, LockFactory lockFactory) throws IOException &#123; if (Constants.JRE_IS_64BIT &amp;&amp; MMapDirectory.UNMAP_SUPPORTED) &#123; return new MMapDirectory(path, lockFactory); &#125; else if (Constants.WINDOWS) &#123; return new SimpleFSDirectory(path, lockFactory); &#125; else &#123; return new NIOFSDirectory(path, lockFactory); &#125;&#125; 接着会在IndexWriterConfig的父类LiveIndexWriterConfig中进行一系列的默认初始化操作，包括12345678910111213141516171819202122232425262728293031323334353637383940LiveIndexWriterConfig(Analyzer analyzer) &#123; this.analyzer = analyzer; //默认在内存中缓存16.0M大小的文件 ramBufferSizeMB = IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB; //默认禁用缓存文档 maxBufferedDocs = IndexWriterConfig.DEFAULT_MAX_BUFFERED_DOCS; //禁用禁用缓存删除的Terms maxBufferedDeleteTerms = IndexWriterConfig.DEFAULT_MAX_BUFFERED_DELETE_TERMS; mergedSegmentWarmer = null; //索引删除策略，默认只保留最后一次提交的索引，在最新的提交成功后，删除所有之前的提交 delPolicy = new KeepOnlyLastCommitDeletionPolicy(); commit = null; //默认使用复合索引文件，主要是提升性能考虑 useCompoundFile = IndexWriterConfig.DEFAULT_USE_COMPOUND_FILE_SYSTEM; //索引默认打开策略，存在则追加，不存在则创建 openMode = OpenMode.CREATE_OR_APPEND; //默认的相似度计算，使用BM25Similarity similarity = IndexSearcher.getDefaultSimilarity(); //并发的合并调度器，可以指定一次最多运行的线程个数和同时合并的最大数目，如果同时合并的数目超过了线程数，那么大的合并将暂停直至小的合并完成 mergeScheduler = new ConcurrentMergeScheduler(); //使用默认的索引链 indexingChain = DocumentsWriterPerThread.defaultIndexingChain; //使用Lucene60作为默认的编解码器，主要用来对反向索引段进行编码/解码 codec = Codec.getDefault(); if (codec == null) &#123; throw new NullPointerException(); &#125; //调试用，用于IndexWriter和SegmentInfos等 infoStream = InfoStream.getDefault(); //分层合并策略 mergePolicy = new TieredMergePolicy(); //刷新策略，基于Ram或者数量 flushPolicy = new FlushByRamOrCountsPolicy(); //默认禁用reader池 readerPooling = IndexWriterConfig.DEFAULT_READER_POOLING; //用来控制如何将线程分配给DocumentsWriterPerThread indexerThreadPool = new DocumentsWriterPerThreadPool(); //设置单个段的RAM使用的硬上限，之后将强制刷新段 perThreadHardLimitMB = IndexWriterConfig.DEFAULT_RAM_PER_THREAD_HARD_LIMIT_MB; &#125; 接着会在IndexWriter的构造函数中进行获取锁的操作1234// obtain the write.lock. If the user configured a timeout,// we wrap with a sleeper and this might take some time.// 忽略上面注释，obtain(long lockWaitTimeout)已经移除writeLock = d.obtainLock(WRITE_LOCK_NAME); 这里面的d在64位的JRE中就是MMapDirectory的实例，该类从BaseDirectory继承得到obtainLock方法123456/** Holds the LockFactory instance (implements locking for this Directory instance). */protected final LockFactory lockFactory;@Overridepublic final Lock obtainLock(String name) throws IOException &#123; return lockFactory.obtainLock(this, name);&#125; 真正的获取锁的操作是在LockFactory的子类中，上面已经介绍过各个子类，所以此处的lockFactory是NativeFSLockFactory的实例，而lockFactory的引用是LockFactory类，这就是常说的父类引用指向子类实例。在LockFactory中，obtainLock是一个抽象方法，在FSLockFactory中得到实现，同时FSLockFactory是NativeFSLockFactory的父类，所以接着调用FSLockFactory中的obtainLock方法1234567@Overridepublic final Lock obtainLock(Directory dir, String lockName) throws IOException &#123; if (!(dir instanceof FSDirectory)) &#123; throw new UnsupportedOperationException(getClass().getSimpleName() + " can only be used with FSDirectory subclasses, got: " + dir); &#125; return obtainFSLock((FSDirectory) dir, lockName);&#125; 在obtainLock中调用obtainFSLock方法，这也是个抽象方法，由NativeFSLockFactory实现，所以这时候才真的跳转到了获取锁的核心实现方法里1234567891011121314151617181920212223242526272829303132333435363738@Overrideprotected Lock obtainFSLock(FSDirectory dir, String lockName) throws IOException &#123; Path lockDir = dir.getDirectory(); // Ensure that lockDir exists and is a directory. // note: this will fail if lockDir is a symlink Files.createDirectories(lockDir); Path lockFile = lockDir.resolve(lockName); try &#123; Files.createFile(lockFile); &#125; catch (IOException ignore) &#123; // we must create the file to have a truly canonical path. // if it's already created, we don't care. if it cant be created, it will fail below. &#125; // fails if the lock file does not exist final Path realPath = lockFile.toRealPath(); // used as a best-effort check, to see if the underlying file has changed final FileTime creationTime = Files.readAttributes(realPath, BasicFileAttributes.class).creationTime(); if (LOCK_HELD.add(realPath.toString())) &#123; FileChannel channel = null; FileLock lock = null; try &#123; channel = FileChannel.open(realPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE); lock = channel.tryLock(); if (lock != null) &#123; return new NativeFSLock(lock, channel, realPath, creationTime); &#125; else &#123; throw new LockObtainFailedException("Lock held by another program: " + realPath); &#125; &#125; finally &#123; if (lock == null) &#123; // not successful - clear up and move out IOUtils.closeWhileHandlingException(channel); // TODO: addSuppressed clearLockHeld(realPath); // clear LOCK_HELD last &#125; &#125; &#125; else &#123; throw new LockObtainFailedException("Lock held by this virtual machine: " + realPath); &#125;&#125; 首先会递归的创建多层目录，然后创建write.lock文件，如果已经存在抛java.nio.file.FileAlreadyExistsException异常，捕获该异常之后不做任何处理，因为一旦索引创建之后，write.lock文件会一直存在，即使IndexWriter已经关闭，所以不能以该文件是否存在来判断是否有多个IndexWriter被打开了，那么是根据什么来判断的呢？ 接着会获取该文件创建时候的时间戳，并且将该文件的真实路径加入LOCK_HELD，LOCK_HELD的声明如下，是一个同步的HashSet集合，在第一次打开IndexWriter的时候LOCK_HELD.add(realPath.toString())成功，然后调用FileChannel的API去获取锁，注意这里面重点就是channel.tryLock()获取到的是FileLock，这是系统级别的锁，即使其它进程想打开IndexWriter的时候，它虽然能够LOCK_HELD.add(realPath.toString())成功，但是在channel.tryLock()步会加锁失败，得到null，这时候程序依然会抛LockObtainFailedException异常；对于多JVM去同时写同一份索引的情况同样如此。如果在同一个进程内，后面希望打开另外的IndexWriter的时候必然LOCK_HELD.add(realPath.toString())失败，抛LockObtainFailedException异常。所以如果程序或JVM崩溃，LOCK_HELD在内存中必然也失效，系统级别的锁（使用tryLock获取的文件锁）也会释放，相当于是自动解锁了，不影响下次的重新加锁操作。1private static final Set&lt;String&gt; LOCK_HELD = Collections.synchronizedSet(new HashSet&lt;String&gt;()); 释放锁源码解析释放锁是在调用indexWriter.close()之后，默认在关闭时提交，所以会进入shutdown方法12345678@Overridepublic void close() throws IOException &#123; if (config.getCommitOnClose()) &#123; shutdown(); &#125; else &#123; rollback(); &#125;&#125; 在shutdown中，Lucene会先判断一系列预先设置的参数，然后进行刷新操作，将所有在内存中缓存的更新刷新到Directory中，然后静静等待合并结束，合并之后会进行内部的提交操作123456789101112131415161718192021222324252627282930private void shutdown() throws IOException &#123; if (pendingCommit != null) &#123; throw new IllegalStateException("cannot close: prepareCommit was already called with no corresponding call to " + "commit"); &#125; // Ensure that only one thread actually gets to do the // closing if (shouldClose(true)) &#123; boolean success = false; try &#123; if (infoStream.isEnabled("IW")) &#123; infoStream.message("IW", "now flush at close"); &#125; flush(true, true); waitForMerges(); commitInternal(config.getMergePolicy()); rollbackInternal(); // ie close, since we just committed success = true; &#125; finally &#123; if (success == false) &#123; // Be certain to close the index on any exception try &#123; rollbackInternal(); &#125; catch (Throwable t) &#123; // Suppress so we keep throwing original exception &#125; &#125; &#125; &#125;&#125; 在commitInternal(config.getMergePolicy())方法中会调用finishCommit方法，该方法会更新索引文件的代数，同时会创建段文件信息的备份，接着调用rollbackInternal()方法，该方法主要是确保没有提交操作在运行，这样即使有其它线程在同步文件到硬盘中，依然可以关闭。123456private void rollbackInternal() throws IOException &#123;// Make sure no commit is running, else e.g. we can close while another thread is still fsync'ing: synchronized(commitLock) &#123; rollbackInternalNoCommit(); &#125;&#125; 在rollbackInternalNoCommit方法中，进行释放锁的操作IOUtils.close(writeLock)，改方法会将所有需要关闭的资源转换为一个List链表123public static void close(Closeable... objects) throws IOException &#123; close(Arrays.asList(objects));&#125; 然后逐个关闭该链表中持有的资源12345678910111213141516public static void close(Iterable&lt;? extends Closeable&gt; objects) throws IOException &#123; Throwable th = null; for (Closeable object : objects) &#123; try &#123; if (object != null) &#123; object.close(); &#125; &#125; catch (Throwable t) &#123; addSuppressed(th, t); if (th == null) &#123; th = t; &#125; &#125; &#125; reThrow(th);&#125; 这里的object.close()关闭操作调用的就是NativeFSLock中的close方法，因为object其实就是NativeFSLock的实例，由NativeFSLock的父类Lock持有引用。NativeFSLock中的close方法如下12345678910111213141516@Overridepublic synchronized void close() throws IOException &#123; if (closed) &#123; return; &#125; // NOTE: we don't validate, as unlike SimpleFSLockFactory, we can't break others locks // first release the lock, then the channel try (FileChannel channel = this.channel; FileLock lock = this.lock) &#123; assert lock != null; assert channel != null; &#125; finally &#123; closed = true; clearLockHeld(path); &#125;&#125; 因为NativeFSLock是NativeFSLockFactory的静态内部类，在finally中会直接调用NativeFSLockFactory的clearLockHeld方法，该方法代码如下123456private static final void clearLockHeld(Path path) throws IOException &#123; boolean remove = LOCK_HELD.remove(path.toString()); if (remove == false) &#123; throw new AlreadyClosedException("Lock path was cleared but never marked as held: " + path); &#125;&#125; 至此又看到了熟悉的身影，LOCK_HELD一个同步的HashSet集合会移除write.lock文件的真实路径，释放锁完毕，结束。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《动物农场》]]></title>
    <url>%2F2016%2F11%2F20%2FRead-animal-farm%2F</url>
    <content type="text"><![CDATA[作者简介：乔治·奥威尔（1903—1950），原名埃里克·亚瑟·布莱尔。英国伟大的人道主义作家、新闻记者和社会评论家，著名的英语文体家，以小说《动物农场》和《一九八四》闻名于世。1944年写成讽刺苏联革命的政治寓言小说《动物农场》，次年出版后头一次使奥威尔名利双收。1949年出版政治讽喻小说，也是他最后的作品《一九八四》。 《动物农场》讲述了农场的一群动物成功地进行了一场“革命”，将压榨他们的人类东家赶出农场，建立起一个平等的动物社会。然而，动物领袖，那些聪明的猪们最终却篡夺了革命的果实，成为比人类东家更加独裁和极权的统治者。不妨可以把这个故事理解为从封建主义走向社会主义，表面上看是一个极大的进步，动物们实现了自己当家做主的愿望。可是愿望终归是愿望，它并不是现实，当每个动物们都喊着民主的时候，伴随着它们那首随民主而诞生的歌谣，却过着比以前更辛苦的生活以及遭受着更严重的压迫与剥削，看似的民主早已不是真正的民主了。透过现象看本质，不管口号与名称如何变，有些事物的本质却并没有变。比如人民代表大会制度，有哪个代表是代表人民的呢？又有哪个代表是人民选出来的呢？当然了，特权阶层总有忽悠你们的借口，比如实现全民选举并不现实。那么我们可以实现部分全民选举吗？例如在发达的珠三角、长三角、京津冀这些发达的地方实现全民选举不是很好吗？如果特权阶层说人口的素质跟不上，还不能实现全民选举，那么在知识分子中率先实现全民选举如何呢？在高素质人才聚集的高校科研院所率先实现全民选举如何呢？当然了特权阶层又会说了……，理由总是无数的，真要有决心改变的话，方法也总是有的。 绝对的权利造就绝对的腐败，在动物农场中是这样，在现实世界中亦是如此。不知多少年前就说把权利关进笼子里，现今呢？权利还在笼子外面欢快的跑着呢。最近在新西兰地震营救事件上，又掀起了一股盲目地自我感觉爆棚之风，吹牛逼者甚重，不明吃瓜群众盲目鼓吹者甚重。我不得不向那些无知愚昧之人泼上一大盆冷水，不是因为他们的无知得到了广泛的传播，更是因为无知的看官，助长了他们无知的传播。他们说外国使馆不救援，我觉得恰恰是因为他们有着良好的预算制度，不会无缘无故去滥用纳税人的钱款，虽然就营救一事上，其存在着弊端，但是它带来的好处，必将在其他方方面面有所体现，而这些方面所带来的好处将远远超过这一场不及时营救所带来的价值。所以我们在攻击国外使馆不救援的同时，也思考一下为什么我们纳税人的钱能不受限制的随意使用？为什么我们的纳入人的钱还要拿来营救外国人？本着人道主义精神，救外国人当然是可以的，我想表达的重点是纳税人的钱根本不受任何的限制，可以被统治阶层恣意挥霍，这带来的弊端将远远大于它所带来的这场及时的营救所带来的益处大的多得多。每种制度都有其优劣之分，我们所能做的仅仅是一种取舍，选取那个给我们带来收益更多的制度，而摒弃那种弊端大于收益的制度。 一个社会的进步需要靠全民素质的提升，进而去推动社会做出哪怕微小的进步，积少成多、集腋成裘，这些微小的进步只要不断积聚，终究会形成强大的优势。但是事实上，这么多年来，人们的学历不断提高，其素质与政治觉悟却并没有跟着提高，整个社会的风气与价值观也是越来越病态的发展着，所有人都是一副事不关己高高挂起的心态，对不涉及自己利益的事情完全不关心。难道忘记了那个血腥的沉默的代价了吗？起初他们抓了所有的共产党人；我没有出声，因为我不是共产党人。接着他们抓了所有的犹太人；我没有出声，因为我不是犹太人。然后他们抓了所有的工会骨干；我没有出声，因为我不是工会骨干。后来他们抓了所有的天主教徒；我没有出声，因为我属于新教。最后他们来抓我；到那时候，已经没有剩下能出声讲话的人了。 极权不是在我们与资本主义决裂的时候被同时抛弃了，而是任何制度都会产生的一种东西。作为不能影响政治的人，在你吃不饱、穿不暖、自由受到限制的时候，要相信自己的感官，而不是官方的宣传。并确信，出现这种状况绝对是不正常的。这个时候，极权很可能已经形成了。水能载舟亦能覆舟，当大多数人不在沉默中爆发的话就会在沉默中灭亡，这条定理如今一样适用。如果有兴趣的话，可以去看一个类似的实验，斯坦福监狱实验。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《精益创业》]]></title>
    <url>%2F2016%2F11%2F20%2FRead-the-lean-startup%2F</url>
    <content type="text"><![CDATA[作者简介：埃里克•莱斯，IMUV联合创始人及CTO，哈佛商学院驻校企业家，其“精益创业”的理念被《纽约时报》、《华尔街日报》、《哈佛商业评论》、《赫芬顿邮报》等多家媒体广泛报道。他还为多家新创企业、大型公司及风险投资公司提供商业及产品战略方面的咨询服务。 愿景精益创业的主要内容是一种不断形成创新的方法，它源于“精益生产”的理念，提倡企业进行“验证性学习”，先向市场推出极简的原型产品，然后通过不断地试验和学习，以最小的成本和有效的方式验证产品是否符合用户需求，灵活调整方向。如果产品不符合市场需求，最好能“快速地失败、廉价地失败”，而不要“昂贵地失败”；如果产品被用户认可，也应该不断学习，挖掘用户需求，迭代优化产品。这一模式，不仅针对车库创业派，对于全球最大企业内部的新创业务也同样适用。 创业者判断其进展的方法应该和其它类型的企业有所不同。制造业的发展是用高质量的实体产品生产来衡量的，而精益创业则采用不同的发展单元，这些发展单元称为“经证实的认知”，用科学的认知作为衡量标准，我们可以发现并消除令创业者苦恼的浪费根源所在。 新创企业千万不要怕别人抄袭，在当年大家都骂腾讯抄遍全国的时候，真应该把书中的如下观点亮出来：作为一个新创企业，想要你的构想、公司或产品不为任何人所知，几乎是不可能的，更别提竞争对手了，不要担心大型成熟的企业会盗用了新创企业的构想，要知道盗用一个好点子并没有那么容易。而且一般在大多数公司，大多数经理人对好的构想早已应接不暇，他们的挑战是如何把这些构想排出先后顺序，并且落实执行。正因为如此，才给了新创企业生存的希望。一家新创企业迟早要直面快速跟进者的竞争，先发优势几乎起不了太大的作用。而避开消费者的那种秘密开发状态所争取的时间，也几乎不可能带来起步优势。唯一的取胜之道是比任何人学得更快。 驾驭新创公司在成长过程中会遭遇诸多困难，最严重的是增长引擎熄火，这时候要么转型要么等死。如果及早转型的话，或许还有成功的可能。转型有各种各样的形式，主要有如下几种： 放大转型：之前被视为产品中单独的一个功能特性，成为产品的全部 缩小转型：在相反的情况中，有时候单独的一个功能不足以支持整个产品，这类转型就是把原来的整个产品转化为一个更大型产品中的一项单独的功能特性 客户细分市场转型：产品的前提假设得到部分证实，解决了相关问题，但针对的是与原本预期不同的顾客 客户需求转型：目前拥有200多家连锁店的大肚三明治就是一个注明的例子，它在1997年开办时是一家古董店。为了吸引更多顾客到他们店里，店主开始售卖三明治。他们很快就改了行，转型从事一种完全不同的业务 平台转型：平台转型指的是从应用产品转为平台产品，或反方向的转化 商业架构转型：商业架构主要有两种，其一高利润低产量模式，其二低利润高产量模式。有一些公司通过进入大众市场，从高利润低产量架构中转型；另一些原本计划服务大众市场的公司，却发现他们需要长时间的、昂贵的销售周期 价值获取转型：有很多方法来获取公司创造的价值。通常，公司获取价值方式的改变，会对业务的其他部分、产品和市场营销战略造成深远的影响 增长引擎转型：带动新创企业成长的增长引擎主要有三种，病毒式、黏着式和付费式。在这类转型中，公司为寻求更快速、更高利润的增长而改变其增长战略。增长引擎转型也会要求相应改变价值获取的方式，这种要求虽非必然，却也常见 渠道转型：在传统销售术语中，公司把产品交付顾客的途径称为销售渠道，或分销渠道。只要公司放弃了过去复杂的销售流程，转向针对终端顾客“直接销售”，那么渠道转型就发生了 技术转型：有时候，公司会发现运用一种截然不同的技术，也可以获得相同的解决方案，技术转型在成熟的企业业务中更加常见。也就是说，它是一种用来吸引并保留现有顾客群的可持续创新，一种递增式的改进 转型并非仅仅是一种改变。要记住，它是一种有组织有条理的特殊改变，用以测试一个关于产品、商业模式和增长引擎的新的基础假设。它是精益创业的核心所在，让采用精益创业的企业在错误面前百折不挠：如果我们转错了弯，我们有必要的工具来发现错误，并能迅速找到另一条道路。 加速创投资本投资家肖恩·卡罗曾这样说过，“新创企业不会饿死，而会饱死”。总有无数让产品变得更好的想法飘荡在半空，但现实是残酷的，大多数想法带来的改变微乎其微，只能算是产品优化而已。新创企业必须关注能产生经证实的认知的重大实验。增长引擎的框架结构帮助他们把注意力集中在紧要的衡量指标上。在新创企业高速成长的过程中，一定要抓住核心点，而不要过多关注次要点，尽可能的实现企业的高速成长以及规模的快速增加。 使用黏着式增长引擎的公司要非常仔细地追踪顾客损耗率，亦称流失率。所谓流失率就是指在任意一段时间内，没有继续使用公司产品的那部分顾客占顾客总数的比率。控制黏着式增长引擎的规则很简单：如果取得新顾客的比率超过流失率，产品将会增长。增长的速度取决于“复合率”，其实就是自然增长率减去流失率。 依靠病毒式增长引擎的公司必须关心如何提高病毒系数，这比其他任何事情都重要。这个数字哪怕只发生微小的变化，都会让公司未来的前景产生戏剧性变化。 不管是新创企业还是企业内部的新创企业，都要具备三种架构特征：稀少但稳定的资源，开发业务的独立权，以及与绩效挂钩的个人利益。这些组织架构的要求和成熟企业部门中的架构要求不同。要记住，组织架构只是先决条件，它不能保证成功。但是，错误的架构却几乎一定会导致失败。 人在一个良好运转的体系中，其创造的价值不亚于一个伟大人物在一个糟糕的体系中所创造的价值。这一点泰罗在其《科学管理原则》中早有指出，他说过去，人是第一位的；将来，体系必须是第一位的。这并不意味着不再需要伟大人物。恰恰相反，任何好体制的第一位目标必须是发掘第一流的人才，并在系统管理之下，使最佳人才能比以前更有把握、更迅速地提升到领导岗位上来。 李开复序创业的第一阶段是把想法变成产品。这时开发的产品是精简的原型，投入最少的金钱和精力开发出体现核心价值的产品。此时创业者们率领精干的成员，用类似特种部队的组织方式，在有限的资源和时间窗口内用很短的时间做出产品，并快速投入市场，通过不断的小规模实验，获得顾客反馈，进而不断迭代，让产品得到市场验证。 创业的第二阶段，新创企业要对正确的产品形态进行重点投入，做好做细，做“最了解用户的人”，做到极致。这个阶段，必须用最小的成本，在最短时间里找到最有价值的认知。 创业的第三阶段，成功者往往伴随着爆发式的增长，全面的扩张。企业开始与传统的、陈旧的势力展开阵地战。这一过程中，创业企业的力量之源正是来自此前积累的对用户的深入理解和对市场的快速反应，即“爱与速度”。能达到第三个阶段的创业企业，大都把“对用户有爱，对产品有爱”作为一种信仰。他们不能容忍产品有缺点，不能容忍产品的用户体验不好，不能容忍BUG，跟0.1秒的延迟较劲，跟0.1M的大小较真……]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《稀缺:我们是如何陷入贫穷与忙碌的》]]></title>
    <url>%2F2016%2F11%2F19%2FRead-scarcity-why-having-too-little-means-so-much%2F</url>
    <content type="text"><![CDATA[作者简介塞德希尔·穆来纳森（Sendhil Mullainathan）哈佛大学终身教授，哈佛大学行为经济学领域重要领头人。与普林斯顿大学心理学教授埃尔德·沙菲尔等人联合创立非营利性组织ideas42，致力于利用行为科学帮助人们解决社会问题。在麻省理工学院，与《贫穷的本质》作者阿比吉特·班纳吉等人联合创立“贫困行动实验室”，并于2002年荣获“麦克阿瑟天才奖”。出生于印度农村，7岁时随父母移民美国。1993年，获得康奈尔大学计算机科学、数学和经济学3个学士学位。1998年，获得哈佛大学经济学博士学位。 埃尔德·沙菲尔（Eldar Shafir）1988年，获得麻省理工学院认知科学博士学位。古根海姆奖获得者。普林斯顿大学心理学教授，研究领域涉及认知科学、判断与决策、行为经济学等。与诺贝尔经济学奖获得者彼得·戴蒙德和著名心理学家阿莫斯·特沃斯基共同开展过“货币幻觉”方面的研究。 阅读一本书并不是要把每一行的每个字都认一遍，而是较认真地弄明白作者的意思，并理解作者的意思，才能进一步真正掌握一本书的观点、视角以及可能的现实应用。 稀缺心态是一切稀缺的根源稀缺俘获大脑：稀缺造成的后果不仅仅是因为我们会因拥有的太少而感到不悦，而是因为它会改变我们的思维方式，会强行侵入我们的思想之中。 稀缺心态：稀缺是一种心态。当它俘获我们的注意力时，就会改变我们的思维方式，影响我们的决策和行为方式。 带宽：就是心智的容量，包括两种能力，分别为认知能力和执行控制力。稀缺会降低所有这些带宽的容量，致使我们缺乏洞察力和前瞻性，还会减弱我们的执行控制力。 管窥：专注于某一事物就意味着我们会忽略其他事物，也叫“隧道视野”。即只能透过“管子”的孔洞看清少量物体，而无视管外的一切。 稀缺会俘获我们的注意力，并带来一点点好处：我们能够在应对迫切需求时做的更好。但从长远的角度来看，我们的损失更大：我们会忽视其他需要关注的事项，在生活的其他方面变得不那么有效。 当稀缺俘获大脑时，人们的注意力会集中在紧急的事情上，并将其他事物排除在外。这种专注会让人们从稀缺中获益，让人们获得“专注红利”。由于“目标抑制”的作用，人们在专注于某项重要事物的同时，就不容易想到其它重要事物。因此，专注也会导致管窥，让人们的视野变窄，从而付出沉重代价。无论是有关工作还是娱乐，只要时间有限，我们都会尽量将其利用的淋漓尽致。我们将这种现象称为“专注红利”，也就是稀缺俘获大脑时所产生的积极成果。 如果你对此不甚了解，举一个消防员的例子，据统计25%的消防员死于机动车相撞事故，而在这些事故中的79%是由于消防员没有系安全带。也许你会说，让他们每个人都系上不就完事了。原因就在于此，消防员在接到报警后，他们直面的就是时间稀缺问题，他们不仅要迅速地跳上消防车，赶赴火灾现场，而且要在抵达现场前做好诸多准备工作——制定消防策略；研究起火建筑的结构和布局；制定出进出火灾现场的路线；计算出所需水龙头的数量等。所有这些，导致时间稀缺，他们跟本没有精力去记得系上安全带。 持续的关注对我们的思想产生了影响，将我们吸入“管子”之内。正如外部噪声会干扰我们进行清晰而有序的思考一样，稀缺也会让我们产生内部干扰。稀缺还会直接减少带宽——不是减少某人与生俱来的带宽容量，而是减少其当下能用得上的容量。 认知能力：它是我们解决问题、获得信息、进行逻辑推理等能力背后的心理学机制。认知能力中最突出的就是“流体智力”，即在进行抽象思维和推理时，在无须特定学习或体验的情况下解决问题的能力。 执行控制力：其作用存在于我们管理自身认知能力的过程中，包括计划、关注、发起并抑制行为和控制冲动等。 分心、爱忘事、缺乏冲动控制能力。这些效应的规模也体现了带宽负担对各类行为所产生的实质性影响，甚至包括那些一般被归于“个性”或“才华”的耐心、忍耐力、关注力和风险精神。所以人一旦身陷贫穷，其有效带宽都会变窄。 稀缺，不仅仅会令我们入不敷出，不知如何分配资源，而且还会让我们在生活的其他方面手足无措。稀缺会使人变笨，变得更加冲动。我们不得不在流体智力和执行控制力被减弱的情况下，依靠更为有限的脑力去勉强度日。生活，就这样变得举步维艰起来。 贫穷和忙碌是如何让“带宽”变窄的权衡式思维：它是由稀缺所引发的一种思维方式。在稀缺状态下，因为所有没有被满足的需要俘获了我们的大脑，以致我们开始对之念念不忘，开始产生决策难题。 现实偏见：我们会将未来的利益作为代价，过高地估计即刻的利益。 稀缺陷阱：就是某人的行为有助于稀缺形成的一种情况。 当今天的资源，无论是时间还是金钱，能在当下为你带来比未来更大的利益时，借用就是行得通的做法。而当我们有了管窥之见时，借用就会超越得失均衡的限度。当面临稀缺状况时，我们的借用，从长期来看通常是不合理的。 穷人会一直贫穷下去，而孤独者也注定会继续形单影只；繁忙之人永远会日理万机，而节食者的计划也会以失败告终。稀缺造就了一种心态，而这种心态会令稀缺长存。稀缺心态是环境造成的结果，更有可能通过一些措施来加以改变。稀缺并非个人特质，而是自身创造的环境条件所引发的结果，而这些条件是可以进行管理的。我们越是深入了解稀缺在大脑中的发展历程，就越有可能找到办法去避免稀缺陷阱，或至少去减轻稀缺陷阱的影响程度。 良好的教育水平需要足够的带宽、复杂的决策能力和自我牺牲精神。最需要用劳动换取薪水的穷人，他们的生产力却承受着最为沉重的负担。负担过重的带宽，意味着处理新信息的能力有所减弱。同时，带宽负担也意味着用来进行自我控制的心智资源变得更少了。任何形式的技能习得，无论是去学习社交技巧还是养成良好的消费习惯，都需要带宽。如果穷人缺乏带宽，那么他们就无法很好地掌握这些实用技能。 如何从稀缺走向富足通过外力将人们所处的环境进行小小的改变，把重要的事情拉入“管子”视野，就能缓解稀缺带来的不良后果。当带宽有限时，让“疏忽”等同于“默许”就是一个不错的办法。稀缺会产生带宽负担，因此节约利用带宽是对稀缺进行管理的重要内容。应对稀缺，要在富足和带宽充裕时开始行动。更重要的是，要留有应对突发事件的余闲。 人们总是忽略带宽的重要性。当决定做某件事情时，人们会想到他们能利用的时间以及这件事需要花费的时间，但很少会想到带宽。迫近截止日期的稀缺体验，起因于人们在充裕时所采取的时间管理方式，我们在“富足”时往往想不到为将来留有余闲。所以，只要规划和管理好“带宽”，稀缺问题就会迎刃而解。 全文用大量的示例来说明人们是如何陷入稀缺的，以及给出了针对性极强的稀缺处理策略。但是有些例子看来不免可笑乏味，比如肯尼亚的农民居然连给作物施肥都不愿意做，原因无他，没钱买肥料，但是有钱的时候他们依然不会去购买肥料，在我看来，陷入稀缺的人们不外乎心智不足以及自制力不够，这才是最根本的原因。而对这些人无论如何拯救，其作用都是有限的。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows安装Redis服务]]></title>
    <url>%2F2016%2F11%2F19%2FWindows-installs-the-redis-service%2F</url>
    <content type="text"><![CDATA[Windows安装Redis首先Redis官方并不支持Windows，而Windows版的Redis是由MSOpenTech提供的支持，所以首先去下载一个发布版，我选择的是Redis-x64-3.2.100.zip压缩包，解压缩得到如下文件列表1234567891011121314EventLog.dllRedis on Windows Release Notes.docxRedis on Windows.docxredis.windows.confredis.windows-service.confredis-benchmark.exeredis-benchmark.pdbredis-check-aof.exeredis-check-aof.pdbredis-cli.exeredis-cli.pdbredis-server.exeredis-server.pdbWindows Service Documentation.docx 有了这些文件之后，就可以开始安装服务了。12345678910111213141516171819202122D:\Redis-x64-3.2.100&gt;redis-server.exe redis.windows.conf _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 3.2.100 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6379 | `-._ `._ / _.-&apos; | PID: 33824 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos;[33824] 10 Nov 15:32:54.884 # Server started, Redis version 3.2.100[33824] 10 Nov 15:32:54.885 * The server is now ready to accept connections on port 6379 在启动成功之后，可以使用自带的客户端工具进行测试。双击打开“redis-cli.exe”，如果成功连接上之后就可以进行set及get等操作了。如果需要帮助，可以输入help查看。 12345678910111213141516127.0.0.1:6379&gt; set &quot;key&quot; &quot;testValue&quot;OK127.0.0.1:6379&gt; get &quot;key&quot;&quot;testValue&quot;127.0.0.1:6379&gt; helpredis-cli 3.2.100To get help about Redis commands type: &quot;help @&lt;group&gt;&quot; to get a list of commands in &lt;group&gt; &quot;help &lt;command&gt;&quot; for help on &lt;command&gt; &quot;help &lt;tab&gt;&quot; to get a list of possible help topics &quot;quit&quot; to exitTo set redis-cli perferences: &quot;:set hints&quot; enable online hints &quot;:set nohints&quot; disable online hintsSet your preferences in ~/.redisclirc 再输入help空格，然后敲tab键，可以像命令提示符一样显示候选项。通常不需要每次都使用cmd界面去操作，所以附上一些快速处理的脚本，注意在安装服务之前先关闭防火墙和杀毒软件，否则安装不成功。 快速操作脚本service-install.bat redis-server –service-install redis.windows.conf –loglevel verbosepause 安装成功之后输出结果： D:\Redis-x64-3.2.100&gt;redis-server –service-install redis.windows.conf –loglevel verbose[13868] 26 Dec 09:39:58.364 # Granting read/write access to ‘NT AUTHORITY\NetworkService’ on: “D:\Redis-x64-3.2.100” “D:\Redis-x64-3.2.100\”[13868] 26 Dec 09:39:58.365 # Redis successfully installed as a service.D:\Redis-x64-3.2.100&gt;pause请按任意键继续. . . uninstall-service.bat redis-server –service-uninstallpause 卸载成功之后输出结果： D:\Redis-x64-3.2.100&gt;redis-server –service-uninstall[11368] 26 Dec 09:45:47.460 # Redis service successfully uninstalled.D:\Redis-x64-3.2.100&gt;pause请按任意键继续. . . 注意在卸载服务之前，需要先去控制面板|管理工具|服务|停止Redis服务，然后才能卸载成功。 starting-service.bat redis-server –service-startpause 服务启动成功之后输出： D:\Redis-x64-3.2.100&gt;redis-server –service-start[20120] 26 Dec 10:12:30.946 # Redis service successfully started.D:\Redis-x64-3.2.100&gt;pause请按任意键继续. . . stopping-service.bat redis-server –service-stoppause 服务停止成功之后输出： D:\Redis-x64-3.2.100&gt;redis-server –service-stop[19612] 26 Dec 10:19:27.974 # Redis service successfully stopped.D:\Redis-x64-3.2.100&gt;pause请按任意键继续. . . 命名服务，一个可选的参数可以用来指定安装的服务的名称，这个参数可以跟在service-install、service-start、service-stop或service-uninstall命令的后面。比如以下命令会安装和启动三个独立的Redis实例 redis-server –service-install –service-name redisService1 –port 10001redis-server –service-start –service-name redisService1redis-server –service-install –service-name redisService2 –port 10002redis-server –service-start –service-name redisService2redis-server –service-install –service-name redisService3 –port 10003redis-server –service-start –service-name redisService3 Windows配置主从Redis先将Redis目录复制一份，命名为Redis-x64-3.2-Slave，并将原Redis目录命名为Redis-x64-3.2-Master，注释掉master目录redis.windows.conf文件的集群配置，或者将其设置为no # cluster-enabled yes# 或者设置为nocluster-enabled no 修改slave目录redis.windows.conf的端口号为6380，同样注释掉集群配置或设置为no，在slaveof后面添加master机器的IP和端口号 port 6380slaveof 127.0.0.1 6379# cluster-enabled yes# 或者设置为nocluster-enabled no 然后先启动Master，之后再启动Slave机器，输出如下 123456789101112131415161718192021222324252627282930313233D:\Redis-x64-3.2-Master&gt;redis-server redis.windows.conf[20048] 26 Dec 13:04:49.064 * Node configuration loaded, I&apos;m f1c7db8c98b01e9fbd5089b1231ca0e2d23a1766 _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 3.2.100 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in cluster mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6379 | `-._ `._ / _.-&apos; | PID: 20048 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos;[20048] 26 Dec 13:04:49.066 # Server started, Redis version 3.2.100[20048] 26 Dec 13:04:49.067 * DB loaded from disk: 0.000 seconds[20048] 26 Dec 13:04:49.067 * The server is now ready to accept connections on port 6379[20048] 26 Dec 13:05:14.723 * Slave 127.0.0.1:6380 asks for synchronization[20048] 26 Dec 13:05:14.723 * Full resync requested by slave 127.0.0.1:6380[20048] 26 Dec 13:05:14.724 * Starting BGSAVE for SYNC with target: disk[20048] 26 Dec 13:05:14.726 * Background saving started by pid 26568[20048] 26 Dec 13:05:14.867 # fork operation complete[20048] 26 Dec 13:05:14.867 * Background saving terminated with success[20048] 26 Dec 13:05:14.869 * Synchronization with slave 127.0.0.1:6380 succeeded 12345678910111213141516171819202122232425262728293031323334353637D:\Redis-x64-3.2-Slave&gt;redis-server redis.windows.conf _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 3.2.100 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6380 | `-._ `._ / _.-&apos; | PID: 21036 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos;[21036] 26 Dec 13:05:14.720 # Server started, Redis version 3.2.100[21036] 26 Dec 13:05:14.721 * DB loaded from disk: 0.001 seconds[21036] 26 Dec 13:05:14.721 * The server is now ready to accept connections on port 6380[21036] 26 Dec 13:05:14.721 * Connecting to MASTER 127.0.0.1:6379[21036] 26 Dec 13:05:14.722 * MASTER &lt;-&gt; SLAVE sync started[21036] 26 Dec 13:05:14.722 * Non blocking connect for SYNC fired the event.[21036] 26 Dec 13:05:14.722 * Master replied to PING, replication can continue...[21036] 26 Dec 13:05:14.722 * Partial resynchronization not possible (no cachedmaster)[21036] 26 Dec 13:05:14.726 * Full resync from master: a326a4da325133537c655c16b3a6c2546620f816:1[21036] 26 Dec 13:05:14.868 * MASTER &lt;-&gt; SLAVE sync: receiving 75 bytes from master[21036] 26 Dec 13:05:14.871 * MASTER &lt;-&gt; SLAVE sync: Flushing old data[21036] 26 Dec 13:05:14.871 * MASTER &lt;-&gt; SLAVE sync: Loading DB in memory[21036] 26 Dec 13:05:14.872 * MASTER &lt;-&gt; SLAVE sync: Finished with success 如果在配置中不小心启用了cluster-enabled yes的话，会抛slaveof directive not allowed in cluster mode和(error) CLUSTERDOWN Hash slot not served异常。 主从测试在主机中使用set，添加一个key之后 D:\Redis-x64-3.2-Master&gt;redis-cli127.0.0.1:6379&gt; set “mm” “mm in master”OK 在从机中使用get，得到这个key的value D:\Redis-x64-3.2-Slave&gt;redis-cli127.0.0.1:6379&gt; get “mm”“mm in master” 反过来，在从机使用set之后也可以同步到master D:\Redis-x64-3.2-Slave&gt;redis-cli127.0.0.1:6379&gt; set “kk” “mm in slave”OK 在主机get得到value D:\Redis-x64-3.2-Master&gt;redis-cli127.0.0.1:6379&gt; get “kk”“mm in slave” 注意，redis的主从配置和集群配置方法是不同的，从机一般是只读的，主机用来进行写操作，一个主数据库可以有多个从数据库，而一个从数据库只能有一个主数据库。一般redis的持久化策略是： save 900 1 #当有一条Keys数据被改变时，900秒刷新到Disk一次save 300 10 #当有10条Keys数据被改变时，300秒刷新到Disk一次save 60 10000 #当有10000条Keys数据被改变时，60秒刷新到Disk一次 Redis的主从复制是建立在内存快照的持久化基础上的，只要有Slave就一定会有内存快照发生。可以很明显的看到，RDB有它的不足，就是一旦数据库出现问题，那么我们的RDB文件中保存的数据并不是全新的。从上次RDB文件生成到Redis停机这段时间的数据全部丢掉了。 AOF(Append-Only File)比RDB方式有更好的持久化性。由于在使用AOF持久化方式时，Redis会将每一个收到的写命令都通过Write函数追加到文件中，类似于MySQL的binlog。当Redis重启是会通过重新执行文件中保存的写命令来在内存中重建整个数据库的内容。对应的设置参数为： appendonly yes #启用AOF持久化方式appendfilename appendonly.aof #AOF文件的名称，默认为appendonly.aofappendfsync always #每次收到写命令就立即强制写入磁盘，是最有保证的完全的持久化，但速度也是最慢的，一般不推荐使用。appendfsync everysec #每秒钟强制写入磁盘一次，在性能和持久化方面做了很好的折中，是受推荐的方式。appendfsync no #完全依赖OS的写入，一般为30秒左右一次，性能最好但是持久化最没有保证，不被推荐。 AOF的完全持久化方式同时也带来了另一个问题，持久化文件会变得越来越大。比如我们调用INCR test命令100次，文件中就必须保存全部的100条命令，但其实99条都是多余的。因为要恢复数据库的状态其实文件中保存一条SET test 100就够了。为了压缩AOF的持久化文件，Redis提供了bgrewriteaof命令。收到此命令后Redis将使用与快照类似的方式将内存中的数据以命令的方式保存到临时文件中，最后替换原来的文件，以此来实现控制AOF文件的增长。由于是模拟快照的过程，因此在重写AOF文件时并没有读取旧的AOF文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的AOF文件。 no-appendfsync-on-rewrite yes #在日志重写时，不进行命令追加操作，而只是将其放在缓冲区里，避免与命令的追加造成DISK IO上的冲突。auto-aof-rewrite-percentage 100 #当前AOF文件大小是上次日志重写得到AOF文件大小的二倍时，自动启动新的日志重写过程。auto-aof-rewrite-min-size 64mb #当前AOF文件启动新的日志重写过程的最小值，避免刚刚启动Reids时由于文件尺寸较小导致频繁的重写。 参考文献[1] http://blog.chinaunix.net/uid-20682890-id-3603246.html]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《引爆点》]]></title>
    <url>%2F2016%2F11%2F07%2FRead-the-tipping-point%2F</url>
    <content type="text"><![CDATA[作者简介：马尔科姆·格拉德威尔，曾是《华盛顿邮报》商务科学专栏作家，目前是《纽约客》杂志专职作家。2005年被《时代》周刊评为全球最有影响力的100位人物之一。2005年，他更是创造书市神话，两部作品同时位居《纽约时报》畅销书排行榜精装本和平装本第一名。 引爆点主要解决了关于流行的三大难题：谁制造流行？流行的奥秘何在？如何创造流行？作者归纳了引爆流行的三项法则，即个别人物法则、附着力因素法则、环境威力法则，并根据这些法则剖析了种种极具感染力的事件，例如各种风尚、传染病传播、电视节目、直邮广告等。 我们常常会与那些同自己共事的人或与那些与自己做事方式相似的人成为朋友。换句话说，我们并不是在挑选朋友本身。我们往往和那些与自己生活工作在同一个小空间的人发生联系。正如我们是依靠一些人把我们与其他人联系起来，同样地，我们也是依靠一些人把我们与信息联系在一起。既存在人际疏通专家，也存在信息传播专家。 环境威力法则的实质就是，对于有些环境来说情况也是如此——我们所处的外部环境决定着我们的内心状态，尽管我们对此并不完全了解。环境威力法则的一个最知名的例子就是破窗户理论，虽然我们的内心知道该做什么或者不该做什么，但在潜意识中还是极易受到外部环境的影响，进而做出并非本意的事情，另外一项著名的例子是斯坦福大学的模拟监狱实验，那些监狱看守们，他们当中有些人原来认为自己是反对暴力的和平主义者，很快就变成了冷酷的训导者。该实验得出的结论是，在某些具体情境下，我们内在的癖性会屈服于强大的情境。 该文通过大量的例子以及实验来说明产生这些现象背后的原因，读来稍感乏味，在最后部分，作者总结如下 要想发起流行潮，就必须做到把有限的资源集中用到关键方面 世界并非是我们一厢情愿认为的我们直觉中的世界 成功发起流行潮最重要的因素，是要具备一个基本的信念，那就是，制造变化是可能的，人们是能够在一些特定力量的驱使下，骤然改变自己的行为或观念的 引爆点最终再次证实了存在改变一切的能力和采取明智行动的力量。看看周围世界吧，它看上去似乎雷打不动、无法改变。其实不然。只要你找准位置，轻轻一触，它就可能倾斜。 我们都生活在一个遵循着梅特卡夫定律的世界中，该定律称：一个网络的价值是与其节点数量成正比的。传统经济中，越稀缺价值越高，例如钻石和黄金。但是网络的逻辑与此相反，只有加入的人越多，网络的价值才越大。但是信息呢？信息从早期的稀缺到如今的过载，我们每个人都被淹没在信息的洪流中，当人们被铺天盖地的信息淹没，他们就对这些形式的交流产生了免疫力。作为替代，他们开始向生活中自己尊敬的人、仰慕的人和信任的人征询意见，从他们那里获取信息。由此我在想是不是建立一个权威的品牌网站能更好的帮助我们做出决策呢？比如无论选购何种东西都会被众多的品牌和型号所淹没，不可能每个人都是方方面面的专家，由此若推出细分的商品品牌推荐榜，会不会有市场呢？如果没有这么多的专家，可以借鉴豆瓣的模式，由用户对品牌进行添加、编辑、评分，将该品牌评分榜单展示在细分的商品名下，当我们需要做出购买决策时，借助该商品的推荐榜单，能否让我们用最短的时间做出最正确的决定呢？]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《大停滞？科技高原下的经济困境：美国的难题与中国的机遇》]]></title>
    <url>%2F2016%2F11%2F04%2FRead-the-great-stagnation%2F</url>
    <content type="text"><![CDATA[作者简介：泰勒•考恩Tyler Cowen，哈佛大学经济学博士，目前任教于乔治梅森大学，并主持该校知名智库Mercatus Center。他的博客在全美经济类点阅率的排行榜上高居第二位，超过克鲁格曼在《纽约时报》的博客。《彭博商业周刊》称他为“美国最炙手可热的经济学家”，也入选英国《经济学人》过去十年“最有影响力的经济学家”，《大停滞》一书即源于其广受欢迎的博客，整理出版后即成为当年畅销之作。因其在此书中对科技与生产力关系的独特思考，使其被誉为“下一个托马斯•弗里德曼”。 偶遇此书，薄薄的一本，遂全读之，一气呵成。我觉得此书所谈之事未必适于中国，比如：停滞，并不是一场金融危机，而是一代人的懒惰与不思进取。中国的问题更多的是政策性的问题，而不是国民懒惰与不思进取，天朝人民历来是世界上勤劳与忍耐度并列第一的民族。与西方不同，对于民选政府，从政者必然极力讨好选民，导致西方社会福利负担过重，而整个社会生产效率与能力却并未提升，人的生活水平倒是不断提升，靠着救济金就能挣着比工作还多的钱，试问谁还会去努力工作？今天的西方国家，政府开支占GDP的比例已经从1875年的5%增长到50%左右，个别国家甚至一度高达60%，也和民主制度有关。 在该书中，作者指出1970年代中期开始，已经进入一个停滞阶段。这个停滞，可以归结为两个主要原因。一是人类的天性，喜欢采摘“低垂的果实”，即贪图便宜，总是挑容易的事先做，向上爬的苦事一定要等有压力才肯使劲。二是美国进入了“科技高原”，即科技创新也遭遇了高原“窒息”，换句话说，科技进步也停滞了。这两点都是谈不上多么深邃与难以理解，大多数人都明白也理解的了；我们都是凡夫俗子不是圣人，贪图便宜是人之本性，不能以太高的道德标准去要求普通大众，至于科技极限，任何事物都有极限，科技也难以置身事外，在人类社会和自然界，停滞是普遍存在的，甚至可以说是事物发展的一个必然规律，只要你去细心观察，停滞现象是一个广泛的存在。那么我们每个人仅仅知道停滞就够了吗？重要的是停滞背后的原因，唯知因才有果，进而研究如何制造新条件打破这种停滞。 然而天朝子民无需担心，美国的停滞对于中国来说可能正是一个弯道超车的绝佳机会。中国借助于跨越式发展，比如跳过固定电话，直接进入无线通信的时代，而美国当年在固定电话上进行了大量的投入与普及工作，使得我们与世界领头羊美国的差距越来越小。除此之外，中国还将在诸多领域获得跨越式发展，例如互联网金融、全民信用制度、ATM机物理网点、电动汽车、自动驾驶等。 在这本书中，作者认为美国应该提高科学家的地位。作者说即使在美国，一名重要科学家的地位，也不如一位歌星、艺术家更能引起媒体和大众的关注，他建议美国的政府和社会，不仅要舍得拨款、在经济上满足科研人员的需要，还要在家庭、学校、媒体倡导对科学家的尊重，这难道不正是中国需要的吗？愿国人重视之。 尽管作者指出了很多问题，但在文章收尾阶段，作者对未来还是抱有极大的信心。其一有利的趋势是印度和中国在科技上的兴趣，二者除了提供可用商品和服务的廉价版本，随着时间的推移，二者还会扮演伟大创新者的角色。其二互联网可以为利润增长贡献更多，互联网使科学的学习和沟通传播的便捷程度前所未有，也以不同寻常的方式提高了科学家的产生效率。例如在coursera上，你可以学习到世界上最好的课程。其三美国也正在采取更好的激励措施、更好的监督机制或是其它工作来改善教育现状。基于这些原因，未来还会出现更多的“低垂的果实”（指一些有非常重要、容易攫取成果的物质财富）。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建系统Gradle备忘录]]></title>
    <url>%2F2016%2F11%2F03%2FBuild-system-gradle-memos%2F</url>
    <content type="text"><![CDATA[Gradle是什么？Gradle是一个集合了Maven和Ant优点的构建工具，据说要取代Maven，不置可否。 什么是projects和tasks？每一个构建都是由一个或多个projects构成的。一个project到底代表什么取决于你想用Gradle做什么。每一个project是由一个或多个tasks构成的，一个task代表一些更加细化的构建。可能是编译一些classes，创建一个JAR，生成javadoc或者生成某个目录的压缩文件。 经常用的gradle -q，其中-q是干什么的？-q代表quiet模式，它不会生成Gradle的日志信息(log messages)，所以用户只能看到tasks的输出，它使得输出更加清晰。 如何定义一个task？在build.gradle中 12345task hello &#123; doLast &#123; println("hello world!") &#125;&#125; 其中task名称叫hello，hello的action叫doLast。 如何快捷的定义一个task？ 123task hello &lt;&lt; &#123; println("hello world!")&#125; 其中doLast被替换成了&lt;&lt; 任务之间如何相互依赖？ 123456task first &lt;&lt; &#123; println("first!")&#125;task after(dependsOn: first) &lt;&lt; &#123; println("after!")&#125; gradle -q after输出： 12first!after! 动态的创建任务 1234564.times &#123; counter -&gt; task "task$counter" &lt;&lt; &#123; println("I'm task number $counter!") &#125;&#125; 这里动态的创建了task0，task1，task2，task3gradle -q task3输出：I&#39;m task number 3!如果使用task0.dependsOn task2, task3指定任务依赖关系，那么gradle -q task0输出： 123I'm task number 2!I'm task number 3!I'm task number 0! 短标记法有一个短标记$可以访问一个存在的任务，也就是说每个任务都可以作为构建脚本的属性。 123456task first &lt;&lt; &#123; println("first!")&#125;task after &lt;&lt; &#123; println("$first.name")&#125; gradle -q after输出：first其中name是任务的默认属性，代表当前任务的名称。 自定义任务属性 123456task first &#123; ext.myProperty = "value"&#125;task after &lt;&lt; &#123; println(first.myProperty)&#125; gradle -q after输出：value 默认任务 1234567defaultTasks 'validate', 'run'task validate &lt;&lt; &#123; println("Default Validating")&#125;task run &lt;&lt; &#123; println("Default Running")&#125; gradle -q输出： 12Default ValidatingDefault Running 列出可以执行的任务使用gradle tasks来列出项目的所有任务 Java插件下常用任务使用apply plugin: &#39;java&#39;之后，Java插件会向你的项目里加入许多任务，常用的如下build任务，它会建立你的项目；clean任务，删除build生成的目录和所有生成的文件；assemble任务，编译并打包你的代码，但是并不运行单元测试。其他插件会在这个任务里加入更多的东西。举个例子，如果你使用War插件，这个任务将根据你的项目生成一个 WAR文件；check任务，编译并测试你的代码。其他的插件会加入更多的检查步骤。举个例子，如果你使用checkstyle插件，这个任务将会运行Checkstyle来检查你的代码。 查看哪些属性是可用的使用gradle properties命令来列出项目的所有属性。 发布Jar文件 1234567uploadArchives &#123; repositories &#123; flatDir &#123; dirs 'repos' &#125; &#125;&#125; 运行gradle uploadArchives命令来发布Jar文件。 多项目构建目录结构为 12345multiproject/ api/ services/webservice/ services/shared/ shared/ 则在项目根目录下的settings.gradle中添加include &quot;shared&quot;, &quot;api&quot;, &quot;services:webservice&quot;, &quot;services:shared&quot; 多项目构建-项目之间的依赖假设api项目的构建需要用到shared项目的Jar文件，那么在api/build.gradle中加入 123dependencies &#123; compile project(':shared')&#125; 依赖配置compile：用来编译项目源代码的依赖。runtime：在运行时被生成的类使用的依赖。默认的，也包含了编译时的依赖。testCompile：编译测试代码的依赖。默认的，包含生成的类运行所需的依赖和编译源代码的依赖。testRuntime：运行测试所需要的依赖。默认的，包含上面三个依赖。 构建一个WAR文件在build.gradle中加入apply plugin: &#39;war&#39;。这个插件也会在你的项目里加入Java插件，运行gradle build将会编译，测试和创建项目的WAR文件。 运行Web应用在build.gradle中加入apply plugin: &#39;jetty&#39;。由于Jetty plugin继承自War plugin。使用gradle jettyRun命令将会把你的工程启动部署到jetty容器中。调用gradle jettyRunWar命令会打包并启动部署到jetty容器中。 多任务调用你可以以列表的形式在命令行中一次调用多个任务，它们所依赖的任务也会被调用。这些任务只会被调用一次。 123456task last &lt;&lt; &#123; println("LAST")&#125;task first(dependsOn: last) &lt;&lt; &#123; println("FIRST")&#125; gradle -q first last first和gradle -q first命令的输出结果是一样的： 12LASTFIRST 排除任务你可以用命令行选项-x来排除某些任务，例如gradle dist -x test命令会排除test任务。即使test任务是dist任务的依赖，test也会被排除，同时test任务的依赖任务也都会被排除，而被test和其他任务同时依赖的任务则不会被排除。 失败后继续执行构建默认情况下，只要有任务调用失败，Gradle就会中断执行。这可能会使调用过程更快，但那些后面隐藏的错误就没有办法发现了。所以你可以使用--continue选项在一次调用中尽可能多的发现所有问题。采用--continue选项，Gralde会调用每一个任务以及它们依赖的任务。而不是一旦出现错误就会中断执行，所有错误信息都会在最后被列出来。但是一旦某个任务执行失败，那么所有依赖于该任务的子任务都不会被调用。 选择执行构建调用gradle命令时，默认情况下总是会构建当前目录下的文件，可以使用-b参数选择其他目录的构建文件，并且当你使用此参数时settings.gradle将不会生效。-b参数用以指定脚本具体所在位置，格式为dirpwd/build.gradle，-p参数用以指定脚本目录即可。 项目列表执行gradle projects命令会为你列出子项目名称列表。 任务列表执行gradle tasks命令会列出项目中所有任务。当然你也可以用--all参数来收集更多任务信息。这会列出项目中所有任务以及任务之间的依赖关系。 获取任务具体信息执行gradle help --task someTask可以显示指定任务的详细信息，或者多项目构建中相同任务名称的所有任务的信息。 获取依赖列表执行gradle dependencies命令会列出项目的依赖列表，所有依赖会根据任务区分，以树型结构展示出来。还可以通过--configuration参数来查看指定构建任务的依赖情况。例如gradle -q api:dependencies --configuration testCompile。 查看特定依赖执行gradle dependencyInsight命令可以查看指定的依赖。例如gradle -q webapp:dependencyInsight --dependency groovy --configuration compile。 获取项目属性列表执行gradle properties可以获取项目所有属性列表。 构建日志--profile参数可以收集一些构建期间的信息并保存到build/reports/profile目录下。并且会以构建时间命名这些文件。 Gradle图形界面为了辅助传统的命令行交互，Gradle还提供了一个图形界面。我们可以使用Gradle命令中--gui选项来启动它。注意：这个命令执行后会使得命令行一直处于封锁状态，直到我们关闭图形界面。如果是在*nix系统下，则可以通过加上“&amp;”让它在后台执行：gradle --gui&amp;，此命令对Windows系统无效。 标准项目属性Project对象提供了一些常用的标准属性如下 Name Type Default Value project Project Project 实例对象 name String 项目目录的名称 path String 项目的绝对路径 description String 项目描述 projectDir File 包含构建脚本的目录 build File projectDir/build group Object 未具体说明 version Object 未具体说明 ant AntBuilder Ant实例对象 声明变量在Gradle构建脚本中有两种类型的变量可以声明：局部变量(local)和扩展属性(extra)。局部变量使用关键字def来声明，其只在声明它的地方可见，扩展属性使用ext声明，例如ext.purpose = null，另外，使用ext扩展块可以以此添加多个属性。 1234ext &#123; springVersion = "5.0.0" emailNotification = "build@master.org"&#125; Java插件-依赖配置 名称 扩展 被使用时运行的任务 含义 compile - compileJava 编译时的依赖 runtime compile - 运行时的依赖 testCompile compile compileTestJava 编译测试所需的额外依赖 testRuntime runtime test 仅供运行测试的额外依赖 archives - uploadArchives 项目产生的信息单元（如:jar包） default runtime - 使用其他项目的默认依赖项，包括该项目产生的信息单元以及依赖 gradle利用maven的本地仓库有的人是从maven转向gradle，那么这时maven已经在本地仓库中缓存了大量的jar文件，如何让gradle利用maven的本地仓库呢？首先在build.gradle配置 123repositories &#123; mavenLocal()&#125; 这时，gradle会查找maven的本地仓库地址，查找顺序如下，可以参考在线文档 12341. The value of system property 'maven.repo.local' if set;2. The value of element &lt;localRepository&gt; of ~/.m2/settings.xml if this file exists and element is set;3. The value of element &lt;localRepository&gt; of $M2_HOME/conf/settings.xml (where $M2_HOME is the value of the environment variable with that name) if this file exists and element is set;4. The path ~/.m2/repository. 例如我的maven配置文件位置在D:/apache-maven-3.3.3/conf/settings.xml，并且指定了&lt;localRepository&gt;D:/apache-maven-3.3.3/repo&lt;/localRepository&gt;那么gradle就会找到该位置 IDEA中如何设置gradle的本地仓库地址在IDEA中打开Settings || Build, Execution, Deployment || Build Tools || Gradle，修改Service directory path，该值为Gradle的默认缓存目录中，比如填D:/apache-maven-3.3.3/repo，这样gradle就会将jar下载到该路径下，例如我的IDEA中的gradle下载的jar地址为：D:/apache-maven-3.3.3/repo/caches/modules-2/files-2.1，这样就可以将maven的本地仓库和gradle的本地仓库合二为一了。顺便在Gradle VM options中填-Dfile.encoding=UTF-8可以解决一些中文乱码问题。 前提是IDEA中的maven已经配置好了，同样在Settings || Build, Execution, Deployment || Build Tools || Maven中配置 123Maven home directory: D:/apache-maven-3.3.3User settings file: D:/apache-maven-3.3.3/conf/settings.xmlLocal repository: D:/apache-maven-3.3.3/repo]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《财务自由之路》]]></title>
    <url>%2F2016%2F10%2F29%2FRead-the-road-to-financial-freedom%2F</url>
    <content type="text"><![CDATA[作者简介：博多•舍费尔。1960年9月10日出生在德国科隆，是德国著名的投资家、企业家、演说家以及畅销书作家。从小，博多便目睹了贫穷给人们带来的种种伤害，这让他厌恶贫穷。6岁时，他决心要在30岁时成为百万富翁。16岁时，博多只身远赴美国闯荡。26岁时，他陷入了严重的个人财务危机。然而，凭借坚强的意志和正确的投资理念，博多在30岁时摆脱了债务，获得了成功——这时，凭借个人资产产生的利息，他就可以过上富有的生活。博多决心把他的理财知识传播给更多的人。他开始频频参加电视访谈节目，在世界各地举办讲座并著书立说。他的《赢家法则》成为德国最佳畅销书；《小狗钱钱》全球销量逾千万册，也已在中国出版，并受到广大中国读者的喜爱。 创造奇迹的5个层次 第一个层次：你对现状严重不满。因此，你决定采取行动 第二个层次：期望的结果没有出现，你开始认识到一般意义的行动还不够。你必须采取更有针对性的行动，以解决问题 第三个层次：技巧已经帮助你取得了一定的进步。但是，当你与其他成功人士比较时，你会发现他们取得成就好像比你更容易。他们可能有一些有影响力的朋友帮他们打开了门路 第四个层次：多想一想你自己以及别人，重新考虑你的世界观。很多人凭直觉就把这个美好的世界想象成丛林，认为你需要通过尖利的爪牙才能生存 第五个层次：通过改变对自身的认知，我们可以带来巨大的改变 记住，不要认为学校毕业就意味着学习的结束。这仅仅是开始。可惜的是，我们并未将这一知识推广到生活的其他领域，否则我们不会如此短视。如果做别人都在做的事，你的价值就像沧海一粟，你的收获和大家不会有区别。如果你是某方面的专家，客户就会自己找上门来，所以，关键不是比别人好，而是要和别人不一样。一个人的专家地位只能由自己来创造，如果你想生活有所改变，就必须从你忙碌的工作中安排出时间。留出一些时间来打造你的专家地位，通过持续学习来做好准备，选定目标领域，并锲而不舍地寻找通往目标领域的道路。 收入并不代表财富，挣大钱并不意味着富有，通常，我们的生活水平随着收入的增长而提高。我们的“需求”不断增长，而需求增长总是和收入增长保持同步。只有依靠资本——而不是工作——生活时，你才算拥有财富。你挣钱并不能使你富有，能存下钱才能使你富有。在积累到足够的资本、可以靠利息生活之前不要停止增加收入。 税收体制本应创造一种平等的机会，然而，众所周知，我们的税收体制只对精英和专业人士有好处。有钱人可以聘请好的顾问，而好顾问了解法律的漏洞。通常，只有积累了一定数额的金钱，人们才会对高利率感兴趣。在此之前，复利的魔力与他们无缘。好顾问要收费，合法的纳税方案要收费，优秀的税务会计师也要收费。但你因此节省的钱是支付给他们的许多倍。有钱又有好顾问辅佐的人，能够从投资中获得12~30%的年收益率——既合法又免税！钱少又不懂理财的人，每年只能得到2.5~7.5%的收益率。即使这点微博的收益，税务局和通货膨胀还要来蚕食。 如今，只有为别人创造大量财富的人才能致富。想要致富的人要创造别人需要的就业机会、产品或服务。放眼社会，如果你想要获得大量的财富，要么让别人致富（例如淘宝）、要么服务于别人（例如滴滴出行、摩拜单车），这不仅仅是一个思维方式更应该是一个行动格言。 股市投资的十条黄金法则 股市涨跌交替转换 放长线，钓大鱼 至少买5只不同的股票，但不要超过10只 卖掉股票之前，盈亏没有定论 利润来自股价上涨和红利 崩盘给你低价吃进的机会 不要从众 抓住时机、理性决策，避免感情用事 以自有资本投资，决不可借贷入市 股票总是胜过货币，将来依然如此 实现财务自由很难吗？是的，很难。但是，如果不去努力，会更难。努力提高自己很苦，但是混吃等死更苦。雁过留声，人过留名。在这个世界上空走一遭是悲哀的，为生活的意义全力以赴才是可取的。只有这样，我们才能完成自己的使命，我们的生活才有意义。满足和惰性使一个人区别于另一个人，并最终造成人们生活的差距。我们应当永不满足。成千上万的人自甘穷困潦倒。如果你的周围都是穷人，你也会沉沦，然后在不停的抱怨中了此残生。仅仅阅读此书不能使你致富。你必须付诸行动，而且越快越好。最重要的是：你必须创造一种督促自己成功的环境。平凡的生活使大多数人迅速湮没在人群之中。“知识就是力量”这句话其实并不确切。正确的说法应当是：“应用知识就是力量”。 大多数人高估了他们在一年内能做的事情，而低估了他们在10年里能做的事情。竹子就是一个形象的例子。竹农栽下竹苗，然后用土覆盖。竹苗要在地下沉睡四年之久。竹农每天早晨给它浇水，4年里每天如此。4年后的某一天，竹苗终于破土而出。然后，它在90天的时间里疯长到20米高。在长达4年的时间里，竹农根本不知道竹苗是死是活。但他坚信竹苗终有一天会破土而出，因此坚持浇水、永不言弃。一位目光远大的思想家需要有这样的信念。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人性的嫉妒]]></title>
    <url>%2F2016%2F10%2F21%2FJealousy-of-human-nature%2F</url>
    <content type="text"><![CDATA[有感而发，之前在网上看到过类似文章，深以为然。起因是一次扣费事件，事情是这样的，新接的通知党员交纳党费的比例为：每月工资收入在3000元以下(含3000元)者，交纳月工资收入的0.5%；3000元以上至5000元(含5000元)者，交纳1%；5000元以上至10000元(含10000元)者，交纳1.5%；10000元以上者，交纳2%。如果按照这个标准，我要缴2%，对此我不讳言，一份死工资，处处受剥削，本已所剩无几，还要存钱买房，于是我发了个朋友圈论及此事。 好了，结果来了，某些熟人观点总结如下（真心的好朋友还是有的）：① 你工资高多缴点；这是什么神逻辑，工资高低是我的事，和你半毛钱关系？不论工资高低那是自己挣来的，不是天上掉下来的，他们宁愿你的钱被多扣点也不愿看到你挣钱多，这样显得和你的差距也并没有这么大了，总之如果有件事，可以让你或陌生人受益，那么他们宁愿便宜陌生人，因为毕竟陌生人和他们无有交集，眼不见心不烦，但是如果熟人受益的话，这个梗总是存在，如刺在喉 ② 跟你挣的比，这些都是小钱；我每月就这点死工资，养老金、医疗金、失业金、公积金、个税扣除四千，房租生活费扣除三千，你有没有看到我税后到手多少？不管大钱小钱那不是你能定义的，对于攒首付的来说，我觉得每一笔扣除的钱都不是小钱，你简单一句话完全无视别人的努力与付出，仅仅一句拿别人的努力结果说事，照你这逻辑，反而是没本事没能力挣钱的人理直气壮了？ ③ 你就哭穷吧，潜台词是工资那么高，还在我们面前装穷；其一我从没觉得工资高；其二我也没说我是穷光蛋，我只是表达不乐意被剥削而已；其三，燕雀安知鸿鹄之志哉，他们总是会潜意识中将你的收入套在他们所处的物价水平之上，而不晓得大上海的生活成本有多高。大上海这个房价，我工资就是翻倍，也未必买的起，换个角度，与上海这房价相比，我还真的是穷光蛋了。你心里见得别人工资高，然后一看在你所处的地方买房还挺容易啊，于是别人说没钱的时候在你的心里就变成装穷了。《庄子·秋水》有云：惠子相梁，庄子往见之，或谓惠子曰‘庄子来，欲待子相’。于是惠子恐，搜于国中，三日三夜，庄子往见之，曰：‘南方有鸟，其名鹓鶵（yuān chú，神鸟），子知之乎？夫鹓鶵发于南海而飞于北海，非梧桐不止，非练食不食，非醴泉不饮，于是鸱（chī）得腐鼠，鹓鶵过之，仰而视之曰：“吓！”今子欲以子之梁国吓我邪？’。你潜意识中拿我和你作比，却不知道我从没把你当做对手，因为你还够不了和我同台竞技的资格。 当今之世，每个人身边都有很多熟人，或是昔日同窗或是今日同事。当你取得一点点成绩在与人分享时，总会有同事会背地里八卦你：切，我还不知道他那几把刷子，狗屎运罢了。有同学会私下里嘲讽你：他上学时就是个白痴，真是瞎猫撞上死耗子了。有些熟人是不愿意承认怎么身边那个平平无奇的他突然变的比我好这么多了？提拔这么快，赚了这么多，这怎么可以？“只要你过得比我好，我就受不了！！”我就要诋毁你讽刺你，想办法拉低你，就像和美人拍合影一样，我高不了瘦不了，那我得把你拖下来！ 这样的嘲讽和诋毁根本不值得在意，他们自己不会去努力不会去付出，也看不见你的努力和付出，但见你的成绩却会不舒服，他们会劝你何必这么拼命啊，何必牺牲那么多时间去学习去充电啊，一起喝喝啤酒享受玩乐多愉快，他们心里也会自我安慰：我之所以没有取得好的成就，只是我不屑于付出和那傻逼一样的努力罢了。 人无完人，而任性、懒惰、嫉妒这人性的三大弱点相辅相生，并没有完全的独立开来，任性可以衍生固执。固执本身就是一种懒惰，是思维的懒惰，固执己见不愿换位思考，在某种程度上讨厌思考的麻烦，人的懒惰首先是思想的懒惰；固执的人总是相信自己是最正确的，认为别人的都不如自己，一旦发现别人差就会在心里强制否认别人，尽管他们自己也清楚别人在某方面比自己强，但就是不愿对自己承认和妥协，于是嫉妒心理就产生了。同样懒惰可以衍生任性，嫉妒可以衍生任性固执。 这些人不求上进，当你们本来处于同一水平而你通过自己的努力，一骑绝尘将他远远甩于身后之时，人的嫉妒之心便暴露无遗，这样的人总是存在，你很难避免，他们不求上进同样也看不得别人上进。俗话说远的崇拜，近的嫉妒；够不着的崇拜，够得着的嫉妒。对方比你强，但强不了多少，你的小心眼里会嘀咕『这孙子，看上去一点也不高大上，和我一样普通，咋就骑我头上了呢？』——你因为不服，所以会嫉妒。只有当差距大到他们难以企及之时，才会由嫉妒转为崇拜吧，所以借此督促自己，继续奋勇向前。所有的嫉妒都源于比较，嫉妒之心，人皆有之，不愿苛求别人，但如果我的朋友中有这些人，那我只能敬而远之了。我们所能做的仅仅是真心的对待身边的好友，默默的远离这些嫉妒心大于友情的人吧。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 配置Maven-javadoc-plugin]]></title>
    <url>%2F2016%2F10%2F18%2FJava-8-configure-maven-java-doc-plugin%2F</url>
    <content type="text"><![CDATA[在升级JDK至1.8之后，使用Maven-javadoc-plugin插件打包报错，[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.10.4:jar (attach-javadocs) on project 详细信息如下 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.10.4:jar (attach-javadocs) on project StatisticsReport: MavenReportException: Error while generating Javadoc:[ERROR] Exit code: 1 - D:\Multi-module-project\StatisticsReport\src\main\java\com\yuewen\statistics\report\service\db\PullData.java:29: 警告: @param 没有说明[ERROR] @param preparedStatement[ERROR] ^[ERROR] D:\Multi-module-project\StatisticsReport\src\main\java\com\yuewen\statistics\report\service\db\PullData.java:30: 警告: @param 没有说明[ERROR] @param params[ERROR] ^[ERROR] D:\Multi-module-project\StatisticsReport\src\main\java\com\yuewen\statistics\report\service\db\PullData.java:31: 警告: @return 没有说明[ERROR] @return[ERROR] ^[ERROR] D:\Multi-module-project\StatisticsReport\src\main\java\com\yuewen\statistics\report\service\db\PullData.java:32: 警告: @throws 没有说明[ERROR] @throws SQLException[ERROR] ^[ERROR] D:\Multi-module-project\StatisticsReport\src\main\java\com\yuewen\statistics\report\service\db\PullData.java:34: 警告: logFlag没有 @param[ERROR] public static ResultSet pullData(PreparedStatement preparedStatement, boolean logFlag, String… params) throws SQLException {[ERROR] ^[ERROR] D:\Multi-module-project\StatisticsReport\src\main\java\com\yuewen\statistics\report\service\db\PullData.java:51: 警告: @param 没有说明[ERROR] @param preparedStatement[ERROR] ^[ERROR] D:\Multi-module-project\StatisticsReport\src\main\java\com\yuewen\statistics\report\service\db\PullData.java:52: 警告: @param 没有说明[ERROR] @param params[ERROR] ^[ERROR] D:\Multi-module-project\StatisticsReport\src\main\java\com\yuewen\statistics\report\service\db\PullData.java:53: 警告: @return 没有说明[ERROR] * @return[ERROR] ^ 经查得知，在JDK 8中，Javadoc中添加了doclint，而这个工具的主要目的是旨在获得符合W3C HTML 4.01标准规范的HTML文档，在JDK 8中，已经无法获取如下的Javadoc，除非它满足doclint： 不能有自关闭的HTML tags，例如&lt;br/&gt;或者&lt;a id=&quot;x&quot;/&gt; 不能有未关闭的HTML tags，例如有&lt;ul&gt;而没有&lt;/ul&gt; 不能有非法的HTML end tags，例如&lt;/br&gt; 不能有非法的HTML attributes，需要符合doclint基于W3C HTML 4.01的实现 不能有重复的HTML id attribute 不能有空的HTML href attribute 不能有不正确的嵌套标题，例如类的文档说明中必须有&lt;h3&gt;而不是&lt;h4&gt; 不能有非法的HTML tags，例如List&lt;String&gt;需要用&lt;&gt;对应的实体符号 不能有损坏的@link references 不能有损坏的@param references，它们必须匹配实际的参数名称 不能有损坏的@throws references，第一个词必须是一个类名称 注意违反这些规则的话，将不会得到Javadoc的输出。 一种解决办法就是关闭doclint，如果你在Maven中运行，你需要使用additionalparam设置：1234567891011&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;disable-javadoc-doclint&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;[1.8,)&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 或者是添加到maven-javadoc-plugin中：1234567891011121314151617181920212223&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.10.4&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;$&#123;chartset.UTF8&#125;&lt;/encoding&gt; &lt;aggregate&gt;true&lt;/aggregate&gt; &lt;charset&gt;$&#123;chartset.UTF8&#125;&lt;/charset&gt; &lt;docencoding&gt;$&#123;chartset.UTF8&#125;&lt;/docencoding&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;additionalparam&gt;-Xdoclint:none&lt;/additionalparam&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《异类》]]></title>
    <url>%2F2016%2F10%2F10%2FRead-outliers-the-story-of-success%2F</url>
    <content type="text"><![CDATA[作者简介：马尔科姆·格拉德威尔（Malcolm Gladwell），被《快公司》誉为“21世纪的彼得·德鲁克，曾是《华盛顿邮报》商务科学专栏作家，目前是《纽约客》杂志专职作家。马尔科姆出生于英格兰，是牙买加人的后裔。在加拿大长大，现居纽约市。他曾经做过卫生政策和科学新闻方面的记者，其文章喜欢以小见大。马尔科姆是一个非常有创意的作家，他认为自己的文风属于一种adventure of ideas的风格，虽然大部分作品都不属于虚构，又不是围绕一个人物或者一个故事展开，但其中的故事情节却一般比较曲折，看他的文章很有adventure波澜起伏的韵味。 那些取得杰出成就的人，不仅仅靠的是天赋，更多靠的是机遇，当然机遇包括整个社会所创造的机遇以及他们的家庭、生长环境、出生年月等等诸多未可知因素。而这些取得非凡业绩的人如若选择了那些专业性极强的方面，例如绘画、演奏、作曲、体育运动等，这时候就离不开一万小时理论了，除了机遇之外，剩下的就是勤勉了，相对来说，那些有着更好机遇同时又更加勤勉的人能够更快地达到一万小时的训练水平，当然也会取得更加杰出地成就。 该书中提到了一个非常重要的实验，就是从1921年开始，刘易斯·特曼就满腔热忱地把测试智商、发现天才作为自己的终身事业。在联邦会的大力支持下，他组织了一个团队到加利福尼亚州的中小学进行测试智商的研究。挑选出最聪明、最有发展前途、智商最高的学生。可是这些人长大成人之后，有些人成了政府官员，有些人成了高级法院的法官，有些人成了图书、学术文章的出版人，有些人在商业上取得了一定成功…… 尽管他们中的大部分人生活得还不错，但缺少出类拔萃之辈，远远低于当时的预期。最令刘易斯·特曼感到尴尬，甚至感到无地自容的是：没有任何一个人获得诺贝尔奖，就连成为全美知名人士的也寥寥无几；可当年测试智商时被认定智商不够高、没有什么发展前途的威廉·肖克利和路易斯·阿尔瓦雷茨，却都获得了诺贝尔奖。 后来，刘易斯·特曼在《天才基因的研究》一书中，以一种极端失望的笔调写道:“这些人的智商和成就并没有完美地结合在一起。”有时候，获得失败教训的价值并不比获得成功经验的价值小。心理学家刘易斯·特曼智商测试的预言虽然失败了，但却给人们留下了不可小觑的启示。那就是：从总体上看，一个孩子的成功，主要并不决定于智商。 最重要的是，组成智商的记忆、观察、想象、思考、判断等因素是很重要的，但组成情商的性格力量、意志力量、情感力量、社交力量等因素至少是不可忽视的。在一定意义上可以说，对于一个人的成功，情商比智商更重要。同样地，作者举出了大量的示例，这些事例直接说明并且证实了成功还源自优势的不断积累：你出生在何时何地，你父母的职业以及你成长的环境，这三者互相作用才塑造出这个世界上独特的你。 在造成飞机失事的民族理论部分中，作者举了阿维安卡52号坠机事故作为例子，这是一个沉痛的沟通案例，在飞机因为各种原因造成燃油即将耗尽的时候，副驾却用异常平静的语气对指挥控制室说“我们正在18 000英尺的右上方，嗯，我们将再试一次。我们的燃油快用完了。”这种轻描淡写的语气根本不足以引起飞行控制室的重视，而结果也异常打脸，飞机因为燃油耗尽而坠毁。 副驾飞行员的语调并未向控制室管理员传递燃料紧急的严重信息。许多管理员接受过专门的训练，可以在各种情境下捕捉到飞行员声音中极细微的语调变化。尽管机组成员相互之间表现出对燃料问题的极大忧虑，但他们向机场传达信息的语调却是冷静而职业化的。在这次事故中，飞行员的文化、传统以及职业习惯会使飞行员不愿意声明情况紧急。如果正式报告紧急情况之后，飞行员需要写出大量的书面汇报；同时，如果发现飞行员在计算飞行油量方面疏忽大意，联邦飞行管理局就会吊销其驾驶执照。这些消极措施极大地阻碍了飞行员发出紧急呼救的信息。在这种情况下，飞行员的专业技能和荣誉感不必要地变成了决定生死命运的赌注。基于此作者进而引出了“权利距离指数”，简称PDI。权利距离指数具体而言，就是一种特定文化中重视和尊重权威的程度，我想天朝必在此列。 文中还提到了中国传统的“水稻田文化”，中国人真的是世界上最勤劳的民族了。比如法国农民到了冬天要去“冬眠”，非洲一些地方农民靠捡果子谋生，你让他去种东西，他会说世上有这么多白捡的果子，你种这些干嘛？中国农民不仅勤劳质朴善良还逆来顺受，不到活不下去的地步绝不反抗，这种特质使得我们很难发展成为欧美化的那种高度分权的民选国家。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene设置单索引文件大小之性能测试]]></title>
    <url>%2F2016%2F10%2F09%2FLucene-sets-a-single-index-file-size-of-the-performance-test%2F</url>
    <content type="text"><![CDATA[环境Operating System：Windows 7 企业版 64-bit (6.1, Build 7601) Service Pack 1Processor：Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz (8 CPUs), ~3.6GHzJava version：”1.8.0_91” Java HotSpot(TM) 64-Bit Server VM (build 25.91-b15, mixed mode)Lucene Version：V 5.5.0 合并策略Lucene使用TieredMergePolicy作为其默认合并策略，这个默认的策略是合并那些差不多大小的段，受限于每次合并时允许的段的数量。该合并策略的一些默认属性如下 12345678public static final double DEFAULT_NO_CFS_RATIO = 0.1;private int maxMergeAtOnce = 10;//在一次合并过程中最多合并十个segmentsprivate long maxMergedSegmentBytes = 5*1024*1024*1024L;//合并之后的最大段文件为5GBprivate int maxMergeAtOnceExplicit = 30;private long floorSegmentBytes = 2*1024*1024L;private double segsPerTier = 10.0;private double forceMergeDeletesPctAllowed = 10.0;private double reclaimDeletesWeight = 2.0; 这种默认的合并策略会导致单个段文件的大小达到5GB，那么如果想要控制单个段文件大小的上限要怎么办呢？追踪该类至其父类MergePolicy，继续追踪MergePolicy的所有子类，发现MergePolicy的子类包括：NoMergePolicy、MergePolicyWrapper、LogMergePolicy、TieredMergePolicy，查看LogMergePolicy发现该类有两个子类，分别为LogDocMergePolicy和LogByteSizeMergePolicy，顾名思义，前者考虑段文件的数量，后者考虑段文件的大小。 另外TieredMergePolicy和LogByteSizeMergepolicy区别在于前者可以合并不相邻的段，并且区分最多允许一次合并的段数setMaxMergeAtOnce(int v)和一层最多容许的段数setSegmentsPerTier(double v)。 至此，找到了我们需要使用的合并策略，即LogByteSizeMergePolicy。 复合索引文件复合索引文件是指，除了段信息文件，锁文件，以及删除的文件外，其他的一系列索引文件压缩一个后缀名为cfs的文件，意思就是所有的索引文件会被存储成一个单例的Directory，而非复合索引是灵活的，可以单独的访问某几个索引文件，而复合索引文件则不可以，因为其压缩成了一个文件，所以在某些场景下能够获取更高的效率，比如说，查询频繁，而不经常更新的需求，就很适合这种复合索引格式。 如果仅仅设置setUseCompoundFile(false)，那么在生成的索引中，依然会有复合索引文件并且其大小会超过setMaxMergeMB(double mb)所设置的大小，是不是很奇怪，这是为什么呢？收先查看setMaxMergeMB(double mb)Doc说明如下： org.apache.lucene.index.LogByteSizeMergePolicypublic void setMaxMergeMB(double mb)Determines the largest segment (measured by total byte size of the segment’s files, in MB) that may be merged with other segments. Small values (e.g., less than 50 MB) are best for interactive indexing, as this limits the length of pauses while indexing to a few seconds. Larger values are best for batched indexing and speedier searches.Note that setMaxMergeDocs is also used to check whether a segment is too large for merging (it’s either or). 该函数用于控制可用于合并的最大的段文件大小，通常合并之后的段文件的大小均会超过此值。所以单靠此函数不能实现控制单索引文件上限的目标。 继续查看setUseCompoundFile(false)Doc说明如下： org.apache.lucene.index.LiveIndexWriterConfigpublic LiveIndexWriterConfig setUseCompoundFile(boolean useCompoundFile)Sets if the IndexWriter should pack newly written segments in a compound file. Default is true.Use false for batch indexing with very large ram buffer settings.Note: To control compound file usage during segment merges see MergePolicy.setNoCFSRatio(double) and MergePolicy.setMaxCFSSegmentSizeMB(double). This setting only applies to newly created segments. 其中提到了MergePolicy.setNoCFSRatio(double)，继续查看该方法的Doc说明如下： org.apache.lucene.index.MergePolicypublic void setNoCFSRatio(double noCFSRatio)If a merged segment will be more than this percentage of the total size of the index, leave the segment as non-compound file even if compound file is enabled. Set to 1.0 to always use CFS regardless of merge size. 继续追踪noCFSRatio查找到了预定义的默认值DEFAULT_NO_CFS_RATIO，该默认值为1.0，而根据Doc说明，当该值为1.0的时候，总是使用复合索引文件并且忽略合并大小的设置，所以就出现了设置不使用复合索引文件但无效的情况。12protected static final double DEFAULT_NO_CFS_RATIO = 1.0;protected double noCFSRatio = DEFAULT_NO_CFS_RATIO; 那么可以通过调用logByteSizeMergePolicy.setNoCFSRatio();去更改noCFSRatio的值，但是究竟设置多少合适，不得而知。此时不妨去关注下setMaxCFSSegmentSizeMB(double)方法，既然总是会有复合索引文件，那么通过设置复合索引文件的上限一样可以实现控制单索引文件大小的目的。该方法Doc说明如下： org.apache.lucene.index.MergePolicypublic void setMaxCFSSegmentSizeMB(double v)If a merged segment will be more than this value, leave the segment as non-compound file even if compound file is enabled. Set this to Double.POSITIVE_INFINITY (default) and noCFSRatio to 1.0 to always use CFS regardless of merge size. 可知如果合并之后段文件大小会超过此值时，就保留该段文件为非复合索引文件，亦即不进行合并，所以此方法可以达到控制单索引文件上限的目标。 测试使用不同的合并策略配合三种设置，添加50000000个Document，其中每个Document包含10个Field，这些Field类型包括Int、Long、String，其值随机生成。随后根据主键搜索1000000次，统计其累计搜索耗时，其中主键为String类型数字，主键值为[0，50000000)。 合并策略 参数设置 索引文件数 索引总量 最大索引 搜索总耗时 单次搜索最大耗时 TieredMergePolicy 默认 123 7.33GB 464MB 58.2s 140ms LogByteSizeMergePolicy LogByteSizeMergePolicy lbs;lbs.setMinMergeMB(1);lbs.setMaxMergeMB(64); 239 7.77GB 127MB 127.1s 314ms LogByteSizeMergePolicy IndexWriterConfig iwc;LogByteSizeMergePolicy lbs;iwc.setUseCompoundFile(false);lbs.setMinMergeMB(1);lbs.setMaxMergeMB(64); lbs.setMaxCFSSegmentSizeMB(64); 451 7.77GB 60.8MB 152.1s 446ms 总结虽然每次运行结果时间稍有不同，但总体趋势应该是不变的。即单索引文件上限越小，则生成的索引文件数量越多，索引文件数量越多，对应的单次搜索最长时间和总搜索时间均越长。所以应根据业务需求选择适当的合并策略，在满足需求之后尽可能提高搜索性能。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《万能钥匙》]]></title>
    <url>%2F2016%2F10%2F07%2FRead-the-master-key-system%2F</url>
    <content type="text"><![CDATA[花半秒钟就看透事物本质的人，和花一辈子都看不清事物本质的人，注定是截然不同的命运。——《教父》 简介：《万能钥匙》（The Master Key System）是一本奇书，由作者在1912年创作出版，当时销售了20多万册，1933年起，它奇迹般的从市场上消失了，原因是很多成功的商人不愿意让更多的人看到此书，而担心更多人因此书而创富。这些商人联合起来，让美国教会查禁了这本书。 这本上是硅谷最神奇的成功奇书，早年，这本书的手抄本甚至炒到3000多美金，几乎所有硅谷的亿万富翁都阅读过此书，世界首富比尔·盖茨在哈佛大学上学时，因为偶然的机会读到此书，立即决定弃学从商，从而创造了软件帝国的神话。美国成功学之父拿破仑·希尔因为得到本书的启示成为成功学大师。 70年间，在美国硅谷有一个人尽皆知的秘密，那就是：每个硅谷创业者都是通过研习汉尼尔先生80年前笔耕的结晶，才掘到了自己的财富。从硅谷起家的百万富翁到亿万富翁，几乎每个人都读过这本《万能钥匙》。由于此书被列为禁书，直到近年才得以重见天日，因此一度在硅谷掀起了一股秘密复印该书的热潮。 不管是曾经的李笑来还是和菜头，他们都是中国最早一批对比特币有了解的人，但是李笑来付出了实际行动，大量囤积比特币，并藉此实现了财富自由。反观和菜头呢？虽然看到了这个点，但是并未有任何切实的行动，你或许会说，人家才不在乎那些钱呢，呵呵，注意这可是实现财务自由的钱哦，没人会不在乎的。我觉得最根本的问题还是因为一切财富都基于认知，你的认知水准决定了你的行动指南。李笑来在他的书籍《把时间当作朋友》中也谈到了这个问题，一个人的心智或者说认知水准决定了你的高度。 为什么有的人能毫不费力地实现自己的雄心壮志，有人却要付出百倍的艰辛，还有一些人则一败涂地？原因必定不在于人的体魄，否则，那些伟人们一定是体格最健壮的人了。差异必定是精神上的——必定在于人的心智。物理或者身体上所创造出的结果，其差距有限；只有心智上所创造的结果才会导致无限的差距，这也正是脑力劳动者攫取的财富永远高于体力劳动者的必然所在。 如果你不打算做一件事情，那就别开始；如果你开始了，即便天塌下来也要把它做成。如果你决定做某事，那就动手去做；不要受任何人、任何事的干扰。成功的秘诀有时很简单，就是重复并且坚持，不断的重复，不断的坚持，最终必有所获。但是大多数人都败在了坚持上，就以我去健身房为例，每周一三五只要没什么意外耽搁，就去健身房，在器械区经常会看到些新面孔，而老面孔寥寥无几，可见不论是什么事情，能够坚持下来的总是少数，所以成功人士也总是少数，大多数的人都是三天打鱼两天晒网罢了。 培养全神贯注的一个小方法：为了培养全神贯注的能力，取一张照片，然后再回到以前那个房间，用同样的姿势坐在那个座位上。仔细观察这张照片，至少要十分钟，注意眼神的表情，面部特征，衣着装扮，发型样式等。也就是说，注意照片上的每一个细节。然后，盖住这张照片，闭上眼睛，试着用心灵观看这张照片，如果你能够把每一个细节看得非常清楚，在脑海中出现了这张照片非常清晰的图景，那么就要恭喜你了。如果不能，那就重复这个过程，直到成功为止。 我们付出的越多，得到的就越多。一个想要让身体更加强壮的运动员会花费更多的力气去锻炼，他越是苦练，就越是强健。一个希望财富不断累积的金融家，首先必须抛掷大量的金钱，因为只有使用这些钱财，钱财才能给他带来回报。无论何时，不劳而获都是不可取的，另外使用不合法的或者损害他人利益的手段的获益者都是应当被唾弃的。即便这个社会存在着很大的不公，但是就付出与回报而言，还是相对公平的，总的来说，如果你的朋友中有不愿付出，却奢望不菲的回报或者说当回报没达到他的期望进而满腹牢骚的人的话，那么可以敬而远之了。 人生一切苦难无非有两种：肉身病痛与精神焦虑。这些往往可以溯源到某些违反自然法则的行为。这种违背，毫无疑问，是由迄今为止的知识局限性所造成的。然而，过往年代中积聚的阴云即将散去，随之消逝的是由于信息不完备而导致的种种悲苦境遇。在肉身病痛方面，人的身体千万不能逆自然而行，比如你在家吃饭肯定比在外面吃地沟油健康，再比如人类并没有犬齿，从生物进化角度来说，人就不是纯肉食性动物，人是杂食性动物，所以尽力保持吃蔬菜的量超过肉类的量，一定对你身体有益。顺自然而行就像这个社会一样，社会中的我们一定要懂得顺势而为，顺应整个社会的发展趋势，如果你的工资很高，想想是你是否真能创造这么大的价值？抑或仅仅是你在这个社会的大趋势下所得到的额外附加值比别人多而已。至于精神层面，我觉得更多的要靠自己去调节，不断强化正面信息给大脑以愉悦的刺激，不断背弃负面信息，将他们收进思维的小黑屋中，常置角落。 据说，人每隔7年就会全部更新一次，而当今又有一些科学家指出，事实上我们每隔11个月整个人体都会重塑一次。所以，我们只有11个月的年龄。如果我们年复一年地把缺陷植入我们的身体，那可就怪不得别人了，只能从我们自己身上找原因。人是有主观能动性的，外界诸多条件我们无力改变，我们能够做到的仅仅是改变自己而已。不断激励自己，始终保持着一种积极昂扬向上的心态，不断重塑自己以最终获得最好的自己。所有的成功，都是通过把意念恒久地集中于看得见的目标而实现的。 很多人的努力漫无方向，浪费了大量的时间、想法、精力，如果用来朝着愿景中的某些特定的目标努力的话，可能会创造出奇迹。为了做到这一点，你必须集中你的精神能量，定位在某一特定的想法上，排除一切杂念的干扰。如果你曾经观察过照相机的镜头，你就会知道假如不对准镜头，物体产生的影像就会模糊不清，而当你调整好焦距，图像就会变得清晰明朗起来。这说明了集中精神所具有的力量。如果你不能把精力集中在你所期待的目标上，你只能得到一个朦胧暧昧、模糊不清的理想轮廓，其结果与你的精神图景相一致。 永远不要对环境表示不满。你越是将思想集中于负面的环境，这种环境就越是成长壮大，最终成为成功和幸福的绊脚石。面对生活，你要积极，积极，再积极。让你的思想明朗、清晰、坚实、确定，决不变更！在天朝，有能力改变的就改变之，这是最好的结果了，没有能力改变的就闭嘴，因为网络上的各种信息已经可以不经同意就被公安机关取证了，万不可再随意发泄一吐心中的愤懑了。 一个人不应该成为运气的玩偶，而应是命运的宠儿。而做到这一点的惟一前提就是拥有知识。当你认识了越来越多的真理，未来便会在你面前铺出光明之路。其实不仅社会是不平等的，而且人生下来就是不平等的，基于这种天生的不平等状态还蕴含着诸多不平等，比如运气，不要总是感叹某人运气好，更多的要去探寻背后的缘由，为什么好运气如此青睐他人而不是自己呢？为什么机会总是被他人抓住而不是自己呢？可能好运降临而你视而不见，更可能机会就在你眼前，而你却不懂得把握罢了。我们应该像船长控制他的船舰，像火车司机驾驶火车一样，牢牢地掌握运气和机遇。 懂得去思考，便认识到了自己的力量。你要明白，如果不愿“劳心”，就不得不去“劳力”。正所谓，想的越少，干的就越多，收获反而越小。这个世界，没有上帝，你能依靠的就是自己，你的“天国”就在自己心中。正所谓谋定而后动，假设做成一件事的成本是一样的，而脑力劳动和体力劳动在单位成本上是对等的，那么这就像零和博弈一样，你所付出的脑力成本越小，必然就要付出越大的体力成本。然而现实世界中，脑力劳动所创造的价值远远超过体力劳动，所以我们要不断地督促自己多动脑，进而促使脑力成本在完成一件事的整个成本中所占的比例越来越大。 知识是无价的，通过对知识的运用我们可以去实现自己理想的未来。当我们认识到自己目前的性情、境遇、力量以及健康状况都是过去思维方式的结果时，就能更好地领会知识的价值。如果我们的健康状况并不理想，那我们首先应该反省一下我们的思维方式是否有问题。我们不要忘记，一切的思想都会在心灵中留下印记。而所有的印记都是一颗种子，沉入到潜意识的土壤中，形成某种倾向。这种倾向就会把类似的想法吸引过来，种子就这样慢慢长大，甚至在我们意识到它以前就已经丰收在望了。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 目录结构和功能模块]]></title>
    <url>%2F2016%2F09%2F27%2FLucene-6-0-directory-structure-and-functional-module%2F</url>
    <content type="text"><![CDATA[Lucene英文目录结构和功能模块 core: Lucene core library analyzers analyzers-common: Analyzers for indexing content in different languages and domains. analyzers-icu: Analysis integration with ICU (International Components for Unicode). analyzers-kuromoji: Japanese Morphological Analyzer analyzers-morfologik: Analyzer for dictionary stemming, built-in Polish dictionary analyzers-phonetic: Analyzer for indexing phonetic signatures (for sounds-alike search) analyzers-smartcn: Analyzer for indexing Chinese analyzers-stempel: Analyzer for indexing Polish analyzers-uima: Analysis integration with Apache UIMA backward-codecs: Codecs for older versions of Lucene. benchmark: System for benchmarking Lucene classification: Classification module for Lucene codecs: Lucene codecs and postings formats. demo: Simple example code expressions: Dynamically computed values to sort/facet/search on based on a pluggable grammar. facet: Faceted indexing and search capabilities grouping: Collectors for grouping search results. highlighter: Highlights search keywords in results join: Index-time and Query-time joins for normalized content memory: Single-document in-memory index implementation misc: Index tools and other miscellaneous code queries: Filters and Queries that add to core Lucene queryparser: Query parsers and parsing framework replicator: Files replication utility sandbox: Various third party contributions and new ideas spatial: Geospatial search spatial3d: 3D spatial planar geometry APIs spatial-extras: Geospatial search suggest: Auto-suggest and Spellchecking support test-framework: Framework for testing Lucene-based applications Lucene中文目录结构和功能模块 core: Lucene核心类库 analyzers analyzers-common: 不同语言和领域的内容索引分析器 analyzers-icu: 集成ICU的分析器 analyzers-kuromoji: 日文分析器 analyzers-morfologik: 字典词干分析器，内建的波兰语字典 analyzers-phonetic: 索引语音特征分析器（用于类声音搜索） analyzers-smartcn: 索引中文分析器 analyzers-stempel: 索引波兰语分析器 analyzers-uima: 集成Apache UIMA的分析器 backward-codecs: Lucene旧版本的编解码器 benchmark: Lucene系统基准测试 classification: Lucene分类器模块 codecs: Lucene编解码器和postings格式 demo: 简单代码示例 expressions: 基于可插拔语法的一个动态计算的值进行sort/facet/search facet: 切面索引和搜索功能 grouping: 分组搜索结果收集器 highlighter: 高亮搜索结果中的关键词 join: 标准化内容时的索引和搜索连接 memory: 单文档内存索引实现 misc: 索引工具和其它杂项的代码 queries: 加入Lucene核心的过滤器和查询器 queryparser: 查询解析器和解析框架 replicator: 文件复制工具 sandbox: 各种第三方贡献和新的想法 spatial: 地理空间搜索 spatial3d: 3D空间平面几何的APIs spatial-extras: 地理空间搜索 suggest: 自动推荐和拼写检查 test-framework: 基于Lucene的应用测试框架 Notes：英文水平有限，翻译若有不妥之处，还请见谅。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员理财之道]]></title>
    <url>%2F2016%2F09%2F14%2FProgrammers-financial-management%2F</url>
    <content type="text"><![CDATA[为什么要理财关于这方面的废话我就不多说了，如果你资产低于千万，必然难以通过境外投资达到增保值的目的，亦很难通过多处房产达到增保值的目的，对于我们这些工薪狗仅仅几万块的资产真的是啥也做不了，每月工资在被强奸之后（险、金、税），到手已所剩无几，但也不能坐等让通胀把我们的辛苦钱统统吃掉，所以会点投资理财才是当前可行之道。 投资渠道至于银行的理财产品和某宝的收益率就不多说了，扣掉CPI基本收益无几，而且我天朝公布的CPI数据一项注水，实际通胀率应该远在公布的CPI之上，所以你的投资收益率必须要跑赢通胀才能达到资金保值甚至增值的目的。其它渠道要么是风险低收益低，要么是风险高收益高，还有就是那专坑散户的股票市场，所以可投的渠道其实并不多，但是对于资金有限的工薪阶层来说已经够了，我目前投资的有黄金、P2P、基金。 我的理财陆金所这个不多说，有平安背书，常年盘踞各大P2P平台第一名，收益率在4-9个点左右。具体投资项目，请君自选。如您有意，可通过下面链接跳转注册，平安陆金所新人注册可领100投资券（可抵本金使用），7天有效。&lt;新手专区&gt;首投零活宝-芝麻开花理财计划，额外奖励最高960投资券，快去看看，点我注册。 人人贷人人贷，唯一上市的P2P投资平台，靠谱。收益率在5-10个点左右，定投期限不同，任君选购。由民生银行资金存管，中国互联网协会AAA级信用平台，新人红包高达4999元，新手专享标年化收益9.6%，点我注册。 挖财投资挖财很简单，因为他们是国内为数不多的使用Scala语言的公司，试问哪一家要跑路的平台会使用这么强大新颖的技术，此家公司必然是有着长久的打算的，所以放心投资。收益率在6-12个点左右，新手注册就送20元红包，可抵扣本金，另外还有新手专享12%年化收益的新人宝（最高送150现金）等着你，点我注册。 招财宝蚂蚁金服旗下的蚂蚁聚宝，在上面可以买基金、黄金、定期、股票等，由阿里巴巴背书。收益还不错，就是期限有点长，一直在投姚秋博士的封闭基金。招财宝是蚂蚁聚宝上的一个子平台，该平台的主要投资品种是企业和个人直接或通过推荐机构发布的借款类产品，以及部分金融机构根据监管法律法规发行的其他产品。 存金宝黄金从年初一直涨到现在，苦于没钱投资，只能干看着别人挣钱，哎，都是泪啊。存金宝也是蚂蚁聚宝上的一个子平台，该平台主要提供黄金买卖服务。当你买入存金宝即购买了博时黄金ETF的I类份额。博时黄金ETF基金是一款投资现货黄金的ETF基金，为用户提供1元起买、买卖均0手续费的便捷服务，还给其持有者提供“买黄金生黄金”的权益。 京东金融京东金融是京东金融集团打造的“一站式”在线投融资平台，以“成为国内最值得信赖的互联网投融资平台”为使命，依托京东集团强大的资源，发挥整合和协同效应优势，将传统金融业务与互联网技术相结合，探索全新的互联网金融发展模式，致力于为个人和企业用户提供安全、高收益、定制化的金融服务，让投资理财变得简单快乐。该平台稳定安全，投资渠道种类都很多，值的推荐。 稳赚不赔最大的投资就是投资自己，投资自己包括身体上（吃、喝、健身）、学识上（学习、充电）、精神上（音乐会、舞台剧、演唱会）等，试想，万一在退休前挂了，那么所上交了半辈子的险基本上是免费贡献给国家了，这还是在没有延退的情况下。所以人生苦短，没有经历过的还是很值得去尝试享受一番的。 声明：各位如果想注册的话，请走我的推荐链接，谢谢。以上仅是个人投资项目，仅供参考。 彩蛋在「卢冠廷BEYOND IMAGINATION LIVE 2016演唱会」上，已经年过花甲的卢冠廷重新演唱了经典的《一生所爱》，听来不禁让人潸然泪下，奉献给大家。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《大数据：正在到来的数据革命》]]></title>
    <url>%2F2016%2F08%2F18%2FRead-Big-Data-The-data-revolution-is-coming%2F</url>
    <content type="text"><![CDATA[知道可能面对的困难和痛苦，在死亡的恐惧中不断挣扎，而仍然能战胜自己，选择这条道路，才是真正的勇气。—— 《明朝那些事》 作者简介：涂子沛，知名信息管理专家，曾居美国硅谷，现任阿里巴巴副总裁。毕业于华中科技大学、中山大学和卡内基梅隆大学。赴美留学之前，曾在省、市、县几级政府的不同部门磨砺10年，做过职业程序员，担任过公安边防巡逻艇的指挥官，也从事过政府统计工作。在美期间，先后担任软件公司的数据仓库程序员、数据部门经理、数据中心主任、亚太事务总监、首席研究员等职务。除了工作、写作，还热心公益，曾任中国旅美科技协会匹兹堡分会主席，现任中国旅美科技协会副主席，上海真爱梦想公益基金会理事。著有《大数据》、《数据之巅》。 凡是属于最多数人的公共事务，却常常受到最少人的照顾，人们关怀着自己的所有，而忽视公共事务；对于公共的一切，他至多只留心到其中和他个人有些相关的事务。千百年来，人的自私和有限，并没有改变。 专制者将一个国家大部分人的一部分权利和自由都剥夺了、抹杀了，大家都憎恶专制，但在一定时间、一定程度内，却很少有人反抗。为什么？这是因为，通过反抗，即使能推翻专制制度，其结果是所有人都受益，但出头冒尖的人却可能要付出极大的代价。人的自私自利的天性，使大部分人都选择了沉默，并把希冀的目光投向他们。 专制者当然也懂得这个道理，他们在维护专制的同时，会不遗余力地打击出头冒尖的反抗者，杀鸡儆猴，全力瓦解一切可能发生的集体行动。 这就是鼎鼎有名的“沉默的大多数”，正是因为对冒尖者的奖赏和保护不足，使得冒尖者有所顾忌，也不能全身心的投入到为大多数人谋福利的地步。在这些冒尖者为正义事业进行奋斗之时，不乏一些低素质蝇营狗苟者对这些冒尖者打击、泼冷水、嘲笑、冷嘲热讽等等，这些人其实正是这个社会的渣滓所在，因为他们不仅不能推进社会的进步，同时还是社会进步的绊脚石。 之前风靡全国的火车站持刀砍人事件，想必大家都有所耳闻，就几个悍匪持刀在数万人的火车站随意砍杀，却没有人能站出来阻止，这就是对“沉默的大多数”的最好证明，因为大家都不想去做冒尖者，这可能使你在与持刀悍匪的搏斗中面对生命的危险，并且一旦身亡或者受到重大的身体伤害，很难得到应有的补偿或奖励，所以大多数人都选择了坐享其成，期盼其他人可以成为冒尖者，但正是所有人都持有这种想法，自以为自己最聪明，全世界的人都是傻瓜，这应当算作中国人的一大特色吧，才使得所有的人最后都变成了沉默者。 再举一例吧，傅作义的女儿傅冬菊作为早年的中共党员，在解放战争期间，不断的为共党提供其父亲傅作义的军队部署以及作战计划，使得其父傅作义屡战屡败，最后只能固守北京城，而在此期间傅冬菊为北京的和平解放做出了重大贡献，但是政府对其奖赏与保护都不足，使得傅冬菊在文革中受到了极大的迫害，最后晚年生活亦相当凄凉，甚至文革期间，父女俩难以相见，这是整个社会的悲哀以及对人性自私自利最好的诠释，有兴趣的参见百度百科傅冬菊。 是时候做出改变了，最好的方式就是一个集体、一个社会，要建立合适的激励机制，奖励那些为共同利益作贡献的人，惩罚那些没有承担集体行动成本的“搭便车”者，从而营造关心公共利益的社会文化和运行机制。 林纳斯·托瓦兹说过，“一个人做事情的动机，可以分为三类：一是求生，二是社会生活，三是娱乐。当我们的动机上升到一个更高的阶段时，我们才会取得进步：不是仅仅为了求生，而是为了改变社会，更理想的是——为了兴趣和快乐”。放眼当下之中国，我们大多数人做事情的动机都异常简单，那就是求生，因为就我而言，三五月不工作就面临饿死的窘境，已不知从何时开始，国人的这种状态已经成为了一种常态，大多数的农民工都面临终年辛劳而所获甚少的局面，政府随便开动印钞机，涨点M2，就把你的辛劳给通胀掉了，偏偏可悲的是，不断增长的M2根本难以得到有效的控制。政府目前畸形的房价政策（靠土地出让金而存活）透支的不仅是上下三代人的积蓄，还有彻底吞噬掉中产阶级，中产阶级不仅是一个国家的支柱，更是一个国家走向繁荣富强的保证，当所有的中产阶级都为了求生而做事时，这是多大智力财富的浪费啊，如果这些人都能为了兴趣和快乐，各尽所能，其智力产出所创造的财富价值将远远超过为了求生所产生的财富价值。 大众创新，指以普通公民而不是以知识精英为主体的创新。社会创新是指为了解决某个社会问题、满足某种社会需要、改善某部分人群的生存状况，民间力量自发产生的一种新的思想、行动和举措。社会创新也指政府在公共政策、社会治理方面的创新。社会创新最著名的例子是孟加拉的乡村银行，它向贫穷的人发放不需要担保的小额贷款，以帮助穷人改善生活。其创建人Muhammad Yunus获得了2006年的诺贝尔和平奖。 回顾人类社会从古至今所有政府的历史，无论民主也好、专制也罢，所有的政府有一点是共通的：因为缺乏竞争，官僚体制与生俱来都有一种僵化保守的本性，政府机关也往往因此固守不前。但这种僵化保守也不是一成不变的，很多时候，官僚们也需要制造新的口号、接受新的概念来“收买”民意、顺承时代、说到底，一个社会，普罗大众有没有公民精神、是否勇于承担公共职责、争与不争，才是最重要的。 网络外部性是理解现代经济的一个重要概念。它是指某件产品对单个消费者的使用价值，取决于这件产品总用户的多少。用户的人数越多、形成一个越大的网络，每个用户从产品中得到的效用就越高。电话、即时通讯软件、社交网站等等都是网络外部性的典型例子。 信息时代的竞争，不是劳动生产率的竞争，而是知识生产率的竞争。数据，是信息的载体、是知识的源泉，当然也就可以创造价值和利润，可以预见，基于知识的竞争，将集中表现为基于数据的竞争，这种数据竞争，将成为经济发展的必然。 读完此书深感忧虑，美国的数据开放程度远超中国，而这才是社会民主文明平等进步的基石，中国即便从现在开始努力追赶，亦难以企及。美国政府定期向公民公布各行各业的一系列数据资源，包括何人何地何时因何事访问白宫，与总统会谈时间等，并且免费分发到互联网上，允许任何公民免费下载，还可以基于这些公开的数据，开发一系列可视化、数据分析、数据挖掘等等服务社会大众的应用程序。同时公民亦可以发动联名请愿，强制敦促政府公开一些暂时还未有公开的数据。但是天朝，用纳税人的钱收集的数据，纳税人却无权获得，即便你愿意付费获取，政府人员都不愿意搭理你，有些公务员其实就是这个社会的蛀虫，一方面他们享受着全部人民推动社会前进所带来的发展成果，另一方面又以人民的管理者的姿态处处与人民为敌，吮吸着维持社会生命运转的血液。 就像国家本身没有它自己的钱一样，国家也没有它自己的权力，但不幸的是，没有人清楚地明白这个道理。国家所有的权力都是社会授予的，或是以各种理由和借口向社会剥夺的，除了社会，国家权力没有任何其他的来源。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 5.5 开发手册]]></title>
    <url>%2F2016%2F08%2F12%2FLucene-5-5-developer-guide%2F</url>
    <content type="text"><![CDATA[Lucene和Solr的历史版本Lucene历史版本，不妨点进去看看，会发现Lucene的版本更新很频繁，所以Lucene的Doc注释比JDK的Doc注释差太多，在研读Lucene In Action的过程中，发现此书的Lucene版本为3.0，而自己使用的Lucene版本是5.5，所以会有诸多冲突之处，现聊记之，以备查用。另外附上Solr历史版本。 在学习Lucene过程中，官方推荐的Lucene索引查看工具是Luke，下载地址点我。 Lucene API变动相关 Field类中的枚举Index已被废弃，转而采用FieldType，并通过setIndexOptions方法设置索引选项 IndexWriter的optimize方法已被删除，推荐用forceMerge方法代替 对Document加权在4.x版本之后已经删除，如果想要对文档加权，需要对该文档中的每个Field都进行加权 NumericField被删除，由FieldType类中的枚举NumericType进行设置，NumericType提供了四种类型的数值，分别是INT，LONG，FLOAT，DOUBLE IndexWriter.MaxFieldLength等类似的一系列常量全被删除 IndexWriter中的isLocked方法和unlock方法全被删除，使用Directory类中的obtainLock方法替代，调用方式如下Directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)，如果已经获得锁的话，再次获取会抛异常LockObtainFailedException:lock instance already obtained IndexWriter中的setInfoStream(System.out)方法被转移到IndexWriterConfig类中，该方法用来打印Lucene操作索引的相关输出信息 BooleanQuery的构造函数被废弃，使用BooleanQuery.Builder替代之 BooleanQuery中的add()被废弃，推荐使用BooleanQuery.Builder().add()代替之，示例如下 1234TermQuery termQuery = new TermQuery(new Term("country", "Italy"));TermQuery termQuery1 = new TermQuery(new Term("city", "Venice"));BooleanQuery build = new BooleanQuery.Builder().add(termQuery,BooleanClause.Occur.MUST).add(termQuery1, BooleanClause.Occur.MUST).build(); 其中Occur有三种选项，分别是MUST表只有匹配该查询子句的文档才在考虑之列，SHOULD意味着该项只是可选项，MUST_NOT意味着搜索结果不会包含任何匹配该查询子句的文档。使用SHOULD可以实现逻辑或（OR）查询。 PhraseQuery中的构造函数和setSlop方法被废弃，推荐使用PhraseQuery build = new PhraseQuery.Builder().setSlop(2).build();代替 同样PhraseQuery中add()方法废弃，推荐使用new PhraseQuery.Builder().setSlop(2).add(term).add(term1).build()代替，同时slop中的数值代表的是term和term1中间的字符个数，不包含term和term1在内。注意slop的默认值时0 MatchAllDocsQuery(String normsField)构造函数被删除，仅保留一个无参构造 内建的TermAttribute被删除，推荐使用CharTermAttribute代替之，TermAttributeImpl被删除，推荐使用CharTermAttributeImpl代替之 PerFieldAnalyzerWrapper.addAnalyzer()方法被删除，使用构造函数初始化指定域所对应的分析器 1234Map&lt;String, Analyzer&gt; fields = new HashMap&lt;&gt;();fields.put("partnum", new KeywordAnalyzer());//对于其他的域，默认使用SimpleAnalyzer分析器，对于指定的域partnum使用KeywordAnalyzerPerFieldAnalyzerWrapper perFieldAnalyzerWrapper = new PerFieldAnalyzerWrapper(new SimpleAnalyzer(), fields); 域缓存加载所有文档中某个特定域的值到内存，便于随机存取该域值。域缓存只能用于包含单个项的域，也就是说使用域缓存的文档都必须拥有一个与指定域对应的单一域值，即域值不能被拆分。FieldCache在Lucene 5.5.0中被lucene-core模块移除，转移到了lucene-misc模块并且访问权限为包级私有，FieldCache类已经不鼓励使用，当要在某个Field上进行排序时，用户应该使用Doc值来索引相应的Field，相对于使用FieldCache更加快且耗费较少的堆大小 IndexSearcher中setDefaultFieldSortScoring()方法被移除 SpanQuery.getSpans()方法转移到SpanWeight中，在调用该方法前，需要先调用createWeight方法创建SpanWeight实例 SpanWeight中的getSpans()方法调用示例如下 1Spans spans = weight.getSpans(indexReader.getContext().leaves().get(0), SpanWeight.Postings.POSITIONS); 如果想在域的起点查询指定的跨度，需要使用SpanFirstQuery(SpanQuery match, int end)，end指定了跨度的结束位置 搜索过滤中的一系列过滤器都被废弃或删除，例如TermRangeFilter、NumericRangeFilter、QueryWrapperFilter、PrefixFilter、CachingWrapperFilter等，推荐使用相应的*Query类和BooleanClause.Occur.FILTER替代之，SpanQueryFilter、CachingSpanFilter、FieldCacheRangeFilter等被移除 FilteredDocIdSet和FilteredQuery将在Lucene 6.0中移除 MultiSearcher被删除，用MultiReader代替 getTermFreqVector被删除，用getTermVector代替 FieldSelector被删除，用StoredFieldVisitor代替，FieldSelector的子类LoadFirstFieldSelector、MapFieldSelector、SetBasedFieldSelector亦被删除 TermVectorMapper被删除，相应地其子类PositionBasedTermVectorMapper、SortedTermVectorMapper、FieldSortedTermVectorMapper亦被删除 FieldSelectorResult被删除，TermVector被废弃 Lucene 开发相关 索引数字时，需要选择一个不丢弃数字的分析器，WhitespaceAnalyzer和StandardAnalyzer可以作为候选，而SimpleAnalyzer和StopAnalyzer两个类会将语汇单元流中的数字剔除 优化索引只能提高搜索速度，而不是索引速度。注意索引优化会消耗大量的CPU和I/O资源，在优化期间，索引会占用较大的磁盘空间，大约为优化初期的3倍 对于一个索引来说，一次只能打开一个Writer，Lucene采用文件锁来提供保障，该锁只有当IndexWriter对象被关闭时才会释放 任意多个线程都可以共享同一个IndexReader类或者IndexWriter类，这些类不仅是线程安全的，而且是线程友好的，能够很好的扩展到新增线程，针对并发访问唯一的限制是不能同时打开多于一个Writer 如果选择自己实现锁机制，要确认该机制能够正确运行，可以使用简单的调试工具，LockStressTest类，该类可以与LockVerifyServer类和VerifyingLockFactory类联合使用，以确认你自己实现的锁机制能正常运行 刷新操作是用来释放被缓存的更改的，而提交操作是用来让所有的更改（包括被缓存的更改或者已经刷新的更改）在索引中保持可见 打开IndexReader需要较大的系统开销，因此必须尽可能重复使用一个IndexReader实例以用于搜索，并限制打开新IndexReader的频率 IndexSearcher实例只能对其初始化时刻所对应的索引进行搜索。如果索引操作是由多线程完成的，新编入索引的文档则不能被该searcher看到 Query的一些子类说明： 通过项进行搜索：TermQuery 在指定的项范围内搜索：TermRangeQuery 通过短语搜索：PhraseQuery 通配符查询：WildcardQuery 搜索类似项：FuzzyQuery 匹配所有文档：MatchAllDocsQuery BooleanQuery对其中包含的查询子句（调用一次add()方法可以看作一个子句）是有数量限制的，默认情况下允许包含1024个查询子句，该限制主要是基于性能方面的考虑。当子句数量超过最大值时，程序会抛出TooManyClauses异常。如果你在一些特殊情况下，需要增大查询子句的数量限制，可以调用setMaxClauseCount(int maxClauseCount)方法进行设置，但是该操作明显会对性能产生影响 举例说明PhraseQuery匹配中slop的计算方式，这里的slop是指若要按顺序组成给定的短语所需要移动位置的次数。如Amsterdam has lots of bridges这句话，假如我需要查询has和bridges，那么它们之间的slop为2，假如我需要查询bridges和has（注意两次查询添加短语的顺序不同），那么它们之间的slop为4，这个4代表的是将has移动到bridges之后需要的步数 PhraseQuery支持复合项短语。无论短语中有多少个项，slop因子都规定了按顺序移动项位置的总次数的最大值。例如 1234//待检索文本：Venice has lots of canalsPhraseQuery build = new PhraseQuery.Builder().setSlop(1).add(new Term("contents", "Venice")).add(new Term("contents", "lots")).add(new Term("contents", "canals")).build();search = indexSearcher.search(build, 10);Assert.assertNotEquals(1, search.totalHits);//移动总次数为1+1=2，所以NotEquals 在使用通配符WildcardQuery进行查询时，可能会降低系统的性能，较长的前缀可以减少用于查找匹配搜索枚举的项的个数，如以通配符为首的查询模式会强制枚举所有索引中的项以用于搜索匹配 类似查询FuzzyQuery会尽可能地枚举出一个索引中所有项，因此，最好尽量少地使用这类查询，即便要使用这类查询，起码也应当知晓其工作原理以及它对程序性能的影响 QueryParser中针对某个项的否定操作必须与至少一个非否定项的操作联合起来进行，否则程序不会返回结果文档，换句话说不能使用“NOT term”而必须使用“a AND NOT b” 查询语句中用双引号括起来的项可以用来创建一个PhraseQuery，例如查询语句”This is some phrase*“被StandardAnalyzer分析时，将被解析成用短语”some phrase”构成的PhraseQuery对象。星号不能被解释成模糊查询，请记住：双引号内的文本会促使分析器将之转换为PhraseQuery。那么说了这么多废话是什么意思呢？举例说明 1234Query query = new QueryParser("field", new StandardAnalyzer()).parse("\"This is some phrase*\"");Assert.assertEquals("analyzed", "\"? ? some phrase\"", query.toString("field"));System.out.println(query.toString("field"));//输出"? ? some phrase"，停用词This is被?代替了，而引号中的*被忽略了 分析器介绍 WhitespaceAnalyzer：顾名思义，就是通过空格来分割文本信息，而并不对生成的语汇单元进行其他的规范化处理 SimpleAnalyzer：该分析器会首先通过非字母字符来分割文本信息，然后将语汇单元统一为小写形式，需要注意的是，该分析器会去掉数字类型的字符，但会保留其他字符 StopAnalyzer：该分析器功能与SimpleAnalyzer类似，区别在于，前者会去除停用词，例如英文中的the、a等 StandardAnalyzer：这是Lucene最复杂的核心分析器。包含大量的逻辑操作来识别某些种类的语汇单元，比如公司名称、E-Mail地址以及主机名称等。它还会将语汇单元转换成小写形式，并去除停用词和标点符号，经试验，Lucene5.5中，该分析器无法切分出完整的邮箱地址 KeywordAnalyzer：将整个文本作为一个单一语汇单元处理 MultiPhraseQuery类与PhraseQuery类似，区别在于前者允许在同一位置上针对多个项的查询，当然要实现这种查询也可以使用BooleanQuery类或者逻辑”或“连接符将所有可能的短语进行联合查询，但是这样会以更高昂的系统消耗为代价 在进行多域值分析时，文档可能包含同名的多个Field实例，而Lucene在索引过程中在逻辑上将这些域的语汇单元按照顺序附加在一起，举例来说，如果一个域值是”It’s time to pay incom tax“而下一个域值是”return library books on time“，那么对于短语”tax return“的搜索将会匹配到这个域。为了解决这个问题，必须通过继承Analyzer类来创建自己的分析器，然后重载getPositionIncrementGap方法（以及tokenStream或resuableTokenStream方法）。默认情况下，getPositionIncrementGap方法会返回0（无间隙），意思是它在运行时会认为各个域值是连接在一起的，如果将这个域值增加到足够大，那么位置查询就不会错误地在各个域值边界进行匹配了。注意一点，如果程序要高亮显示这些域，那么错误的偏移量会导致程序将错误的文本部分进行高亮显示，语汇单元OffsetAttribute对象提供了几个方法来检索偏移量的起始和结束，还提供了一个特殊的endOffset方法，该方法主要用于返回域的最后偏移 另一种针对多域值的查询是使用MultiFieldQueryParser，它是QueryParser的子类，它会在后台程序中实例化一个BooleanQuery对象，并将每个域的查询结合起来。当程序向BooleanQuery添加查询子句时，默认操作符OR被用于最简单的解析方法中，也可以通过使用parse方法指定BooleanClause.Occur的值。MultiFieldQueryParser有一些使用限制，这主要是由它使用QueryParser的方法导致的，你只能使用QueryParser的一些默认设置，而不能更改之。MultiFieldQueryParser的一个重要缺陷就是它会生成更多的复杂查询，而Lucene必须对每条查询分别进行测试，这样会比使用全包含域速度更慢 多域值查询的第三种方式是使用DisjunctionMaxQuery，它会封装一个或多个任意的查询，将匹配的文档进行OR操作，一个有趣的地方是当某个文档匹配到多于一条查询时，该类会将这个文档的评分记为最高分，而与BooleanQuery相比，后者会将所有匹配的评分相加。DisjunctionMaxQuery还包含一个可选的仲裁器，因此所有处理都是平等的，一个匹配更多查询的文档能够获得更高的评分 如果希望停止较慢的搜索，那么可以使用TimeLimitingCollector，它能在耗时较长的情况下停止搜索，显然你必须针对超时搜索情况选择具体的处理内容 TimeLimitingCollector在收集搜索结果时添加一些自己的操作（比如每当获取一个搜索结果文档时都检查此时是否已超时），这会使得搜索速度变得稍慢，其次，它只在收集搜索结果时判断是否超时，然后一些查询有可能在Query.rewrite()操作期间消耗较长时间。对于这类查询来说，程序可能只有在搜索超时时才能捕获TimeExceededException异常 Lucene通过创建抽象基类FieldComparatorSource的子类来实现自定义排序 Lucene所有的核心搜索方法都在后台使用Collector子类来完成搜索结果收集。例如，当通过相关性进行排序时，后台会使用TopScoreDocCollector类，当通过域进行排序时，则使用TopFieldCollector Lucene索引前缀后缀规则：所谓前缀后缀规则，即当某个词和前一个词有共同的前缀的时候，后面的词仅仅保存前缀在词中的偏移（offset），以及除前缀以外的字符串（称为后缀）。例如：Term Terminal存储为Term4inal 索引差值规则：先后保存两个整数的时候，后面的整数仅仅保存和前面整数的差值即可，例如：5,9,11存储为|5|4(9-5)|3(11-9)| 关于Directory子类的选择问题，最简单的方式是使用public static FSDirectory open(Path path)，让Lucene来替你尽力选择最适合当前硬件环境的实现 Lucene事务相关 通过IndexWriter.getReader()获得的Reader是能看到上次commit之后，IndexWriter执行至当前的所有变化，该方法提供一种”near real-time”近实时的搜索 当IndexWriter正在做更改的时候，所有更改都不会对当前搜索该索引的IndexReader可见，直到你commit或者打开了一个新的NRT(near real-time)Reader。一次只能有一个IndexWriter实例对索引进行更改 仅在一个新的目录上打开一个IndexWriter并不会产生一个空的commit操作，即你无法在这个目录上打开一个IndexReader，直到你完成一次commit 单个Lucene索引可以保存多个commit，每次commit都持有该commit被创建的时间点的索引视图 KeepOnlyLastCommitDeletionPolicy是默认的删除机制实现，它会删除掉除了最近一次commit以外的所有commit。如果采用NoDeletionPolicy，那么每一次commit都会保存 你可以在commit的时候传送userData(Map&lt;String,String&gt;)，用于记录关于本次commit的一些用户自定义信息（对Lucene不透明），然后使用IndexReader.listCommits方法获得索引的所有commit信息。一旦你找到了一次commit，你可以在其上找开一个IndexReader对commit点上的索引数据进行搜索 你也可以基于之前的commit打开一个IndexWriter，回滚之后的所有变动。这有点类似于rollback方法，不同之处在于它允许你在多个commit间rollback，而不仅是回滚当前IndexWriter会话中所做的变动 Lucene的QueryParse表达式说明 表达式 匹配文档 java 在字段中包含java java junitjava OR junit 在字段中包含java或者junit +java +junitjava AND junit 在字段中包含java以及junit title:ant 在title字段中包含ant title:extreme-subject:sportstitle:extremeAND NOT subject:sports 在title字段中包含extreme并且在subject字段中不能包含sports (agile OR extreme) AND methodology 在字段中包含methodology并且同时包括agile或者extreme title:”junit in action” 在title字段中包含junit in action title:”junit action”~5 junit和action之间距离小于5的文档 java* 包含以java开头的，例如：javaspaces,javaserver java~ 包含和java相似的，如lava lastmodified:[1/1/04 TO 12/31/04] 在lastmodified字段中值为2004-01-01到2004-12-31中间的 +pubdate:[10100101 TO 20101231] Java AND (Lucene OR Apache) 检索2010年出版的关于Java、且内容中包含Lucene或Apache关键字的所有书籍 注意：如果查询表达式使用了以下特殊字符，你就应该对其进行转义操作，使这些字符在一般的表达式中能够发挥作用。QueryParser在各个项中使用反斜杠(\)来表示转义字符。需要进行转义的字符有：\ + - ! ( ) : ^ ] { } ~ * ? Lucene常见Field 表达式 匹配文档 IntField 主要对int类型的字段进行存储，需要注意的是如果需要对IntField进行排序使用SortField.Type.INT来比较，如果进范围查询或过滤，需要采用NumericRangeQuery.newIntRange() LongField 主要处理Long类型的字段的存储，排序使用SortField.Type.Long,如果进行范围查询或过滤利用NumericRangeQuery.newLongRange()，LongField常用来进行时间戳的排序，保存System.currentTimeMillions() FloatField 对Float类型的字段进行存储，排序采用SortField.Type.Float,范围查询采用NumericRangeQuery.newFloatRange() BinaryDocVluesField 只存储不共享值，如果需要共享值可以用SortedDocValuesField NumericDocValuesField 用于数值类型的Field的排序(预排序)，需要在要排序的field后添加一个同名的NumericDocValuesField SortedDocValuesField 用于String类型的Field的排序，需要在StringField后添加同名的SortedDocValuesField StringField 用于String类型的字段的存储，StringField是只索引不分词 TextField 对String类型的字段进行存储，TextField和StringField的不同是TextField既索引又分词 StoredField 存储Field的值，可以用IndexSearcher.doc和IndexReader.document来获取此Field和存储的值 参考文献[1] Lucene In Action (Second Edition)[2] http://3dobe.com/archives/172/[3] http://blog.mikemccandless.com/2012/03/transactional-lucene.html]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene读写NFS文件系统异常]]></title>
    <url>%2F2016%2F08%2F12%2FLucene-read-write-NFS-file-system-error%2F</url>
    <content type="text"><![CDATA[Caused by: java.lang.InternalError: a fault occurred in a recent unsafe memory access operation in compiled Java codeat org.apache.lucene.store.DataInput.readVInt(DataInput.java:134) ~[lucene-core-5.5.0.jar:5.5.0 2a228b3920a07f930f7afb6a42d0d20e184a943c - mike - 2016-02-16 15:18:34]at org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:157) ~[lucene-core-5.5.0.jar:5.5.0 2a228b3920a07f930f7afb6a42d0d20e184a943c - mike - 2016-02-16 15:18:34]at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507) ~[lucene-core-5.5.0.jar:5.5.0 2a228b3920a07f930f7afb6a42d0d20e184a943c - mike - 2016-02-16 15:18:34]at org.apache.lucene.index.LeafReader.docFreq(LeafReader.java:152) ~[lucene-core-5.5.0.jar:5.5.0 2a228b3920a07f930f7afb6a42d0d20e184a943c - mike - 2016-02-16 15:18:34]at org.apache.lucene.search.IndexSearcher.count(IndexSearcher.java:394) ~[lucene-core-5.5.0.jar:5.5.0 2a228b3920a07f930f7afb6a42d0d20e184a943c - mike - 2016-02-16 15:18:34] 通常来说，在NFS上大规模的操作总是容易出问题的。如果你使用的是NFS（Network File System）那么建议你升级NFS文件系统所在机器的JDK到Java的最新版，而不是升级你跑程序的机器，因为NFS文件系统所在的服务器很可能和你跑程序的不在同一台服务器，通常情况下都是将NFS作为一块硬盘挂载到某个服务器之上。 另外Lucene本身对NFS的支持就不够好，所以如果你服务器资源充足的话，建议在一个服务器上跑写索引进程，在另一台服务器上跑读索引进程，这样也能避免此类问题。 另外，unsafe memory access operation这个帖子中碰到了同样的问题，可以参考之。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用坚果云同步IntelliJ IDEA的配置文件]]></title>
    <url>%2F2016%2F08%2F09%2FUse-nuts-cloud-sync-IntelliJ-IDEA-configuration-files%2F</url>
    <content type="text"><![CDATA[对于IDEA这样的神器，每个人都必然会有很多个性化的配置，那么如何在多台终端同步IDEA的配置呢？配合强大的坚果云同步功能来自动同步你的配置文件吧。另外坚果云免费版虽然对流量有限制，但是同步一个小小的配置文件夹还是足够了。 此方法也适用于JetBrains家的其它IDE系列产品，稍有不同之处请自行调整。 IntelliJ IDEA，一套智慧型的Java整合开发工具，特别专注与强调程序员的开发撰写效率提升 PHPStorm，PHP集成开发工具 PyCharm，智能Python集成开发工具 RubyMine，一个为Ruby和Rails开发者准备的IDE，其带有所有开发者必须的功能，并将之紧密集成于便捷的开发环境中 WebStorm，智能HTML/CSS/JS开发工具 AppCode，开发Obj-C的IDE，是一个XCode的替代物 IDEA默认配置文件存放位置 Windows 保存在 C:/Users/username/.IntelliJIdeaXX (XX表示产品的版本号，当前版本是2016.2) Unix/Linux 保存在 ~/.IntelliJIdeaXX (~ 就是/home/目录) Mac 保存在 ~/Library/Preferences/IntelliJIdeaXX 注意：如果是 IntelliJ IDEA Community，那么文件名就是 IdeaICXX。坚果云的客户端安装就不说了，关闭IDEA，从你的默认配置目录里剪切config这个目录到你的坚果云同步目录。如果你想要同步多个工具的配置目录，那么可以为config搭配一个父目录使用。例如我的： E:/jianguoyun/IDEA/config #IDEA的配置文件路径 E:/jianguoyun/PyCharm/config #PyCharm的配置文件路径 E:/jianguoyun/PHPStorm/config #PHPStorm的配置文件路径 打开IDEA安装目录C:/Program Files (x86)/JetBrains/IntelliJ IDEA 2016.2/bin/idea.properties，找到第8行，把idea.config.path前面的#号去掉，=号后面的路径修改成之前的配置文件夹的目录，例如IDEA修改为idea.config.path=E:/jianguoyun/IDEA/config。记得把路径中的“\”换成“/”。 第二天到公司直接修改idea.properties即可，前提是同样安装了坚果云客户端哦。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>IntelliJ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《大国空巢》]]></title>
    <url>%2F2016%2F07%2F09%2FRead-a-big-country-in-an-empty-nest%2F</url>
    <content type="text"><![CDATA[一个人在科学探索的道路上，走过弯路，犯过错误，并不是坏事，更不是什么耻辱，要在实践中勇于承认和改正错误。—— 爱因斯坦 作者简介：易富贤，男，湖南洪江市（黔阳县）人。1988-1999年在中南大学湘雅医学院学习，获临床医学学士、药理学硕士、药理学博士学位。1999-2002年在美国明尼苏达大学、威斯康星医学院做博士后。2002年起为威斯康星大学妇产科“母胎医学奖学金项目”（MFM Fellowship，由“美国妇产科董事会”推进）的副研究员（Associate Scientist）。2013年晋升为研究员（Senior Scientist）。育有一女两儿。美华学社400名初始会员之一。中国旅美科技协会会员。中国改革发展研究院中国改革论坛网络专家。 作者早年学成归国，但是在国内一直不受待见，于是至今定居国外。这在天朝本不是稀奇的事情，正确的事正确的人往往无法发挥其所拥有的真正价值。这本书的主要观点包括： 主流家庭生一个孩子，这个社会必然出现老龄化等问题 人口不是社会负担，它创造社会财富 人口下降带来的问题的严重程度被低估，不执行计划生育带来的问题被夸大 现在停止计划生育政策已经有点晚了，但是国家还没有完全停止计划生育，结构性问题要纠正需要很长时间 中国目前社会想要完成世代更替，社会总生育率应该保持在2.3的水平，也就是一个家庭生育2.3个孩子 主流家庭需要生三个孩子，人口推动经济繁荣，人口决定文明的兴衰 大国空巢是一本反思计划生育的书籍，所以在当年计划生育大行其道之时一直被当做禁书对待，直到近年放开二孩政策的推行才使得此书与读者得以想见，这确实是一本好书，为什么这么说呢？因为它从另一个侧面让我知道了我们平时接受到的大多数观点并不一定是正确的，例如减少人口可以提高素质，胡扯，真正能提高人口素质的只有良好的教育；例如优生优育可以提高人口质量，胡扯，不管是优质基因还是非优质基因都应该得到传承，这才是真正的基因多样化，因为一代的非优秀基因经过几代的繁衍一样可以孕育出非常优秀的基因，这就像大多数伟大的科学家，其父辈及祖上的基因却并不都是优秀的基因；例如，降低人口数量，可以提高就业率，胡扯，你没看过世界上大多数国家的人口都不及美国人口的数量，但是个个失业率都超过美国吗？再例如，推行晚婚晚育的政策，在医学上早有定论，人在三十岁之前的基因才是质量最好的基因，如果大家都三十岁四十岁之后生孩子，经过多代繁衍，估计自己就把自己灭亡了。很多观点不能一一列举，只是想说这个世界上聪明或者有远见之人一直都在，只是没有机会发声而已，即便他们奋力呐喊，普通愚民也难以企及。 作者在书籍中早就预测到如今东北的经济衰退是必然，而其依据就是人口的流动以及出生率，近几年东北的人口一直都处于净流出状态，从这点来说，东北经济不衰退才奇怪呢。毛爷爷早就说过，人是第一最可宝贵的资源，无论任何事物，离开人都是不行的，这也是毛爷爷时代人口激增的原因。况且那个年代人口激增也不能怪毛爷爷，而主要原因是因为国人经过战乱之后平均寿命大大延长所致。换句话说，我们现代人都是享受毛爷爷的遗留成果而已，如果中国没有这么多廉价的人力资源，没有人口红利的存在，现如今想成长为全球第二大经济体，无异于痴人说梦。所以很多事情并不是在当代才会有结果体现，往往要延后数十年才能真正的认识到一些决策的优劣与否。 所有的独生子女家庭都像是在走钢丝，而失独家庭就是那些走钢丝摔下来的人，现在中国有几百万的失独家庭，预计将来失独家庭可能会达到千万级别，想一想，这一千万个家庭中可能会存在多少伟大的基因，但是都由于失独而导致绝种，伟大基因从此在世界上消失。失独不仅仅是白发人送黑发人，更是有悖人伦的人间惨剧，当这些人进入耄耋之年，整个生活失去目标，想想自己这一血脉从此在世间再无延续，而且随着年龄的增加，老无所依，老无所养，更缺少子孙去慰藉他们的内心世界，同时还会加重整个社会的养老负担。 越是最基本的权利，就越是需要公平地得到保障。比如在空气和阳光这些基本的生活资源，无论贫富基本是平等共享的。生育是最原始的本能，生育权是仅次于生存权的基本人权。一夫一妻制的建立，就是为了保障性和生育这两大基本权利在制度上平等。无论贫富，基本的生育权都需要得到保障。合理的制度是既能让高学历、高收入者“愿意生孩子”，也让低收入者“养得起孩子”。现在即便放开生育，依然没有出现爆发式的补偿性生育高峰，究其原因，并不是人们不愿生，而是生了养不起，哈哈，天朝啊。 人是生物，生物体最基本的特征是新陈代谢，这是与非生物最本质的区别。只有在新陈代谢的基础上，生物体才能进行生长、发育、应激、适应、遗传和变异等各项生命活动。中华文明之所以能够延续数千年，就是因为中华生育文化符合“生物学”原则，强调新陈代谢。而现在美国的生育文化尚处于“物理学”的水平，还没有上升到“生物学”的高度。没有人口的新陈代谢，人类文明将如同温水煮青蛙，在适应中灭亡。 最后肯定会出现读者不认同作者观点的情况，不要紧，不管你相不相信计划生育会带来巨大的社会问题，如今已经有了非常好的佐证，那就是由于穆斯林超强的生育能力，未来的世界中，穆斯林们必然是一支有着强大影响力的存在。有兴趣的话请看欧洲穆斯林化与西方的衰落，这是书籍《America Alone ：The end of the world as we know it》中的部分内容。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《黑客与画家》]]></title>
    <url>%2F2016%2F06%2F29%2FRead-Hackers-and-Painters-Big-Ideas-from-the-Computer-Age%2F</url>
    <content type="text"><![CDATA[一杯清水因滴入一滴污水而变污浊，一杯污水却不会因一滴清水的存在而变清澈。 作者简介：保罗•格雷厄姆，《黑客与画家》一书的作者，硅谷创业之父。1964年，出生于匹兹堡郊区的一个中产阶级家庭。父亲是设计核反应堆的物理学家，母亲在家照看他和他的妹妹。青少年时代，格雷厄姆就开始编程。但是，他还喜欢许多与计算机无关的东西，这在编程高手之中是很少见的。 保罗•格雷厄姆在康奈尔大学读完本科，然后在哈佛大学获得计算机科学博士学位。1995年，他创办了Viaweb，帮助个人用户在网上开店，这是世界上第一个互联网应用程序。1998年夏天，Yahoo!公司收购了Viaweb，收购价约为5000万美元。 此后，他架起了个人网站paulgraham.com，在上面撰写了许许多多关于软件和创业的文章，以深刻的见解和清晰的表达而著称，迅速引起了轰动。2005年，他身体力行，创建了风险投资公司Y Combinator(Y运算子，简称YC，Y Combinator是一个编程术语，意思是创造其它函数的函数)，将己的理论转化为实践，目前已经资助了80多家创业公司。现在，他是公认的互联网创业权威。 《黑客与画家：硅谷创业之父Paul Graham文集》主要介绍黑客即优秀程序员的爱好和动机，讨论黑客成长、黑客对世界的贡献以及编程语言和黑客工作方法等所有对计算机时代感兴趣的人的一些话题。书中的内容不但有助于了解计算机编程的本质、互联网行业的规则，还会帮助读者了解我们这个时代、迫使读者独立思考。 为什么书呆子不受欢迎“书呆子”与“高智商”有强烈的正相关关系，你越喜欢读书，就越不受别人的欢迎，因此“书呆子”和“受欢迎”之间，有一种更强烈的负相关关系。这样看来，“高智商”似乎导致了你不受欢迎。道理应该是浅显直白的，爱读书的人一般不愿意和无聊或低俗的人去讨论生活的乐趣以及人生的意义，相对地，他们更愿意花时间独处，去汲取未知世界的养分，成长自己。而人际关系又多数是和彼此之间的交流或者共同的生活交织在一起，这样就自然导致独处的人不容易受到别人的欢迎，但是记住天才都是孤独的，没人能跟得上他们思维的脚步，他们也不愿意停留，而是一直进取。这些都是我的个人见解，在本书中，作者认为书呆子不受欢迎的真正原因，是他们脑子里想着别的事情。他们的注意力都放在读书或者观察世界上面，而不是放在穿衣打扮、开晚会上面。 黑客与画家判断一个人是否具备“换位思考”的能力有一个好方法，那就是看他怎样向没有技术背景的人解释技术问题。我们大概都认识这样一些人，他们在其它方面非常聪明，但是把问题解释清楚的能力却惊人低下。软件的部分功能就是解释自身。为了写出优秀软件，你必须假定用户对你的软件基本上一无所知。基于这些原因，也导致了目前软件世界里面的主流问题，软件工程师看用户就像傻瓜一样，用户呢？看开发软件的人也像傻瓜一样，互相看不上，互相不理解。所以技术人员也要适当的学会换位思考，这样才能够开发出更符合大众水平更加流行的优秀作品。如果只能让别人记住一句关于编程的名言，那么这句名言就是《计算机程序的结构与解释》一书的卷首语：“程序写出来是给人看的，附带能在机器上运行”。 “换位思考”不仅是为了你的用户，也是为了你的读者。这对你是有利的，因为你也会读自己写的东西。许多黑客六个月后再读自己的程序，却发现根本看不懂它是怎么运行的。注意一点，把代码写得便于阅读，并不是让你塞进去很多注释。只有在那些不太成熟、容易出现问题的地方，你才应该加上注释，提醒读者注意那里，就好像公路上只有在急转弯处才会出现警示标志一样。 不能说的话要是能坐上时间机器回到过去，不管哪一个年代，有一件事都是不会改变的，那就是“祸从口出”。你一定要小心自己说的话，自以为无害的言论会给你惹来大麻烦。历史的常态似乎就是，任何一个年代的人们，都会对一些荒谬的东西深信不疑。他们的信念还很坚定，只要有人稍微表示一点怀疑，就会惹来大麻烦。我们这个时代是否有所不同？只要读过一点历史，你就知道答案几乎确定无疑，就是“没有不同”。即使有那么一丝微小的可能，有史以来第一次，我们这个时代的所有信念都是正确的，那也是处于惊人的巧合，而不是因为我们真正找到了正确的方向。我想作者应该是没有读过中国历史，否则他就会知道不仅有些话不能说，而且还不能写出来，这都要拜大清朝特有的文字狱所赐。 良好的坏习惯不少公司都想知道，什么事情可以外包（指的是聘请另一个公司来执行，而不是指把业务部门转移到海外），什么事情不可以外包。一个可能的答案是，公司内部所有不直接感受到竞争压力的部门都应该外包出去，让他们暴露在竞争压力之下。 如何创造财富从经济学观点看，你可以把创业想象成一个压缩过程，你的所有工作年份被压缩成了短短几年。你不再是低强度地工作四十年，而是以极限强度工作四年。在高技术领域，这种压缩的回报尤其丰厚，工作效率越高，额外报酬就越高。 一个大学毕业生总是想“我需要一份工作”，别人也是这么对他说的，好像变成某个组织的成员是一件多么重要的事情。更直接的表达方式应该是“你需要去做一些人们需要的东西”。公司不过是一群人在一起工作，共同做出某种人们需要的东西。真正重要的是做出人们需要的东西，而不是加入某个公司。 要致富，你需要两样东西：可测量性和可放大性。你的职位产生的业绩，应该是可测量的，否则你做得再多，也不会得到更多的报酬。此外，你还必须有可放大性，也就是说你做出的决定能够产生巨大的效应。就算你无法测量每个员工的贡献，但是你可以得到近似值，那就是测量小团队的贡献。一般情况下，小型团队都由多人组成。只有表演或写作这样的特殊工作，你才会一个人单干。你最好找出色的人合作，因为他们的工作和你的一起平均计算。团队越大，每个人的贡献就越接近于整体的平均值。 如果你有两个选择，就选择较难的那个。如果你要选择是坐在家里看电视，还是外出跑步，那就出去跑步吧。这个方法有效的原因可能是遇到两个一难一易的选择时，往往出于懒惰的缘故，你会选择较易的那个选项。在意识深处，你其实知道不懒惰的做法会带来更好的结果，这个方法只是迫使你接受这一点。 关于工业革命的起因，已经有大量的文献论述过。但是，创造财富的人能够心安理得地享用自己的财富，这确实是工业革命的一个必要条件。如何从经济学观点看什么是创业公司呢？简单说，就是可以让人更快速工作的地方。你不再是慢慢地积累50年的普通工资，而是要尽快地将这笔钱赚到手。所以，政府禁止个人财富积累实际上就是命令人民减慢工作的速度。缓慢工作的后果并不仅仅是延迟了技术革新，而且很可能会扼杀技术革新。只有在快速获得巨大利益的激励下，你才会去挑战那些困难的问题，否则你根本不愿意去碰它们。冷战、第二次世界大战、近代的大多数战争都说明了这个道理。要鼓励大家去创业。只要懂得藏富于民，国家就会变得强大。让书呆子保住它们的血汗钱，你就会无敌于天下。 别老是想着去赚世界上的钱，而是应该想着为世界创造财富，当你真的能为世界创造财富的时候，金钱也就会找上你了。假设你拥有一辆老爷车，你可以不去管它，在家中悠闲度日，也可以自己动手把它修葺一新。这样做的话，你就创造了财富。世界上因为多了一辆修葺一新的车，财富就变的更多了一点，对你尤其是如此。财富和金钱完全是两码事，请大家不要混淆一谈，你拥有财富，比如技术、人脉、后台等等，这些都是可以帮你赢得金钱的，但是你拥有金钱不一定比财富来的稳固，如果被政府没收的话，你还能不能赚到这么多的金钱，但是拥有财富的人是不一样的，财富是别人无法夺走的，哪怕你现在是个穷光蛋，你都有可能随时变成百万富翁。 程序员创造财富的速率存在巨大的差异，一个优秀的程序员连续工作几个星期就可以创造价值100万美元的财富。同样的时间内，一个平庸的程序员不仅无法创造财富，甚至还可能减少财富（比如引入了bug）。这就是为什么如此之多的最优秀程序员都是自由主义者的原因。我们这个世界，你向下沉沦或者向上奋进都取决于你自己，因为最顶尖的5%的程序员写出了全世界99%的优秀软件。 创造财富不是致富的唯一方法，在人类的历史长河中，它甚至不是最常见的方法。就在几个世纪前，财富的主要来源还是矿石、奴隶、农奴、土地、牲畜，而快速获得财富的方法只有继承、婚姻、征服、没收。 关注贫富分化事实上，财富与金钱是两个概念。金钱只是用来交易财富的一种手段，财富才是有价值的东西，我们购买的商品和服务都属于财富。财富从何而来？人类创造出来的。财富总量不是固定不变的，不像大饼那样会被分光。如果你想要更多的财富，自己生产就可以了。由于每个人创造财富的能力和欲望强烈程度都不一样，所以每个人创造财富的数量很不平等。 作者提出了一种关于贫富分化的新观点，即现代社会的收入差距扩大是一种健康的信号。技术使得生产率的差异加速扩大，如果这种扩大没有反映在收入上面，只有三种可能的解释：（1）技术革新停顿了；（2）那些创造大部分财富的人停止工作了；（3）创造财富的人没有获得报酬。如果想社会繁荣而不扩大贫富分化，可以反推（1）（2）的可能性不大，那么只能是（3）。但是如果得不到报酬，人们是否愿意创造财富？唯一的可能就是，工作必须提供乐趣。 如果创业就是比别人工作的更勤奋、赚到更多的钱，那么很显然人人都想去创业。而且一定程度上，创业也比较有趣。但创业是有一些潜规则的，其中一条就是很多事情由不得你。比如，你无法决定到底付出多少。你只想更勤奋工作2到3倍，从而得到相应的回报。但是，真正创业以后，你的竞争对手决定了你到底要有多辛苦，而他们做出的决定都是一样的：你能吃多少苦，我们就能吃多少苦。另一条潜规则就是，创业的付出与回报虽然总体上是成比例的，但是个体上是不成比例的。对于个人来说，付出与回报之间存在一个很随机的放大因子。你努力了30倍，最后得到的回报在现实中并不是30倍，而是0到1000倍之间的一个随机数。假定所有创业者都是努力30倍，最后他们得到的总体平均回报是30倍，但中位数却是0。大多数创业公司都以失败告终，其中并不都是很烂的项目，一种很普遍的情况是，某个创业公司确实在开发一个很好的产品，但是开发时间长了一点，结果就是资金用完，关门散伙。创业公司如同蚊子，往往只有两种结局，要么赢得一切，要么彻底消失。 防止垃圾邮件的一种方法在所有对抗垃圾邮件的方法之中，“贝叶斯过滤”是最有效的工具。但是，使用的不同方法越多，综合效果就越好，因为任何对发送人构成限制的方法往往都会使得过滤器工作起来更顺利。即使同样是基于内容的过滤器，我也认为，如果有多种不同的软件可以同时使用会比较好。过滤器的差异越大，垃圾邮件想要逃过拦截就越不可能。 设计者的品味 好设计是简单的设计 好设计是永不过时的设计 好设计是解决主要问题的设计 好设计是启发性的设计 好设计通常是有点趣味性的设计 好设计是艰苦的设计 好设计是看似容易的设计 好设计是对称的设计 好设计是模仿大自然的设计 好设计是一种再设计 好设计是能够复制的设计 好设计常常是奇特的设计 好设计是成批出现的 好设计常常是大胆的设计 编程语言解析如果你长期使用某种编程语言，你就会慢慢按照这种语言的思维模式进行思考。所以，后来当你遇到其它任何一种有重大差异的语言，即使那语言本身并没有任何不对的地方，你也会觉得它极其难用。 计算机程序只是文本而已，你选择什么语言，决定了你能说什么话，编程语言就是程序员的思维方式。如果你仅仅是要养家糊口，那么劝你最好选择市场流行的以及未来一段时间内仍然会流行的语言，这样你才不会被市场抛弃，而且可以更容易的找到工作。但是如果你要做的是黑客级别的技术高手，那么一个锋利的工具对你会更有帮助。 最优秀的苹果机为什么在市场上表现不佳？还是那个老生常谈的问题：成本太高。微软公司把所有精力都集中在软件上面，微软到现在依然是靠它三个最赚钱的部门即Windows、Office和服务器三大部门来支撑它的整个商业帝国，这些都是微软之根本所在，如果你想要打败微软，那么首先想想这三个里面你能消灭掉哪个，如果你最多消灭两个，那么劝你早日投降，因为结局已定。所以我不反对企业做大做强，但是正如李嘉诚所说的，你一定要有一个支柱性产业，当你的其他方向的发展还没有实现盈利的时候，你的支柱性产业的利润就可以用来培育那些将来可能盈利的方向。同时一旦出现大的金融动荡，你依然能够保证进退自如，因为你始终有一个可以为公司带来利润的产业。所以很多厂商只要专攻硬件就可以了，把硬件成本降了下来。单单是微软的软件或者第三方厂商的硬件都不足以赢得市场优势，但是它们结合起来，就在个人电脑出现后一段关键时期中主导了市场。苹果公司同时做软件和硬件，所以成本上没有优势。IBM的PC机也面临类似的问题，它的PC机卖的相当的贵，质量也很好，但是却没能统治市场，为什么？因为IBM的机器设计的所使用的都是可以用十年甚至更久都不会出现问题的部件与材料，但是现在的电子消费品更新的如此之快，用户干嘛要花那么多钱去买一个他只准备用两年或三年的产品，但是这个产品却可以用十年都没有质量问题，所以说成本是产品占领市场不得不考虑的一个问题。 1998年，当时的搜索市场由雅虎统治，许多人认为再推出一个新的搜索引擎已经太晚了，互联网世界已经定型了。谷歌证明了这种看法是错误的。如果新事物真的有重大改进，那么它总是可以找到生存空间的。就像中国的人人网，模仿的是facebook，但是在人人之后，不是依然有程炳皓做出来的开心网吗，当初为什么开心败在了人人的手下，最根本的问题就是程炳皓根本不是陈一舟的对手，陈一舟是一个在商场里浸淫N年之久的老江湖，而程炳皓呢，仅仅是一个技术了得的技术员而已，所以开心的域名是kaixin001，当年有人劝程把kaixin这个域名买过来（因为创办开心网的时候，这个域名被老外注册过了），但是程根本不把此事放在心上，所以当人人对开心发起攻击的时候，陈一舟做的第一件事就是把kaixin这个域名买到手，然后利用许多用户的无知，把大量的用户引入到了这个假的开心网之上，即便后来陈一舟输了官司，关闭了kaixin，但是依然有大量的用户访问这个域名，而结果就是访问之后页面自动跳转到renren上了。所以对于开心输给人人之事，我不想多说，根本原因就是你不是人家的对手，所以输给别人也是再正常不过的了（说真的我很欣赏程炳皓这个人，一个许多人都应该学习的典范，一个中专学历的人，凭着自己对技术的钻研，到创办开心网，差点功成名就，只不过是由于不太懂得商场的残酷，而最终失败）。 我要纠正一个常识性的错误，几乎我碰到的所有的人都说虽然编程语言不同，但是你会了某种语言，其他的也就会了。我从来就没有这么认为过，首先不说他们的语法特点设计不同，就是语言的设计思想也没有一样的，另外每个语言都有每个语言所擅长的方向，经常看到有人拿Java和C++比速度，我觉得这种人就是无知，就像你拿男人和女人比谁的胸大是一回事，对于那些攻击Java速度的人，你怎么不比较Java和C++的跨平台能力呢？让我们复习一下Java的口号–“Write once, run ererywhere (WORA)”。请你们多多读书，提高修养。另外一点就是人们经常拿链表和数组比添加删除元素时候的速度，但是你有没有想过，你要添加删除某个元素是需要先找到它的，而数组的查找比链表要快的多的多，链表的添加删除是比数组快，但是如果我有几百万个元素，而我想删中间一个元素，数组可以直接定位，但是链表要查找将近一半的元素。谁快谁慢一目了然。如果你长期使用某种语言，你就会慢慢按照这种语言的思维模式进行思考。所以，后来当你遇到其他任何一种有重大差异的语言，即使那种语言本身并没有任何不对的地方，你也会觉得它及其难用。缺乏经验的程序员对于各种语言优缺点的判断经常被这种心态误导。一种编程语言能否流行并不取决于它本身，大多数程序员也许无法分辨语言的好坏。但是，这并不代表优秀的编程语言会被埋没，专家级的黑客一眼就能认出它们，并且会拿来使用。一种真正的优秀的编程语言应该既整洁又混乱。“整洁”的意思是设计得很清楚，内核由数量不多的运算符构成，这些运算符易于理解，每一个都有很完整的独立用途。“混乱”的意思是它允许黑客以自己的方式使用。 一百年后的编程语言编程语言进化缓慢的原因在于它们并不是真正的技术。语言只是一种书写法，而程序则是一种严格符合规则的描述，以书面形式记录计算机应该如何解决你的问题。所以编程语言的进化速度更像数学符号的进化速度，而不像真正的技术的进化速度。数学符号的进化是缓慢的渐变式变化，而不是真正技术的那种跳跃式发展。 拒绝平庸如果从图灵等价的角度来看，所有语言都是一样强大的，但是这对程序员没有意义。程序员关心的那种强大也许很难正式定义，但是有一个办法可以解释，那就是有一些功能在一种语言中是内置的，但是在另一种语言中需要修改解释器才能做到，那么前者就比后者更强大。 技术的变化速度通常是很快的。但是编程语言不一样，与其说它是技术，还不如说是程序员的思考模式。编程语言是技术和宗教的混合物。所以，一种很普通的编程语言就是很普通的程序员使用的语言，它的变化就像冰山那样缓慢。 书呆子的复仇衡量语言的编程能力的最简单方法可能就是看代码数量。所谓高级语言，就是能够提供更强大抽象能力的语言，从某种意义上，就像能够提供更大的砖头，所以砌墙的时候用到的砖头数量就变少了。因此，语言的编程能力越强大，写出来的程序就越短。 梦寐以求的编程语言优秀的语言不一定会被市场接受，很可能无人使用，因为语言的流行不取决于它本身。语言流行的秘诀必须要涵盖以下几点，一种免费的实现，一本相关的书籍，以及语言所依附的计算机系统，除此之外还必须要简洁。其它特点还有诸如可编程性、一次性程序、精心设计的函数库、效率等。 设计与研究设计与研究的区别看来就在于，前者追求“好”，后者追求“新”。优秀的设计不一定很“新”，但必须是“好”的；优秀的研究不一定很“好”，但必须是“新”的。作者认为两条道路最后会发生交叉：只有应用“新”的创意和理念，才会诞生超越前人的最佳设计；只有解决那些值得解决的难题，才会诞生最佳研究。所以设计和研究都通向同一个地方，只是前进的路线不同罢了。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 实战-索引热备份及恢复]]></title>
    <url>%2F2016%2F06%2F24%2FLucene-6-0-in-action-the-index-of-hot-backup-and-recovery%2F</url>
    <content type="text"><![CDATA[索引备份的几个关键问题 最简单的备份方式是关闭IndexWriter，然后逐一拷贝索引文件，但是如果索引比较大，那么这种备份操作会持续较长时间，而在备份期间，程序无法对索引文件进行修改，很多搜索程序是不能接受索引操作期间如此长时间停顿的 那么不关闭IndexWriter又如何呢？这样也不行，因为在拷贝索引期间，如果索引文件发生变化，会导致备份的索引文件损坏 另外一个问题就是如果原索引文件损坏的话，再备份它也毫无意义，所以一定要备份的是最后一次成功commit之后的索引文件 每次在备份之前，如果程序将要覆盖上一个备份，需要先删除备份中未出现在当前快照中的文件，因为这些文件已经不会被当前索引引用了；如果每次都更改备份路径的话，那么就直接拷贝即可 索引热备份从Lucene 2.3版本开始，Lucene提供了一个热备策略，就是SnapshotDeletionPolicy，这样就能在不关闭IndexWriter的情况下，对程序最近一次索引修改提交操作时的文件引用进行备份，从而能建立一个连续的索引备份镜像。那么你也许会有疑问，在备份期间，索引出现变化怎么办呢？这就是SnapshotDeletionPolicy的牛逼之处，在使用SnapshotDeletionPolicy.snapshot()获取快照之后，索引更新提交时刻的所有文件引用都不会被IndexWriter删除，只要IndexWriter并未关闭，即使IndexWriter在进行更新、优化操作等也不会删除这些文件。如果说索引拷贝过程耗时较长也不会出现问题，因为被拷贝的文件时索引快照，在快照的有效期，其引用的文件会一直存在于磁盘上。 所以在备份期间，索引会比通常情况下占用更大的磁盘空间，当索引备份完成后，可以调用SnapshotDeletionPolicy.release (IndexCommit commit) 释放指定的某次提交，以便让IndexWriter删除这些已被关闭或下次将要更新的文件。 需要注意的是，Lucene对索引文件的写入操作是一次性完成的。这意味着你可以简单通过文件名比对来完成对索引的增量备份备份，你不必查看每个文件的内容，也不必查看该文件上次被修改的时间戳，因为一旦程序从快照中完成文件写入和引用操作，这些文件就不会改变了。 segments.gen文件在每次程序提交索引更新时都会被重写，因此备份模块必须要经常备份该文件，但是在Lucene 6.0中注意segments.gen已经从Lucene索引文件格式中移除，所以无需单独考虑segments.gen的备份策略了。在备份期间，write.lock文件不用拷贝。 SnapshotDeletionPolicy类有两个使用限制 该类在同一时刻只保留一个可用的索引快照，当然你也可以解除该限制，方法是通过建立对应的删除策略来同时保留多个索引快照 当前快照不会保存到硬盘，这意味着你关闭旧的IndexWriter并打开一个新的IndexWriter，快照将会被删除，因此在备份结束前是不能关闭IndexWriter的，否则也会报org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed异常。不过该限制也是很容易解除的：你可以将当前快照存储到磁盘上，然后在打开新的IndexWriter时将该快照保护起来，这样就能在关闭旧的IndexWriter和打开新IndexWriter时继续进行备份操作 索引热备份解决方案123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import org.apache.commons.io.FileUtils;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.*;import org.apache.lucene.index.*;import org.apache.lucene.search.TermQuery;import org.apache.lucene.store.FSDirectory;import java.io.File;import java.io.IOException;import java.nio.file.Paths;import java.util.Collection;import static org.apache.lucene.document.Field.Store.YES;/** * 测试Lucene索引热备 */public class TestIndexBackupRecovery &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; String f = "D:/index_test"; String d = "D:/index_back"; IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new StandardAnalyzer()); indexWriterConfig.setIndexDeletionPolicy(new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy())); IndexWriter writer = new IndexWriter(FSDirectory.open(Paths.get(f)), indexWriterConfig); Document document = new Document(); document.add(new StringField("ID", "111", YES)); document.add(new IntPoint("age", 111)); document.add(new StoredField("age", 111)); writer.addDocument(document); writer.commit(); document = new Document(); document.add(new StringField("ID", "222", Field.Store.YES)); document.add(new IntPoint("age", 333)); document.add(new StoredField("age", 333)); writer.addDocument(document); document.add(new StringField("ID", "333", Field.Store.YES)); document.add(new IntPoint("age", 555)); document.add(new StoredField("age", 555)); for (int i = 0; i &lt; 1000; i++) &#123; document = new Document(); document.add(new StringField("ID", "333", YES)); document.add(new IntPoint("age", 1000000 + i)); document.add(new StoredField("age", 1000000 + i)); document.add(new StringField("desc", "ABCDEFG" + i, YES)); writer.addDocument(document); &#125; writer.deleteDocuments(new TermQuery(new Term("ID", "333"))); writer.commit(); backupIndex(writer, f, d); writer.close(); &#125; public static void backupIndex(IndexWriter indexWriter, String indexDir, String backupIndexDir) throws IOException &#123; IndexWriterConfig config = (IndexWriterConfig) indexWriter.getConfig(); SnapshotDeletionPolicy snapshotDeletionPolicy = (SnapshotDeletionPolicy) config.getIndexDeletionPolicy(); IndexCommit snapshot = snapshotDeletionPolicy.snapshot(); //设置索引提交点，默认是null，会打开最后一次提交的索引点 config.setIndexCommit(snapshot); Collection&lt;String&gt; fileNames = snapshot.getFileNames(); File[] dest = new File(backupIndexDir).listFiles(); String sourceFileName; String destFileName; if (dest != null &amp;&amp; dest.length &gt; 0) &#123; //先删除备份文件中的在此次快照中已经不存在的文件 for (File file : dest) &#123; boolean flag = true; //包括文件扩展名 destFileName = file.getName(); for (String fileName : fileNames) &#123; sourceFileName = fileName; if (sourceFileName.equals(destFileName)) &#123; flag = false; break;//跳出内层for循环 &#125; &#125; if (flag) &#123; file.delete();//删除 &#125; &#125; //然后开始备份快照中新生成的文件 for (String fileName : fileNames) &#123; boolean flag = true; sourceFileName = fileName; for (File file : dest) &#123; destFileName = file.getName(); //备份中已经存在无需复制，因为Lucene索引是一次写入的，所以只要文件名相同不要要hash检查就可以认为它们的数据是一样的 if (destFileName.equals(sourceFileName)) &#123; flag = false; break; &#125; &#125; if (flag) &#123; File from = new File(indexDir + File.separator + sourceFileName);//源文件 File to = new File(backupIndexDir + File.separator + sourceFileName);//目的文件 FileUtils.copyFile(from, to); &#125; &#125; &#125; else &#123; //备份不存在，直接创建 for (String fileName : fileNames) &#123; File from = new File(indexDir + File.separator + fileName);//源文件 File to = new File(backupIndexDir + File.separator + fileName);//目的文件 FileUtils.copyFile(from, to); &#125; &#125; snapshotDeletionPolicy.release(snapshot); //删除已经不再被引用的索引提交记录 indexWriter.deleteUnusedFiles(); &#125;&#125; 索引文件格式首先索引里都存了些什么呢？一个索引包含一个documents的序列，一个document是一个fields的序列，一个field是一个有名的terms序列，一个term是一个比特序列。 根据Summary of File Extensions的说明，目前Lucene 6.0中存在的索引格式如下 Name Extension Brief Description Segments File segments_N Stores information about a commit point Lock File write.lock The Write lock prevents multiple IndexWriters from writing to the same file Segment Info .si Stores metadata about a segment Compound File .cfs, .cfe An optional “virtual” file consisting of all the other index files for systems that frequently run out of file handles Fields .fnm Stores information about the fields Field Index .fdx Contains pointers to field data Field Data .fdt The stored fields for documents Term Dictionary .tim The term dictionary, stores term info Term Index .tip The index into the Term Dictionary Frequencies .doc Contains the list of docs which contain each term along with frequency Positions .pos Stores position information about where a term occurs in the index Payloads .pay Stores additional per-position metadata information such as character offsets and user payloads Norms .nvd, .nvm Encodes length and boost factors for docs and fields Per-Document Values .dvd, .dvm Encodes additional scoring factors or other per-document information Term Vector Index .tvx Stores offset into the document data file Term Vector Documents .tvd Contains information about each document that has term vectors Term Vector Fields .tvf The field level info about term vectors Live Documents .liv Info about what files are live Point values .dii, .dim Holds indexed points, if any 在Lucene索引结构中，既保存了正向信息，也保存了反向信息。正向信息存储在：索引(index)-&gt;段(segment)-&gt;文档(document)-&gt;field(.fnm/.fdx/.fdt)-&gt;term(.tvx/.tvd/.tvf)反向信息存储在：词典(.tim)-&gt;倒排表(.doc/.pos) 如果想要查看中文，可以参考这里。 恢复索引恢复索引步骤如下 关闭索引目录下的全部reader和writer，这样才能进行文件恢复。对于Windows系统来说，如果还有其它进程在使用这些文件，那么备份程序仍然不能覆盖这些文件 删除当前索引目录下的所有文件，如果删除过程出现“访问被拒绝”（Access is denied）错误，那么再次检查上一步是否已完成 从备份目录中拷贝文件至索引目录。程序需要保证该拷贝操作不会碰到任何错误，如磁盘空间已满等，因为这些错误会损坏索引文件 对于损坏索引，可以使用CheckIndex（org.apache.lucene.index）进行检查并修复 通常Lucene能够很好地避免大多数常见错误，如果程序遇到磁盘空间已满或者OutOfMemoryException异常，那么它只会丢失内存缓冲中的文档，已经编入索引的文档将会完好保存下来，并且索引也会保持原样。这个结果对于以下情况同样适用：如出现JVM崩溃，或者程序碰到无法控制的异常，或者程序进程被终止，或者操作系统崩溃，或者计算机突然断电等。123456789101112131415161718192021222324252627282930313233343536/** * @param source 索引源 * @param dest 索引目标 * @param indexWriterConfig 配置相关 */public static void recoveryIndex(String source, String dest, IndexWriterConfig indexWriterConfig) &#123; IndexWriter indexWriter = null; try &#123; indexWriter = new IndexWriter(FSDirectory.open(Paths.get(dest)), indexWriterConfig); &#125; catch (IOException e) &#123; log.error("", e); &#125; finally &#123; //说明IndexWriter正常打开了，无需恢复 if (indexWriter != null &amp;&amp; indexWriter.isOpen()) &#123; try &#123; indexWriter.close(); &#125; catch (IOException e) &#123; log.error("", e); &#125; &#125; else &#123; //说明IndexWriter已经无法打开，使用备份恢复索引 //此处简单操作，先清空损坏的索引文件目录，如果索引特别大，可以比对每个文件，不必全部删除 try &#123; FileUtils.deleteDirectory(new File(dest)); FileUtils.copyDirectory(new File(source), new File(dest)); &#125; catch(IOException e)&#123; log.error("", e); //使用备份恢复出错，那么就使用最后一招修复索引 log.info("Check index &#123;&#125; now!", dest); try &#123; IndexUtils.checkIndex(dest); &#125; catch (IOException | InterruptedException e1) &#123; log.error("Check index error!", e1); &#125; &#125; &#125;&#125; 修复索引当其它所有方法都无法解决索引损坏问题时，你的最后一个选项就是使用CheckIndex工具了。该工具除了能汇报索引细节状况以外，还能完成修复索引的工作。该工具会强制删除索引中出现问题的段，需要注意的是，该操作还会全部删除这些段包含的文档，该工具的使用目标应主要着眼于能够在紧急状况下让搜索程序再次运行起来，一旦我们进行了索引备份，并且备份完好，应优先使用恢复索引，而不是修复索引。1234567891011121314151617181920212223242526/** * CheckIndex会检查索引中的每个字节，所以当索引比较大时，此操作会比较耗时 * * @throws IOException * @throws InterruptedException */public void checkIndex(String indexFilePath) throws IOException, InterruptedException &#123; CheckIndex checkIndex = new CheckIndex(FSDirectory.open(Paths.get(indexFilePath))); checkIndex.setInfoStream(System.out); CheckIndex.Status status = checkIndex.checkIndex(); if (status.clean) &#123; System.out.println("Check Index successfully！"); &#125; else &#123; //产生索引中的某个文件之后再次测试 System.out.println("Starting repair index files..."); //该方法会向索引中写入一个新的segments文件，但是并不会删除不被引用的文件，除非当你再次打开IndexWriter才会移除不被引用的文件 //该方法会移除所有存在错误段中的Document索引文件 checkIndex.exorciseIndex(status); checkIndex.close(); //测试修复完毕之后索引是否能够打开 IndexWriter indexWriter = new IndexWriter(FSDirectory.open(Paths.get(indexFilePath)), new IndexWriterConfig(new StandardAnalyzer())); System.out.println(indexWriter.isOpen()); indexWriter.close(); &#125;&#125; 如果索引完好，输出如下信息： Segments file=segments_2 numSegments=2 version=6.0.0 id=2jlug2dgsc4tmgkf5rck1xgm4 1 of 2: name=_0 maxDoc=1 version=6.0.0 id=2jlug2dgsc4tmgkf5rck1xgm1 codec=Lucene60 compound=true numFiles=3 size (MB)=0.002 diagnostics = {java.runtime.version=1.8.0_45-b15, java.vendor=Oracle Corporation, java.version=1.8.0_45, java.vm.version=25.45-b02, lucene.version=6.0.0, os=Windows 7, os.arch=amd64, os.version=6.1, source=flush, timestamp=1464159740092} no deletions test: open reader………OK [took 0.051 sec] test: check integrity…..OK [took 0.000 sec] test: check live docs…..OK [took 0.000 sec] test: field infos………OK [2 fields] [took 0.000 sec] test: field norms………OK [0 fields] [took 0.000 sec] test: terms, freq, prox…OK [1 terms; 1 terms/docs pairs; 0 tokens] [took 0.007 sec] test: stored fields…….OK [2 total field count; avg 2.0 fields per doc] [took 0.008 sec] test: term vectors……..OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec] test: docvalues………..OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec] test: points…………..OK [1 fields, 1 points] [took 0.001 sec] 2 of 2: name=_1 maxDoc=1001 version=6.0.0 id=2jlug2dgsc4tmgkf5rck1xgm3 codec=Lucene60 compound=true numFiles=4 size (MB)=0.023 diagnostics = {java.runtime.version=1.8.0_45-b15, java.vendor=Oracle Corporation, java.version=1.8.0_45, java.vm.version=25.45-b02, lucene.version=6.0.0, os=Windows 7, os.arch=amd64, os.version=6.1, source=flush, timestamp=1464159740329} has deletions [delGen=1] test: open reader………OK [took 0.003 sec] test: check integrity…..OK [took 0.000 sec] test: check live docs…..OK [1000 deleted docs] [took 0.000 sec] test: field infos………OK [3 fields] [took 0.000 sec] test: field norms………OK [0 fields] [took 0.000 sec] test: terms, freq, prox…OK [1 terms; 1 terms/docs pairs; 0 tokens] [took 0.008 sec] test: stored fields…….OK [2 total field count; avg 2.0 fields per doc] [took 0.012 sec] test: term vectors……..OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec] test: docvalues………..OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec] test: points…………..OK [1 fields, 1001 points] [took 0.001 sec]No problems were detected with this index.Took 0.159 sec total.Check Index successfully！ 在破坏索引之后（删除了一个cfe文件），再次运行输出 Segments file=segments_2 numSegments=2 version=6.0.0 id=2jlug2dgsc4tmgkf5rck1xgm4 1 of 2: name=_0 maxDoc=1 version=6.0.0 id=2jlug2dgsc4tmgkf5rck1xgm1 codec=Lucene60 compound=true numFiles=3FAILED WARNING: exorciseIndex() would remove reference to this segment; full exception:java.nio.file.NoSuchFileException: D:\index_test_0.cfe at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)…… at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) 2 of 2: name=_1 maxDoc=1001 version=6.0.0 id=2jlug2dgsc4tmgkf5rck1xgm3 codec=Lucene60 compound=true numFiles=4 size (MB)=0.023 diagnostics = {java.runtime.version=1.8.0_45-b15, java.vendor=Oracle Corporation, java.version=1.8.0_45, java.vm.version=25.45-b02, lucene.version=6.0.0, os=Windows 7, os.arch=amd64, os.version=6.1, source=flush, timestamp=1464159740329} has deletions [delGen=1] test: open reader………OK [took 0.059 sec] test: check integrity…..OK [took 0.000 sec] test: check live docs…..OK [1000 deleted docs] [took 0.001 sec] test: field infos………OK [3 fields] [took 0.000 sec] test: field norms………OK [0 fields] [took 0.000 sec] test: terms, freq, prox…OK [1 terms; 1 terms/docs pairs; 0 tokens] [took 0.013 sec] test: stored fields…….OK [2 total field count; avg 2.0 fields per doc] [took 0.016 sec] test: term vectors……..OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec] test: docvalues………..OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec] test: points…………..OK [1 fields, 1001 points] [took 0.002 sec]WARNING: 1 broken segments (containing 1 documents) detectedTook 0.165 sec total.Starting repair index files…true]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 实战-索引备份No index commit to snapshot]]></title>
    <url>%2F2016%2F06%2F24%2FLucene-6-0-in-action-index-backup-No-index-commit-to-snapshot%2F</url>
    <content type="text"><![CDATA[备份之前先提交在使用Lucene进行索引备份的时候，报错 Exception in thread “main” java.lang.IllegalStateException: No index commit to snapshotat org.apache.lucene.index.SnapshotDeletionPolicy.snapshot(SnapshotDeletionPolicy.java:160) 其中导致抛错的代码是123SnapshotDeletionPolicy snapshotDeletionPolicy = (SnapshotDeletionPolicy)indexWriter.getConfig().getIndexDeletionPolicy();IndexCommit snapshot = snapshotDeletionPolicy.snapshot(); 查看snapshot()方法源码发现1234if (lastCommit == null) &#123; // No commit yet, eg this is a new IndexWriter: throw new IllegalStateException("No index commit to snapshot");&#125; 可见如果是第一次也就是说还没有commit过，这种情况通常是第一次使用IndexWriter的时候，那么就会抛此异常。解决办法很简单，就是在热备之前commit索引，将所有的缓存flush到硬盘，这也是热备的正确逻辑，可以确保将索引的最新变动保存到备份之中。 确保使用同一个SnapshotDeletionPolicy另外还有一点需要注意，如果你在备份的时候使用的是新new的SnapshotDeletionPolicy，那么同样会抛出此异常123SnapshotDeletionPolicy snapshot= new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());IndexCommit commit = snapshot.snapshot();Collection&lt;String&gt; fileNames = commit.getFileNames(); 这是因为采用new的形式得到的SnapshotDeletionPolicy和最初实例化IndexWriter的时候使用的并不是同一个，所以对于IndexWriter所做的提交，新的SnapshotDeletionPolicy是无法知道的。正确方式如下1234567IndexWriterConfig config = (IndexWriterConfig) indexWriter.getConfig();SnapshotDeletionPolicy snapshotDeletionPolicy = (SnapshotDeletionPolicy) config.getIndexDeletionPolicy();IndexCommit snapshot = snapshotDeletionPolicy.snapshot();//设置索引提交点，默认是null，会打开最后一次提交的索引点config.setIndexCommit(snapshot);Collection&lt;String&gt; fileNames = snapshot.getFileNames();]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《把时间当作朋友》]]></title>
    <url>%2F2016%2F06%2F10%2FRead-treat-time-as-a-friend%2F</url>
    <content type="text"><![CDATA[人生就是两件事：一是该做的；二是想做的。摆对顺序成就一生，摆错顺序一事无成。 作者简介：李笑来，原新东方老师，现艾德睿智国际教育咨询合伙人。他，经历丰富，与众小有不同。东北人，就读普通高校，本科学财会，毕业干销售，惯于洞察他人心理，一不留神，成了新东方教写作的名师，授业解惑，乐此不疲。 《把时间当作朋友》主要讲述的是关于开启心智方面的，可以这么说，李笑来用最浅显直白的语言表述了我们日常生活中经常发生的场景，其思想虽谈不上深邃，但总能比常人多看一层，而这一层就足以和常人拉开差距。阅读本书也让我认识到我以前心中一些不甚明了的困惑，李笑来旁征博引，用简短明了的示例或名言，直指我们日常生活中习以为常以至于司空见惯的坏习惯。然诸多当事人却沉浸其中，而无法自知。 很多学生都是“既勤奋又懒惰”的类型，我觉得对于绝大多数资质平平而又努力向上的人来说，都或多或少的会沾染些许“既勤奋又懒惰”的坏习惯。然而，不要忘记生活中明显有另外一些人——尽管数量上并不占优——在用另外一种状态生活。他们从容，他们优雅，他们善于化解各种压力，安静地去做他们认为应该做的事情，并总能有所成就。这些人中不乏天资聪颖，亦不乏后天环境优越或是父辈竭尽全力而为其创造各种优越条件。那么先不谈如何赶上甚至超越这部分人，就谈与他们并驾齐驱好了，你可能首先想到的方法就是“管理时间”抑或“时间管理”。然而时间不会服从于任何人的管理，它只会自顾自地流逝。时间不理任何人，它用自己特有的速度流逝，不受任何外界因素影响。所以李笑来老师给出的观点是：我们无法管理时间，我们真正能够管理的，是我们自己。 在阅读本书过程中，仅摘录部分要点如下，如果你想要获得更全面的知识，请自行阅读全书。 人们很难接受与已有知识和经验相左的信息或观念 被灌输的观念，越是错误的，越有惊人的繁殖力 人类能将自己的思考作为思考对象的能力被称作“元认知能力” 你的大脑并不是你，你的大脑是属于“你的”大脑，尽管你用它思考，好像它也在指导你的行为，但是你要明白，你不应该隶属于你的大脑，而应该是它隶属于你 人所拥有的任何东西，都可以被剥夺，唯独人性最后的自由——也就是在任何境遇中选择一己态度和生活方式的自由——不能被剥夺 心智与智商不同，大多数人都拥有正常的智商，但并非每个拥有正常智商的人都拥有正常的心智，许多人的心智仍处于未开启的状态 骗子想要成功行骗，必须把想让别人相信的谎言掺到大量的真理之中 有些认识，哪怕是简单的常识，也需要亲身经历后才能真正体会 我们总在不由自主地想，如果有速成的办法就好了——可惜，没有，确实没有 英文代码中的“foo”、“bar”类似于中文里的“张三”、“李四”，是一个无特别含义的名字。（我表示刚知道） 为了进步，我们必须忍受一定的未知，不要陷入“牛角尖陷阱”，例如循环代码块里的起始变量名称为什么是“i”，为什么第二次循环的起始变量名称通常是“j”，为什么“John”这个英文会被翻译成“约翰”？ 尽管天分很重要，但一个人的能力主要靠积累获得 越是不满现状，摆脱现状的欲望就越强烈，而这种欲望会让一个人最终迷失方向，因为无论是谁，从本质上看都无法摆脱现状——每一时刻的现状都是过去某一或者某些时刻的结果，而每一时刻的现状都是未来某一或者某些时刻的原因。没有人能够逃脱现实的束缚 从某种意义上理解，“逆境造就成功、磨难令人成熟”之类的话纯属胡说八道。显然，在顺境中更容易成功，而且很多磨难根本没有必要——这更可能是失败者对他们自己一生都未曾有机会体验的成功以及成功者“意淫”式的猜想而已 失败者永远没有机会了解成功的真相，因为人最容易受自身经验的限制，而不曾有哪怕一点点成功经验的人更无从摆脱自身的局限 对现状不满、急于摆脱现状，是人们常常不知不觉落入的陷阱（尽管偶尔这也是少数人真正的动力）。接受现状才是最优策略——有什么做什么，有什么用什么；做什么都做好，用什么都用好 资源原本是有限的，经济学里将这种现象描述为“资源稀缺”。在整体上资源稀缺的前提下，“资源并非均匀分布”体现在每个人身上，直接的结果就是“绝大多数人都觉得自己拥有的不够多”。在我们生存的这个世界里，资源稀缺是客观现实，也恰因如此，人们的主观愿望肯定不可能全部被满足 墨菲定律：Murphy’s law，一个经验定律，其一般表述为“凡事只要可能出错，就会出错” 对一个5岁的孩子来讲，未来的1年相当于他已经度过的人生的¹⁄₅，即20%；而对一个50 岁的成年人来讲，未来的1年只相当于他已经度过的人生的¹⁄₅₀，即2%。所以，随着年龄的增加，人们会觉得时间运动得越来越快 每个人都有必要阅读项目管理方面的经典书籍。也许你并没有“项目经理”之类的头衔，但，实际上，每个人都应该是自己的项目经理—自主、独立，是心智成熟的人必有的素质 无论学到了什么东西，都可以接着问自己：“那……这个道理还可以运用在什么地方？”反复问自己这种简单问题，能够锻炼自己融会贯通、举一反三的能力 尽管总是有人劝诫“速成没戏”，但还是有人宣扬各种速成的方法，并且信者大有人在，宛若“野火烧不尽，春风吹又生”。为什么呢？上过中学的人都应该明白“省功不省力、省力不省功”的物理原理啊！其实，这些人缺乏的就是这种思考能力或者说思考习惯 我从来都不相信“人人都能成功”之类的话，我顶多相信“其实人人原本都有可能成功”。我觉得，一个人最终成功的关键，并不是因为他曾经精确地计划过自己的成功，而是坚持 从更高的层面上说，设计验收机制也是任何一个领导者必须拥有的基本能力。哪怕你领导的只是一个很小的团队，你也都必然要向团队成员指派各种各样的任务。在这种情况下，如果你没有设计验收机制，最终的结果肯定会让你非常失望，因为缺少验收机制会使团队成员对自己的工作质量毫不介意，长此以往，团队的执行力将等于零，作为团队领导者的你也必须承担失败的责任 不仅存在无法通过个体或者群体经验获得的知识，还存在与现有经验相悖的知识 我常常暗骂现在的大学本科教育。不夸张地讲，今天的本科教育很大程度上已经忘了“本”。本科教育之“本”在于培养学生的自学能力。从理论上讲，一个人本科毕业之后，应该有能力自学他所需要的任何知识 “自证预言”-罗伯特·莫顿教授发现了这种现象，并将其命名为“自证预言”，即如果人们相信某件事情会发生（事实上其原本并不见得一定会发生），那么这件事情最终真的会发生 “自利性偏差”-人类普遍拥有的一个认知偏差就是：把成功揽到自己身上，把失败归咎于别人或者坏运气 成功背后的东西很难看清楚，所谓成功的真实性也很难判断，而成功者们又会有意无意地美化和包装他们的经验，而这一切，都在干扰我们的判断。不过，观察失败者却相对容易得多，因为失败者的失败往往是显然的、确定的，失败的真正原因也往往很容易查实 有些时候，我们的思维会因我们所使用的语言（表达手段之一）而受到各种各样的影响。恰当而又正确地使用语言，可以帮助修复思维漏洞。一旦明白个中道理，我们就会发现，语言就是一个便宜（甚至免费）而又有效的辅助工具 以下一些句式最好经常使用，因为它们特别有助于独立思考习惯的养成，并且也有刺激思考的作用：▷ ……是一回事，而……是另外一回事。▷ ……和……其实根本不是一回事。▷ ……不一定……▷ ……。可是，这并不意味着……▷ ……也许还有另外一种可能性（解释）。▷ ……看起来像……，可是……▷ ……。而事实却可能远比看起来的更为复杂（简单）。▷ ……。然而，（这个论断）反过来（陈述）却不一定成立……▷ ……其实很可能与……根本就没有任何关系。▷ ……和……之间不一定是单纯的因果关系，它们也可能互为因果。▷ ……和……之间的比较也许没有任何意义。▷ ……其实不过是表面现象，其背后的本质是……▷ ……有一个通常被忽略的前提。▷ ……尽管听起来很有道理，然而却完全不现实。▷ ……也许有人会说……，但是这种质疑却……这些句式看起来简单，却往往能带来不同凡响的思考结果。平时遇到任何问题的时候，都不妨把这些句式套进去填空—就当想着玩了—要不了多久你就能体会这种游戏的有趣之处。不出意外的话，我们会发现自己的思维因为这些句式的运用而不由自主地发生了巨大转变。例如，“……和……其实根本不是一回事”这个句式往往瞬间就能使一个人的脑子更加清楚 大多数人会对自己的记忆力过分高估。这个幻觉来自每时每刻都有一些确实可以记得住的东西，而记不住的东西恰恰则因为没有被记住所以看上去“并不存在”。换言之，每时每刻都有“我记得住”的证据，而“我记不住”的证据基本上难觅其踪。这也就是为什么总有那么多人真诚地相信自己考试成绩差是因为“没发挥好” 我个人认为，在分享知识的时候，“知无不言，言无不尽”是正确的；而在日常交流中，这个原则的适用性非常差 在讨论问题的时候，我们常常会被对方的“固执己见”挫败，但在对方眼里，我们可能也是“固执己见”的，只不过是程度不同而已 我经常鼓励学生只要有时间就去看杂书—越杂越好，多多益善。为什么呢？因为读杂书会大大提高一个人接受新事物的能力（这种能力也是理解能力的一种）。阅历丰富、博览群书的人，肯定拥有更强的理解能力，因为他们在遇到未知的时候，更有可能迅速地在自己已有的知识中找到可以用来类比的信息 方法固然重要，但是比起“用功”来说，方法几乎可以忽略不计。由此可见，所有学习上的成功，都只依靠两件事—策略和坚持，而坚持本身就是最重要的策略。坚持，其实就是重复；而重复，说到底就是时间的投入，准确地说，是大量时间的投入 浪费时间、虚度年华的人，有一个共同的特征—拼命想控制自己完全不能控制的事物，却在自己真正能掌控的地方彻底失控 当一个人没有准备好的时候，对他来讲，不存在任何机会。机会时时刻刻都会出现在我们身边，关键在于，我们有没有足够努力，做到“万事俱备，只欠东风”。而当一个人准备好了的时候，随处都是机会，而且所有的机会都是切实的、可以把握的 从整体上看，人脉当然很重要。不过，针对某个个体来说，比人脉更重要的是他所拥有的资源。有些资源很难靠白手起家获得，比如金钱、地位、名誉。然而，有些资源却可以轻易从零开始积累，比如一个人的才华与学识。才华与学识，是一定可以通过努力获得的 如果一个人的身边都是优秀的人，就往往会出现没有人求他帮忙的景况，因为优秀的人几乎无一例外都以耽误别人的时间为耻，同时，这些人恰恰因为能够独立解决遇到的问题才被其他认为是优秀者 专心做可以提升自己的事情，学习并拥有更多、更好的技能，成为一个值得他人交往的人 学会独善其身，以不给他人制造麻烦为美德，用自己的独立赢得尊重 鸡尾酒会效应：cocktail party effect。指人的一种听力选择能力，在这种情况下，注意力集中在某一个人的谈话之中而忽略背景中其他的对话或噪音 当我们不停地鼓励所有人的时候，最大的受益者其实是我们自己，因为最终我们会发现，自己开始进入一种他人无法想象的状态，成为一个不需要他人鼓励的人。这一点很重要。因为很多人之所以做事裹足不前，浪费时间甚至生命，原因就在于他们是必须获得别人的鼓励才敢于行动的人。可是，我们却能成为另外一种人—我们可以不需要被别人鼓励—这是一种境界 对一个人来说，家庭是最重要的。因为最终有一天我们会发现，在很极端的情况下，依然支持我们的肯定是也通常只是我们的家庭—无论我们认为自己的家庭是好还是坏 当一个团队成功的时候，几乎每个人都会倾向于把成功归因于自己的贡献而忽略别人的存在；当一个团队失败的时候，几乎每个人都会倾向于把失败归咎于他人的过失而尽量把自己排除在外。这种现象被称作“自利性偏差” 劣币驱逐良币：Bad money drives out good，也称格雷欣法则（Gresham’s Law）。该法则认为当有两种名义价值相同但实际价值不同的货币同时在市场上流通时，实际价值高的货币将会被屯积并最终被实际价值低的货币取代 God, grant me the serenity to accept the things I cannot change, Courage to change the things I can, And wisdom to know the difference.（愿上帝赐予我从容去接受我不能改变的，赐予我勇气去改变我可以改变的，并赐予我智慧去分辨这两者间的区别。） 任何积累都需要时间，而且必然需要漫长的时间。也正因如此，大多数人才不肯积累，不愿积累，甚至不屑于积累 爱因斯坦说过：“用当年我们制造麻烦的思路，我们根本无法解决它们。”所以，别再跟时间较劲了。看清楚、想明白，问题出在自己身上。将来，时间可能是我们的敌人，也可能是我们的朋友。时间究竟是敌是友，就看你的了……]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 实战（5）-索引搜索器IndexSearcher]]></title>
    <url>%2F2016%2F05%2F24%2FLucene-6-0-in-action-5-The-index-search-an-IndexSearcher%2F</url>
    <content type="text"><![CDATA[Lucene的主要搜索API一个简单的搜索应用主要包括索引和搜索两部分，在Lucene中，IndexSearcher类是用于对索引中文档进行搜索的核心类，它有几个重载的搜索方法，可以使用最常用的方法对特定的项进行搜索，一个项由一个字符串类型的域值和对应的域名构成。现将搜索相关API汇总如下 类 目的 IndexSearcher 搜索索引的核心类。所有搜索都通过IndexSearcher进行，它们会调用该类中重载的search方法 Query及其子类 封装某种查询类型的具体子类。Query实例将被传递给IndexSearcher的search方法 QueryParser 将用户输入的可读的查询表达式处理成具体的Query对象 TopDocs 保持由IndexSearcher.search()方法返回的具有较高评分的顶部文档 ScoreDoc 提供对TopDocs中每条搜索结果的访问接口 IndexSearcher简介首先看IndexSearcher的doc注释 Implements search over a single IndexReader.Applications usually need only call the inherited search(Query, int) or search(Query, Filter, int) methods. For performance reasons, if your index is unchanging, you should share a single IndexSearcher instance across multiple searches instead of creating a new one per-search. If your index has changed and you wish to see the changes reflected in searching, you should use DirectoryReader.openIfChanged(DirectoryReader) to obtain a new reader and then create a new IndexSearcher from that. Also, for low-latency turnaround it’s best to use a near-real-time reader (DirectoryReader.open(IndexWriter)). Once you have a new IndexReader, it’s relatively cheap to create a new IndexSearcher from it.NOTE: IndexSearcher instances are completely thread safe, meaning multiple threads can call any of its methods, concurrently. If your application requires external synchronization, you should not synchronize on the IndexSearcher instance; use your own (non-Lucene) objects instead. 从这段说明中得出如下几点 提供了对单个IndexReader的查询实现 通常应用程序只需要调用search(Query, int)或者search(Query, Filter, int)方法 如果你的索引不变，在多个搜索中应该采用共享一个IndexSearcher实例的方式 如果索引有变动，并且你希望在搜索中有所体现，那么应该使用DirectoryReader.openIfChanged(DirectoryReader)来获取新的reader，然后通过这个reader创建一个新的IndexSearcher 为了低延迟查询，最好使用近实时搜索（NRT），此时构建IndexSearcher需要使用DirectoryReader.open(IndexWriter)，一旦你获取一个新的IndexReader，再去创建一个IndexSearcher所付出的代价要小的多 IndexSearcher实例是完全线程安全的，这意味着多个线程可以并发调用任何方法。如果需要外部同步，无需对IndexSearcher实例进行同步 Lucene的多样化查询 通过项进行搜索-TermQuery类 在指定的项范围内搜索-TermRangeQuery类 通过字符串搜索-PrefixQuery类 组合查询-BooleanQuery类 通过短语搜索-PhraseQuery类 通配符查询-WildcardQuery类 搜索类似项-FuzzyQuery类 匹配所有文档-MatchAllDocsQuery类 不匹配文档-MatchNoDocsQuery类 解析查询表达式-QueryParser类 多短语查询-MultiPhraseQuery类 一些简单的查询示例如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.StringField;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.RAMDirectory;import org.apache.lucene.util.BytesRef;import org.junit.Assert;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.io.StringReader;public class IndexSearchDemo &#123; private Directory directory = new RAMDirectory(); private String[] ids = &#123;"1", "2"&#125;; private String[] countries = &#123;"Netherlands", "Italy"&#125;; private String[] contents = &#123;"Amsterdam has lots of bridges", "Venice has lots of canals, not bridges"&#125;; private String[] cities = &#123;"Amsterdam", "Venice"&#125;; private IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new WhitespaceAnalyzer()); private IndexWriter indexWriter; @Before public void createIndex() &#123; try &#123; indexWriter = new IndexWriter(directory, indexWriterConfig); for (int i = 0; i &lt; 2; i++) &#123; Document document = new Document(); Field idField = new StringField("id", ids[i], Field.Store.YES); Field countryField = new StringField("country", countries[i], Field.Store.YES); Field contentField = new TextField("content", contents[i], Field.Store.NO); Field cityField = new StringField("city", cities[i], Field.Store.YES); document.add(idField); document.add(countryField); document.add(contentField); document.add(cityField); indexWriter.addDocument(document); &#125; indexWriter.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Test public void testTermQuery() throws IOException &#123; Term term = new Term("id", "2"); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(new TermQuery(term), 10); Assert.assertEquals(1, search.totalHits); &#125; @Test public void testMatchNoDocsQuery() throws IOException &#123; Query query = new MatchNoDocsQuery(); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(0, search.totalHits); &#125; @Test public void testTermRangeQuery() throws IOException &#123; //搜索起始字母范围从A到Z的city Query query = new TermRangeQuery("city", new BytesRef("A"), new BytesRef("Z"), true, true); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(2, search.totalHits); &#125; @Test public void testQueryParser() throws ParseException, IOException &#123; //使用WhitespaceAnalyzer分析器不会忽略大小写，也就是说大小写敏感 QueryParser queryParser = new QueryParser("content", new WhitespaceAnalyzer()); Query query = queryParser.parse("+lots +has"); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(query, 1); Assert.assertEquals(2, search.totalHits); query = queryParser.parse("lots OR bridges"); search = indexSearcher.search(query, 10); Assert.assertEquals(2, search.totalHits); //有点需要注意，在QueryParser解析通配符表达式的时候，一定要用引号包起来，然后作为字符串传递给parse函数 query = new QueryParser("field", new StandardAnalyzer()).parse("\"This is some phrase*\""); Assert.assertEquals("analyzed", "\"? ? some phrase\"", query.toString("field")); //语法参考：http://lucene.apache.org/core/6_0_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package_description //使用QueryParser解析"~"，~代表编辑距离，~后面参数的取值在0-2之间，默认值是2，不要使用浮点数 QueryParser parser = new QueryParser("city", new WhitespaceAnalyzer()); //例如，roam~，该查询会匹配foam和roams，如果~后不跟参数，则默认值是2 //QueryParser在解析的时候不区分大小写（会全部转成小写字母），所以虽少了一个字母，但是首字母被解析为小写的v，依然不匹配，所以编辑距离是2 query = parser.parse("Venic~2"); search = indexSearcher.search(query, 10); Assert.assertEquals(1, search.totalHits); &#125; @Test public void testBooleanQuery() throws IOException &#123; Query termQuery = new TermQuery(new Term("country", "Beijing")); Query termQuery1 = new TermQuery(new Term("city", "Venice")); //测试OR查询，或者出现Beijing或者出现Venice BooleanQuery build = new BooleanQuery.Builder().add(termQuery, BooleanClause.Occur.SHOULD).add(termQuery1, BooleanClause.Occur.SHOULD).build(); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(build, 10); Assert.assertEquals(1, search.totalHits); //使用BooleanQuery实现 国家是(Italy OR Netherlands) AND contents中包含(Amsterdam)操作 BooleanQuery build1 = new BooleanQuery.Builder().add(new TermQuery(new Term("country", "Italy")), BooleanClause.Occur.SHOULD).add(new TermQuery (new Term("country", "Netherlands")), BooleanClause.Occur.SHOULD).build(); BooleanQuery build2 = new BooleanQuery.Builder().add(build1, BooleanClause.Occur.MUST).add(new TermQuery(new Term("content", "Amsterdam")), BooleanClause.Occur .MUST).build(); search = indexSearcher.search(build2, 10); Assert.assertEquals(1, search.totalHits); &#125; @Test public void testPhraseQuery() throws IOException &#123; //设置两个短语之间的跨度为2，也就是说has和bridges之间的短语小于等于均可检索到 PhraseQuery build = new PhraseQuery.Builder().setSlop(2).add(new Term("content", "has")).add(new Term("content", "bridges")).build(); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(build, 10); Assert.assertEquals(1, search.totalHits); build = new PhraseQuery.Builder().setSlop(1).add(new Term("content", "Venice")).add(new Term("content", "lots")).add(new Term("content", "canals")).build(); search = indexSearcher.search(build, 10); Assert.assertNotEquals(1, search.totalHits); &#125; @Test public void testMatchAllDocsQuery() throws IOException &#123; Query query = new MatchAllDocsQuery(); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(2, search.totalHits); &#125; @Test public void testFuzzyQuery() throws IOException, ParseException &#123; //注意是区分大小写的，同时默认的编辑距离的值是2 //注意两个Term之间的编辑距离必须小于两者中最小者的长度：the edit distance between the terms must be less than the minimum length term Query query = new FuzzyQuery(new Term("city", "Veni")); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(1, search.totalHits); &#125; @Test public void testWildcardQuery() throws IOException &#123; //*代表0个或者多个字母 Query query = new WildcardQuery(new Term("content", "*dam")); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(1, search.totalHits); //?代表0个或者1个字母 query = new WildcardQuery(new Term("content", "?ridges")); search = indexSearcher.search(query, 10); Assert.assertEquals(2, search.totalHits); query = new WildcardQuery(new Term("content", "b*s")); search = indexSearcher.search(query, 10); Assert.assertEquals(2, search.totalHits); &#125; @Test public void testPrefixQuery() throws IOException &#123; //使用前缀搜索以It打头的国家名，显然只有Italy符合 PrefixQuery query = new PrefixQuery(new Term("country", "It")); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(1, search.totalHits); &#125; private IndexSearcher getIndexSearcher() throws IOException &#123; return new IndexSearcher(DirectoryReader.open(directory)); &#125; @Test public void testToken() throws IOException &#123; Analyzer analyzer = new StandardAnalyzer(); TokenStream tokenStream = analyzer.tokenStream("myfield", new StringReader("Some text content for my test!")); OffsetAttribute offsetAttribute = tokenStream.addAttribute(OffsetAttribute.class); tokenStream.reset(); while (tokenStream.incrementToken()) &#123; System.out.println("token: " + tokenStream.reflectAsString(true).toString()); System.out.println("token start offset: " + offsetAttribute.startOffset()); System.out.println("token end offset: " + offsetAttribute.endOffset()); &#125; &#125; @Test public void testMultiPhraseQuery() throws IOException &#123; Term[] terms = new Term[]&#123;new Term("content", "has"), new Term("content", "lots")&#125;; Term term2 = new Term("content", "bridges"); //多个add之间认为是OR操作，即(has lots)和bridges之间的slop不大于3，不计算标点 MultiPhraseQuery multiPhraseQuery = new MultiPhraseQuery.Builder().add(terms).add(term2).setSlop(3).build(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); TopDocs search = indexSearcher.search(multiPhraseQuery, 10); Assert.assertEquals(2, search.totalHits); &#125; //使用BooleanQuery类模拟MultiPhraseQuery类的功能 @Test public void testBooleanQueryImitateMultiPhraseQuery() throws IOException &#123; PhraseQuery first = new PhraseQuery.Builder().setSlop(3).add(new Term("content", "Amsterdam")).add(new Term("content", "bridges")) .build(); PhraseQuery second = new PhraseQuery.Builder().setSlop(1).add(new Term("content", "Venice")).add(new Term("content", "lots")).build(); BooleanQuery booleanQuery = new BooleanQuery.Builder().add(first, BooleanClause.Occur.SHOULD).add(second, BooleanClause.Occur.SHOULD).build(); IndexSearcher indexSearcher = getIndexSearcher(); TopDocs search = indexSearcher.search(booleanQuery, 10); Assert.assertEquals(2, search.totalHits); &#125;&#125; 有关QueryParser的详细语法请参考这里。 Lucene的高级搜索技术Lucene包含了一个建立在SpanQuery类基础上的整套查询体系，大致反映了Lucene的Query类体系。SpanQuery是指域中的起始语汇单元和终止语汇单元的位置。SpanQuery有一些常用的子类，如下所示 SpanQuery类型 描述 FieldMaskingSpanQuery 用于在多个域之间查询，即把另一个域看作某个域，从而看起来就像在同一个域里查询，因为 Lucene 默认某个条件只能作用在单个域上，不支持跨域查询，只能在同一个域里查询，所以有了 FieldMaskingSpanQuery SpanTermQuery 和其它跨度查询类型结合使用，单独使用时相当于 TermQuery，唯一的区别就是使用 SpanTermQuery 可以得到 Term 的 Span 跨度信息 SpanNearQuery 用来匹配两个 Term 之间的跨度的，即一个 Term 经过几个跨度可以到达另一个 Term，slop 为跨度因子，用来限制两个 Term 之间的最大跨度。还有一个 inOrder 参数，它用来设置是否允许进行倒序跨度，什么意思？即 TermA 到 TermB 不一定是从左到右去匹配也可以从右到左，而从右到左就是倒序，inOrder 为 true 即表示 order（顺序）很重要不能倒序去匹配必须正向去匹配，false 则反之。注意停用词不在 slop 统计范围内 SpanFirstQuery 表示对出现在一个域中的[0, n]范围内的 term 项进行的匹配查询，关键是n指定了查询的 term 出现范围的上限 SpanContainingQuery 返回在另一个范围内的查询匹配结果，big 和 little 的子句可以是任何 span 类型查询。在包含 little 匹配中从 big 匹配跨度返回。例如“a beautiful and boring world”，big 查询是 SpanNearQuery(SpanTermQuery(“beautiful”), SpanTermQuery(“world”)).setSlop(2)，而 little 查询是 SpanTermQuery(“boring”)，则该 Doc 命中，并从 big 匹配跨度返回，即 big 优先级高 SpanWithinQuery 与 SpanContainingQuery 类似，只不过是从 little 匹配跨度返回，换句话说，SpanContainingQuery 是 big 查询优先级更高，而 SpanWithinQuery 是 little 查询优先级更高。也许英文说的更清晰，In clear, the SpanContainingQuery will keep matches that contain another Spans, while the SpanWithinQuery will keep matches that are contained within another Spans. SpanNotQuery 使用场景是当使用 SpanNearQuery 时，如果两个 Term 从 TermA 到 TermB 有多种情况，即可能出现 TermA 或者 TermB 在索引中重复出现，则可能有多种情况，SpanNotQuery 就是用来限制 TermA 和 TermB 之间不存在 TermC，从而排除一些情况，实现更精确的控制 SpanOrQuery 这个查询会嵌套一些子查询，子查询之间的逻辑关系为或 使用示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.StringField;import org.apache.lucene.document.TextField;import org.apache.lucene.index.*;import org.apache.lucene.search.*;import org.apache.lucene.search.spans.*;import org.apache.lucene.store.RAMDirectory;import org.junit.After;import org.junit.Assert;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.io.StringReader;public class SpanQueryDemo &#123; private RAMDirectory directory; private IndexSearcher indexSearcher; private IndexReader indexReader; private SpanTermQuery quick; private SpanTermQuery brown; private SpanTermQuery red; private SpanTermQuery fox; private SpanTermQuery lazy; private SpanTermQuery sleepy; private SpanTermQuery dog; private SpanTermQuery cat; private Analyzer analyzer; private IndexWriter indexWriter; private IndexWriterConfig indexWriterConfig; @Before public void setUp() throws IOException &#123; directory = new RAMDirectory(); analyzer = new WhitespaceAnalyzer(); indexWriterConfig = new IndexWriterConfig(analyzer); indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND); indexWriter = new IndexWriter(directory, indexWriterConfig); Document document = new Document(); //TextField使用Store.YES时索引文档、频率、位置信息 document.add(new TextField("f", "What's amazing, the quick brown fox jumps over the lazy dog", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("f", "Wow! the quick red fox jumps over the sleepy cat", Field.Store.YES)); indexWriter.addDocument(document); indexWriter.commit(); indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); indexReader = indexSearcher.getIndexReader(); quick = new SpanTermQuery(new Term("f", "quick")); brown = new SpanTermQuery(new Term("f", "brown")); red = new SpanTermQuery(new Term("f", "red")); fox = new SpanTermQuery(new Term("f", "fox")); lazy = new SpanTermQuery(new Term("f", "lazy")); dog = new SpanTermQuery(new Term("f", "dog")); sleepy = new SpanTermQuery(new Term("f", "sleepy")); cat = new SpanTermQuery(new Term("f", "cat")); &#125; @After public void setDown() &#123; if (indexWriter != null &amp;&amp; indexWriter.isOpen()) &#123; try &#123; indexWriter.close(); &#125; catch (IOException e) &#123; System.out.println(e); &#125; &#125; &#125; private void assertOnlyBrownFox(Query query) throws IOException &#123; TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(1, search.totalHits); Assert.assertEquals("wrong doc", 0, search.scoreDocs[0].doc); &#125; private void assertBothFoxes(Query query) throws IOException &#123; TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(2, search.totalHits); &#125; private void assertNoMatches(Query query) throws IOException &#123; TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(0, search.totalHits); &#125; /** * 输出跨度查询的结果 * * @param query * @throws IOException */ private void dumpSpans(SpanQuery query) throws IOException &#123; SpanWeight weight = query.createWeight(indexSearcher, true); Spans spans = weight.getSpans(indexReader.getContext().leaves().get(0), SpanWeight.Postings.POSITIONS); System.out.println(query); TopDocs search = indexSearcher.search(query, 10); float[] scores = new float[2]; for (ScoreDoc sd : search.scoreDocs) &#123; scores[sd.doc] = sd.score; &#125; int numSpans = 0; //处理所有跨度 while (spans.nextDoc() != Spans.NO_MORE_DOCS) &#123; while (spans.nextStartPosition() != Spans.NO_MORE_POSITIONS) &#123; numSpans++; int id = spans.docID(); //检索文档 Document doc = indexReader.document(id); //重新分析文本 TokenStream stream = analyzer.tokenStream("contents", new StringReader(doc.get("f"))); stream.reset(); OffsetAttribute offsetAttribute = stream.addAttribute(OffsetAttribute.class); int i = 0; StringBuilder sb = new StringBuilder(); sb.append(" "); //处理所有语汇单元 while (stream.incrementToken()) &#123; if (i == spans.startPosition()) &#123; sb.append("&lt;"); &#125; sb.append(offsetAttribute.toString()); if (i + 1 == spans.endPosition()) &#123; sb.append("&gt;"); &#125; sb.append(" "); i++; &#125; sb.append("(").append(scores[id]).append(")"); System.out.println(sb.toString()); stream.close(); &#125; if (numSpans == 0) &#123; System.out.println(" No Spans"); &#125; &#125; &#125; @Test public void testSpanTermQuery() throws IOException &#123; assertOnlyBrownFox(brown); dumpSpans(brown); dumpSpans(new SpanTermQuery(new Term("f", "the"))); dumpSpans(new SpanTermQuery(new Term("f", "fox"))); &#125; /** * SpanFirstQuery可以对出现在域中前面某位置的跨度进行查询 */ @Test public void testSpanFirstQuery() throws IOException &#123; SpanFirstQuery spanFirstQuery = new SpanFirstQuery(brown, 2); assertNoMatches(spanFirstQuery); dumpSpans(spanFirstQuery); //设置从前面开始10个跨度查找brown，一个切分结果是一个跨度，使用的是空格分词器 spanFirstQuery = new SpanFirstQuery(brown, 5); dumpSpans(spanFirstQuery); assertOnlyBrownFox(spanFirstQuery); &#125; @Test public void testSpanNearQuery() throws IOException &#123; SpanQuery[] queries = new SpanQuery[]&#123;quick, brown, dog&#125;; SpanNearQuery spanNearQuery = new SpanNearQuery(queries, 0, true); assertNoMatches(spanNearQuery); dumpSpans(spanNearQuery); spanNearQuery = new SpanNearQuery(queries, 4, true); assertNoMatches(spanNearQuery); dumpSpans(spanNearQuery); spanNearQuery = new SpanNearQuery(queries, 5, true); assertOnlyBrownFox(spanNearQuery); dumpSpans(spanNearQuery); //将inOrder设置为false，那么就不会考虑顺序，只要两者中间不超过三个就可以检索到 spanNearQuery = new SpanNearQuery(new SpanQuery[]&#123;lazy, fox&#125;, 3, false); assertOnlyBrownFox(spanNearQuery); dumpSpans(spanNearQuery); //在考虑顺序的情况下，是检索不到的 spanNearQuery = new SpanNearQuery(new SpanQuery[]&#123;lazy, fox&#125;, 3, true); assertNoMatches(spanNearQuery); dumpSpans(spanNearQuery); PhraseQuery phraseQuery = new PhraseQuery.Builder().add(new Term("f", "lazy")).add(new Term("f", "fox")).setSlop(4).build(); assertNoMatches(phraseQuery); PhraseQuery phraseQuery1 = new PhraseQuery.Builder().setSlop(5).add(phraseQuery.getTerms()[0]).add(phraseQuery.getTerms()[1]).build(); assertOnlyBrownFox(phraseQuery1); &#125; @Test public void testSpanNotQuery() throws IOException &#123; //设置slop为1 SpanNearQuery quickFox = new SpanNearQuery(new SpanQuery[]&#123;quick, fox&#125;, 1, true); assertBothFoxes(quickFox); dumpSpans(quickFox); //第一个参数表示要包含的跨度对象，第二个参数则表示要排除的跨度对象 SpanNotQuery quickFoxDog = new SpanNotQuery(quickFox, dog); assertBothFoxes(quickFoxDog); dumpSpans(quickFoxDog); //下面把red排除掉，那么就只能查到一条记录 SpanNotQuery noQuickRedFox = new SpanNotQuery(quickFox, red); assertOnlyBrownFox(noQuickRedFox); dumpSpans(noQuickRedFox); &#125; @Test public void testSpanOrQuery() throws IOException &#123; SpanNearQuery quickFox = new SpanNearQuery(new SpanQuery[]&#123;quick, fox&#125;, 1, true); SpanNearQuery lazyDog = new SpanNearQuery(new SpanQuery[]&#123;lazy, dog&#125;, 0, true); SpanNearQuery sleepyCat = new SpanNearQuery(new SpanQuery[]&#123;sleepy, cat&#125;, 0, true); SpanNearQuery quickFoxNearLazyDog = new SpanNearQuery(new SpanQuery[]&#123;quickFox, lazyDog&#125;, 3, true); assertOnlyBrownFox(quickFoxNearLazyDog); dumpSpans(quickFoxNearLazyDog); SpanNearQuery quickFoxNearSleepyCat = new SpanNearQuery(new SpanQuery[]&#123;quickFox, sleepyCat&#125;, 3, true); dumpSpans(quickFoxNearSleepyCat); SpanOrQuery or = new SpanOrQuery(new SpanQuery[]&#123;quickFoxNearLazyDog, quickFoxNearSleepyCat&#125;); assertBothFoxes(or); dumpSpans(or); &#125; /** * 测试安全过滤 * * @throws IOException */ @Test public void testSecurityFilter() throws IOException &#123; Document document = new Document(); document.add(new StringField("owner", "eric", Field.Store.YES)); document.add(new TextField("keywords", "A B of eric", Field.Store.YES)); indexWriter.addDocument(document); document = new Document(); document.add(new TextField("owner", "jobs", Field.Store.YES)); document.add(new TextField("keywords", "A B of jobs", Field.Store.YES)); indexWriter.addDocument(document); document.add(new TextField("owner", "jack", Field.Store.YES)); document.add(new TextField("keywords", "A B of jack", Field.Store.YES)); indexWriter.addDocument(document); indexWriter.commit(); TermQuery termQuery = new TermQuery(new Term("owner", "eric")); TermQuery termQuery1 = new TermQuery(new Term("keywords", "A")); //把FILTER看做是Like即可，也就是说owner必须是eric的才允许检索到 Query query = new BooleanQuery.Builder().add(termQuery1, BooleanClause.Occur.MUST).add(termQuery, BooleanClause.Occur.FILTER).build(); System.out.println(query); indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); TopDocs search = indexSearcher.search(query, 10); Assert.assertEquals(1, search.totalHits); //如果不加安全过滤的话，那么应该检索到三条记录 query = new BooleanQuery.Builder().add(termQuery1, BooleanClause.Occur.MUST).build(); System.out.println(query); search = indexSearcher.search(query, 10); Assert.assertEquals(3, search.totalHits); String keywords = indexSearcher.doc(search.scoreDocs[0].doc).get("keywords"); System.out.println("使用Filter查询：" + keywords); //使用BooleanQuery可以实现同样的功能 BooleanQuery booleanQuery = new BooleanQuery.Builder().add(termQuery, BooleanClause.Occur.MUST).add(termQuery1, BooleanClause.Occur.MUST).build(); search = indexSearcher.search(booleanQuery, 10); Assert.assertEquals(1, search.totalHits); System.out.println("使用BooleanQuery查询：" + indexSearcher.doc(search.scoreDocs[0].doc).get("keywords")); &#125;&#125; 禁用模糊查询和通配符查询有时候，可能不希望用户使用模糊查询或者通配符查询，这时候可以通过实现自己的QueryParser达到禁用的目的。例如12345678910111213141516171819202122232425262728293031323334353637import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.Query;/** * Description:禁用模糊查询和通配符查询，同样的如果希望禁用其它类型查询，只需要覆写对应的getXXXQuery方法即可 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */public class CustomQueryParser extends QueryParser &#123; public CustomQueryParser(String f, Analyzer a) &#123; super(f, a); &#125; @Override protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException &#123; throw new ParseException("Fuzzy queries not allowed!"); &#125; @Override protected Query getWildcardQuery(String field, String termStr) throws ParseException &#123; throw new ParseException("Wildcard queries not allowed!"); &#125; public static void main(String args[]) throws ParseException &#123; CustomQueryParser customQueryParser = new CustomQueryParser("filed", new WhitespaceAnalyzer()); customQueryParser.parse("a?t"); customQueryParser.parse("junit~"); &#125;&#125; 多索引的搜索合并方法如果存在多索引文件，需要如何搜索并合并搜索结果呢？此时需要使用MultiReader，通过该类的构造函数MultiReader(IndexReader… subReaders)可以接收多个IndexReader实例，而一个IndexReader实例对应一个索引文件目录，之后用该MultiReader实例初始化IndexSearcher。 注意，IndexReader实例是完全线程安全的，多线程可以调用其任何方法，而无需对IndexReader实例进行同步。如果你的应用需要外部同步操作，对你自己的对象进行同步，而不是Lucene的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.StringField;import org.apache.lucene.index.*;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TermRangeQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.RAMDirectory;import org.apache.lucene.util.BytesRef;import org.junit.After;import org.junit.Assert;import org.junit.Before;import org.junit.Test;import java.io.IOException;/** * Description:测试有多个索引文件的情况下，如何进行搜索并合并搜索结果 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */public class MultiReaderDemo &#123; private Analyzer analyzer = new WhitespaceAnalyzer(); Directory aDirectory = new RAMDirectory(); Directory bDirectory = new RAMDirectory(); IndexWriterConfig aIndexWriterConfig = new IndexWriterConfig(analyzer); IndexWriterConfig bIndexWriterConfig = new IndexWriterConfig(analyzer); IndexWriter aIndexWriter; IndexWriter bIndexWriter; @Before public void setUp() throws IOException &#123; String[] animals = &#123;"a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z"&#125;; aIndexWriter = new IndexWriter(aDirectory, aIndexWriterConfig); bIndexWriter = new IndexWriter(bDirectory, bIndexWriterConfig); for (int i = 0; i &lt; animals.length; i++) &#123; Document document = new Document(); String animal = animals[i]; document.add(new StringField("animal", animal, Field.Store.YES)); if (animal.charAt(0) &lt; 'n') &#123; aIndexWriter.addDocument(document); &#125; else &#123; bIndexWriter.addDocument(document); &#125; &#125; aIndexWriter.commit(); bIndexWriter.commit(); &#125; @After public void setDown() &#123; try &#123; if (aIndexWriter != null &amp;&amp; aIndexWriter.isOpen()) &#123; aIndexWriter.close(); &#125; if (bIndexWriter != null &amp;&amp; bIndexWriter.isOpen()) &#123; bIndexWriter.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Test public void testMultiReader() throws IOException &#123; IndexReader aIndexReader = DirectoryReader.open(aDirectory); IndexReader bIndexReader = DirectoryReader.open(bDirectory); MultiReader multiReader = new MultiReader(aIndexReader, bIndexReader); IndexSearcher indexSearcher = new IndexSearcher(multiReader); TopDocs animal = indexSearcher.search(new TermRangeQuery("animal", new BytesRef("h"), new BytesRef("q"), true, true), 10); Assert.assertEquals(10, animal.totalHits); ScoreDoc[] scoreDocs = animal.scoreDocs; for (ScoreDoc sd : scoreDocs) &#123; System.out.println(indexSearcher.doc(sd.doc)); &#125; &#125;&#125; 输出结果如下12345678910Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:h&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:i&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:j&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:k&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:l&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:m&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:n&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:o&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:p&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;animal:q&gt;&gt;]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 实战（4）-文本分析器]]></title>
    <url>%2F2016%2F05%2F23%2FLucene-6-0-in-action-4-The-text-analyzer%2F</url>
    <content type="text"><![CDATA[Analyzer简介在Lucene的org.apache.lucene.analysis模块中提供了顶层的抽象类Analyzer，Analyzer主要是用来构建TokenStreams，如果想实现自定义的Analyzer，必须覆写createComponents(String)方法，并定义自己的TokenStreamComponents。 为什么要有Analyzer呢？对于Lucene而言，不管是索引还是检索，都是针对纯文本而言，对于纯文本的来源可以是PDF，Word，Excel，PPT，HTML等，Lucene对此并不关心，只要保证传递给Lucene的是纯文本即可。 而通常情况下，对于大量的文本，用户在检索的时候不可能全部输入，例如有文本“Hello World，I’am javaer!”，用户在检索的时候可能只是输入了“Hello”，这里就需要Lucene索引该文本的时候预先对文本进行切分，这样在检索的时候才能将词源和文本对应起来。 Lucene常用分析器整理如下 分析器 说明 WhitespaceAnalyzer 根据空格拆分语汇单元 SimpleAnalyzer 根据非字母拆分文本，并将其转换为小写形式 StopAnalyzer 根据非字母拆分文本，然后小写化，再移除停用词 KeywordAnalyzer 将整个文本作为一个单一语汇单元处理 StandardAnalyzer 根据Unicode文本分割算法，具体算法参考Unicode Standard Annex #29，然后将文本转化为小写，并移除英文停用词 SmartChineseAnalyzer SmartChineseAnalyzer 是一个智能中文分词模块，能够利用概率对汉语句子进行最优切分，并内嵌英文 tokenizer，能有效处理中英文混合的文本内容。它的原理基于自然语言处理领域的隐马尔科夫模型（HMM），利用大量语料库的训练来统计汉语词汇的词频和跳转概率，从而根据这些统计结果对整个汉语句子计算最似然（likelihood）的切分。因为智能分词需要词典来保存词汇的统计值，SmartChineseAnalyzer 的运行需要指定词典位置，如何指定词典位置请参考org.apache.lucene.analysis.cn.smart.AnalyzerProfile。SmartChineseAnalyzer 的算法和语料库词典来自于ICTCLAS CJKAnalyzer CJK表示中日韩，目的是要把分别来自中文、日文、韩文、越文中，本质、意义相同、形状一样或稍异的表意文字（主要为汉字，但也有仿汉字如日本国字、韩国独有汉字、越南的喃字）在ISO 10646 及 Unicode 标准内赋予相同编码。对于中文是交叉双字分割，二元分词法 Analyzer部分子类分词示例选取了六个实现类，并分别输出它们对英文、中文、特殊符号及邮箱等的切分效果。123456789101112131415161718192021222324252627282930public class AnalyzerDemo &#123; private static final String[] examples = &#123;"The quick brown 1234 fox jumped over the lazy dog!", "XY&amp;Z 15.6 Corporation - xyz@example.com", "北京市北京大学"&#125;; private static final Analyzer[] ANALYZERS = new Analyzer[]&#123;new WhitespaceAnalyzer(), new SimpleAnalyzer(), new StopAnalyzer(), new StandardAnalyzer(), new CJKAnalyzer(), new SmartChineseAnalyzer()&#125;; @Test public void testAnalyzer() throws IOException &#123; for (int i = 0; i &lt; ANALYZERS.length; i++) &#123; String simpleName = ANALYZERS[i].getClass().getSimpleName(); for (int j = 0; j &lt; examples.length; j++) &#123; TokenStream contents = ANALYZERS[i].tokenStream("contents", examples[j]); //TokenStream contents = ANALYZERS[i].tokenStream("contents", new StringReader(examples[j])); OffsetAttribute offsetAttribute = contents.addAttribute(OffsetAttribute.class); TypeAttribute typeAttribute = contents.addAttribute(TypeAttribute.class); contents.reset(); System.out.println(simpleName + " analyzing : " + examples[j]); while (contents.incrementToken()) &#123; String s1 = offsetAttribute.toString(); int i1 = offsetAttribute.startOffset();//起始偏移量 int i2 = offsetAttribute.endOffset();//结束偏移量 System.out.print(s1 + "[" + i1 + "," + i2 + ":" + typeAttribute.type() + "]" + " "); &#125; contents.end(); contents.close(); System.out.println(); &#125; &#125; &#125;&#125; 输出结果如下123456789101112131415161718192021222324252627282930313233343536WhitespaceAnalyzer analyzing : The quick brown 1234 fox jumped over the lazy dog!The[0,3:word] quick[4,9:word] brown[10,15:word] 1234[16,20:word] fox[21,24:word] jumped[25,31:word] over[32,36:word] the[37,40:word] lazy[41,45:word] dog![46,50:word] WhitespaceAnalyzer analyzing : XY&amp;Z 15.6 Corporation - xyz@example.comXY&amp;Z[0,4:word] 15.6[5,9:word] Corporation[10,21:word] -[22,23:word] xyz@example.com[24,39:word] WhitespaceAnalyzer analyzing : 北京市北京大学北京市北京大学[0,7:word] SimpleAnalyzer analyzing : The quick brown 1234 fox jumped over the lazy dog!the[0,3:word] quick[4,9:word] brown[10,15:word] fox[21,24:word] jumped[25,31:word] over[32,36:word] the[37,40:word] lazy[41,45:word] dog[46,49:word] SimpleAnalyzer analyzing : XY&amp;Z 15.6 Corporation - xyz@example.comxy[0,2:word] z[3,4:word] corporation[10,21:word] xyz[24,27:word] example[28,35:word] com[36,39:word] SimpleAnalyzer analyzing : 北京市北京大学北京市北京大学[0,7:word] StopAnalyzer analyzing : The quick brown 1234 fox jumped over the lazy dog!quick[4,9:word] brown[10,15:word] fox[21,24:word] jumped[25,31:word] over[32,36:word] lazy[41,45:word] dog[46,49:word] StopAnalyzer analyzing : XY&amp;Z 15.6 Corporation - xyz@example.comxy[0,2:word] z[3,4:word] corporation[10,21:word] xyz[24,27:word] example[28,35:word] com[36,39:word] StopAnalyzer analyzing : 北京市北京大学北京市北京大学[0,7:word] StandardAnalyzer analyzing : The quick brown 1234 fox jumped over the lazy dog!quick[4,9:&lt;ALPHANUM&gt;] brown[10,15:&lt;ALPHANUM&gt;] 1234[16,20:&lt;NUM&gt;] fox[21,24:&lt;ALPHANUM&gt;] jumped[25,31:&lt;ALPHANUM&gt;] over[32,36:&lt;ALPHANUM&gt;] lazy[41,45:&lt;ALPHANUM&gt;] dog[46,49:&lt;ALPHANUM&gt;] StandardAnalyzer analyzing : XY&amp;Z 15.6 Corporation - xyz@example.comxy[0,2:&lt;ALPHANUM&gt;] z[3,4:&lt;ALPHANUM&gt;] 15.6[5,9:&lt;NUM&gt;] corporation[10,21:&lt;ALPHANUM&gt;] xyz[24,27:&lt;ALPHANUM&gt;] example.com[28,39:&lt;ALPHANUM&gt;] StandardAnalyzer analyzing : 北京市北京大学北[0,1:&lt;IDEOGRAPHIC&gt;] 京[1,2:&lt;IDEOGRAPHIC&gt;] 市[2,3:&lt;IDEOGRAPHIC&gt;] 北[3,4:&lt;IDEOGRAPHIC&gt;] 京[4,5:&lt;IDEOGRAPHIC&gt;] 大[5,6:&lt;IDEOGRAPHIC&gt;] 学[6,7:&lt;IDEOGRAPHIC&gt;] CJKAnalyzer analyzing : The quick brown 1234 fox jumped over the lazy dog!quick[4,9:&lt;ALPHANUM&gt;] brown[10,15:&lt;ALPHANUM&gt;] 1234[16,20:&lt;NUM&gt;] fox[21,24:&lt;ALPHANUM&gt;] jumped[25,31:&lt;ALPHANUM&gt;] over[32,36:&lt;ALPHANUM&gt;] lazy[41,45:&lt;ALPHANUM&gt;] dog[46,49:&lt;ALPHANUM&gt;] CJKAnalyzer analyzing : XY&amp;Z 15.6 Corporation - xyz@example.comxy[0,2:&lt;ALPHANUM&gt;] z[3,4:&lt;ALPHANUM&gt;] 15.6[5,9:&lt;NUM&gt;] corporation[10,21:&lt;ALPHANUM&gt;] xyz[24,27:&lt;ALPHANUM&gt;] example.com[28,39:&lt;ALPHANUM&gt;] CJKAnalyzer analyzing : 北京市北京大学北京[0,2:&lt;DOUBLE&gt;] 京市[1,3:&lt;DOUBLE&gt;] 市北[2,4:&lt;DOUBLE&gt;] 北京[3,5:&lt;DOUBLE&gt;] 京大[4,6:&lt;DOUBLE&gt;] 大学[5,7:&lt;DOUBLE&gt;] SmartChineseAnalyzer analyzing : The quick brown 1234 fox jumped over the lazy dog!the[0,3:word] quick[4,9:word] brown[10,15:word] 1234[16,20:word] fox[21,24:word] jump[25,31:word] over[32,36:word] the[37,40:word] lazi[41,45:word] dog[46,49:word] SmartChineseAnalyzer analyzing : XY&amp;Z 15.6 Corporation - xyz@example.comxy[0,2:word] z[3,4:word] 15[5,7:word] 6[8,9:word] corpor[10,21:word] xyz[24,27:word] exampl[28,35:word] com[36,39:word] SmartChineseAnalyzer analyzing : 北京市北京大学北京市[0,3:word] 北京大学[3,7:word] Analyzer之TokenStreamTokenStream是分析处理组件中的一种中间数据格式，它从一个reader中获取文本，并以TokenStream作为输出结果。在所有的过滤器中，TokenStream同时充当着输入和输出格式。Tokenizer和TokenFilter继承自TokenStream，Tokenizer是一个TokenStream，其输入源是一个Reader；TokenFilter也是一个TokenStream，其输入源是另一个TokenStream。而TokenStream简单点说就是生成器的输出结果。TokenStream是一个分词后的Token结果组成的流，通过流能够不断的得到下一个Token。1234567891011121314@Testpublic void testTokenStream() throws IOException &#123; Analyzer analyzer = new WhitespaceAnalyzer(); String inputText = "This is a test text for token!"; TokenStream tokenStream = analyzer.tokenStream("text", new StringReader(inputText)); //保存token字符串 CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class); //在调用incrementToken()开始消费token之前需要重置stream到一个干净的状态 tokenStream.reset(); while (tokenStream.incrementToken()) &#123; //打印分词结果 System.out.print("[" + charTermAttribute + "]"); &#125;&#125; 输出结果： [This][is][a][test][text][for][token!] Analyzer之TokenAttribute在调用tokenStream()方法之后，我们可以通过为之添加多个Attribute，从而可以了解到分词之后详细的词元信息，比如CharTermAttribute用于保存词元的内容，TypeAttribute用于保存词元的类型。 在Lucene中提供了几种类型的Attribute，每种类型的Attribute提供一个不同的方面或者token的元数据，罗列如下 名称 作用 CharTermAttribute 表示token本身的内容 PositionIncrementAttribute 表示当前token相对于前一个token的相对位置，也就是相隔的词语数量（例如“text for attribute”，text和attribute之间的getPositionIncrement为2），如果两者之间没有停用词，那么该值被置为默认值1 OffsetAttribute 表示token的首字母和尾字母在原文本中的位置 TypeAttribute 表示token的词汇类型信息，默认值为word，其它值有&lt;ALPHANUM&gt; &lt;APOSTROPHE&gt; &lt;ACRONYM&gt; &lt;COMPANY&gt; &lt;EMAIL&gt; &lt;HOST&gt; &lt;NUM&gt; &lt;CJ&gt; &lt;ACRONYM_DEP&gt; FlagsAttribute 与TypeAttribute类似，假设你需要给token添加额外的信息，而且希望该信息可以通过分析链，那么就可以通过flags去传递 PayloadAttribute 在每个索引位置都存储了payload（关键信息），当使用基于Payload的查询时，该信息在评分中非常有用 123456789101112131415161718192021222324@Testpublic void testAttribute() throws IOException &#123; Analyzer analyzer = new StandardAnalyzer(); String input = "This is a test text for attribute! Just add-some word."; TokenStream tokenStream = analyzer.tokenStream("text", new StringReader(input)); CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class); PositionIncrementAttribute positionIncrementAttribute = tokenStream.addAttribute(PositionIncrementAttribute.class); OffsetAttribute offsetAttribute = tokenStream.addAttribute(OffsetAttribute.class); TypeAttribute typeAttribute = tokenStream.addAttribute(TypeAttribute.class); PayloadAttribute payloadAttribute = tokenStream.addAttribute(PayloadAttribute.class); payloadAttribute.setPayload(new BytesRef("Just")); tokenStream.reset(); while (tokenStream.incrementToken()) &#123; System.out.print("[" + charTermAttribute + " increment:" + positionIncrementAttribute.getPositionIncrement() + " start:" + offsetAttribute .startOffset() + " end:" + offsetAttribute .endOffset() + " type:" + typeAttribute.type() + " payload:" + payloadAttribute.getPayload() + "]\n"); &#125; tokenStream.end(); tokenStream.close();&#125; 在调用incrementToken()结束迭代之后，调用end()和close()方法，其中end()可以唤醒当前TokenStream的处理器去做一些收尾工作，close()可以关闭TokenStream和Analyzer去释放在分析过程中使用的资源。 输出结果如下，其中停用词被分词器过滤掉了 [test increment:4 start:10 end:14 type: payload:null][text increment:1 start:15 end:19 type: payload:null][attribute increment:2 start:24 end:33 type: payload:null][just increment:1 start:35 end:39 type: payload:null][add increment:1 start:40 end:43 type: payload:null][some increment:1 start:44 end:48 type: payload:null][word increment:1 start:49 end:53 type: payload:null] Analyzer之TokenFilterTokenFilter主要用于TokenStream的过滤操作，用来处理Tokenizer或者上一个TokenFilter处理后的结果，如果是对现有分词器进行扩展或修改，推荐使用自定义TokenFilter方式。自定义TokenFilter需要实现incrementToken()抽象函数，并且该方法需要声明为final的，在此函数中对过滤Term的CharTermAttribute和PositionIncrementAttribute等属性进行操作，就能实现过滤功能，例如一个简单的词扩展过滤器如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenFilter;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.core.WhitespaceAnalyzer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.junit.Test;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class TestTokenFilter &#123; @Test public void test() throws IOException &#123; String text = "Hi, Dr Wang, Mr Liu asks if you stay with Mrs Liu yesterday!"; Analyzer analyzer = new WhitespaceAnalyzer(); CourtesyTitleFilter filter = new CourtesyTitleFilter(analyzer.tokenStream("text", text)); CharTermAttribute charTermAttribute = filter.addAttribute(CharTermAttribute.class); filter.reset(); while (filter.incrementToken()) &#123; System.out.print(charTermAttribute + " "); &#125; &#125;&#125;/** * 自定义词扩展过滤器 */class CourtesyTitleFilter extends TokenFilter &#123; Map&lt;String, String&gt; courtesyTitleMap = new HashMap&lt;&gt;(); private CharTermAttribute termAttribute; /** * Construct a token stream filtering the given input. * * @param input */ protected CourtesyTitleFilter(TokenStream input) &#123; super(input); termAttribute = addAttribute(CharTermAttribute.class); courtesyTitleMap.put("Dr", "doctor"); courtesyTitleMap.put("Mr", "mister"); courtesyTitleMap.put("Mrs", "miss"); &#125; @Override public final boolean incrementToken() throws IOException &#123; if (!input.incrementToken()) &#123; return false; &#125; String small = termAttribute.toString(); if (courtesyTitleMap.containsKey(small)) &#123; termAttribute.setEmpty().append(courtesyTitleMap.get(small)); &#125; return true; &#125;&#125; 输出结果如下 Hi, doctor Wang, mister Liu asks if you stay with miss Liu yesterday! 自定义Analyzer实现扩展停用词 继承自Analyzer并覆写createComponents(String)方法 维护自己的停用词词典 重写TokenStreamComponents，选择合适的过滤策略 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class StopAnalyzerExtend extends Analyzer &#123; private CharArraySet stopWordSet;//停止词词典 public CharArraySet getStopWordSet() &#123; return this.stopWordSet; &#125; public void setStopWordSet(CharArraySet stopWordSet) &#123; this.stopWordSet = stopWordSet; &#125; public StopAnalyzerExtend() &#123; super(); setStopWordSet(StopAnalyzer.ENGLISH_STOP_WORDS_SET); &#125; /** * @param stops 需要扩展的停止词 */ public StopAnalyzerExtend(List&lt;String&gt; stops) &#123; this(); /**如果直接为stopWordSet赋值的话，会报如下异常，这是因为在StopAnalyzer中有ENGLISH_STOP_WORDS_SET = CharArraySet.unmodifiableSet(stopSet); * ENGLISH_STOP_WORDS_SET 被设置为不可更改的set集合 * Exception in thread "main" java.lang.UnsupportedOperationException * at org.apache.lucene.analysis.util.CharArrayMap$UnmodifiableCharArrayMap.put(CharArrayMap.java:592) * at org.apache.lucene.analysis.util.CharArraySet.add(CharArraySet.java:105) * at java.util.AbstractCollection.addAll(AbstractCollection.java:344) * at MyAnalyzer.&lt;init&gt;(AnalyzerDemo.java:146) * at MyAnalyzer.main(AnalyzerDemo.java:162) */ //stopWordSet = getStopWordSet(); stopWordSet = CharArraySet.copy(getStopWordSet()); stopWordSet.addAll(StopFilter.makeStopSet(stops)); &#125; @Override protected TokenStreamComponents createComponents(String fieldName) &#123; Tokenizer source = new LowerCaseTokenizer(); return new TokenStreamComponents(source, new StopFilter(source, stopWordSet)); &#125; public static void main(String[] args) throws IOException &#123; ArrayList&lt;String&gt; strings = new ArrayList&lt;String&gt;() &#123;&#123; add("小鬼子"); add("美国佬"); &#125;&#125;; Analyzer analyzer = new StopAnalyzerExtend(strings); String content = "小鬼子 and 美国佬 are playing together!"; TokenStream tokenStream = analyzer.tokenStream("myfield", content); tokenStream.reset(); CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class); while (tokenStream.incrementToken()) &#123; // 已经过滤掉自定义停用词 // 输出：playing together System.out.println(charTermAttribute.toString()); &#125; tokenStream.end(); tokenStream.close(); &#125;&#125; 自定义Analyzer实现字长过滤123456789101112131415161718192021222324252627282930313233343536373839404142434445class LongFilterAnalyzer extends Analyzer &#123; private int len; public int getLen() &#123; return this.len; &#125; public void setLen(int len) &#123; this.len = len; &#125; public LongFilterAnalyzer() &#123; super(); &#125; public LongFilterAnalyzer(int len) &#123; super(); setLen(len); &#125; @Override protected TokenStreamComponents createComponents(String fieldName) &#123; final Tokenizer source = new WhitespaceTokenizer(); //过滤掉长度&lt;len，并且&gt;20的token TokenStream tokenStream = new LengthFilter(source, len, 20); return new TokenStreamComponents(source, tokenStream); &#125; public static void main(String[] args) &#123; //把长度小于2的过滤掉，开区间 Analyzer analyzer = new LongFilterAnalyzer(2); String words = "I am a java coder! Testingtestingtesting!"; TokenStream stream = analyzer.tokenStream("myfield", words); try &#123; stream.reset(); CharTermAttribute offsetAtt = stream.addAttribute(CharTermAttribute.class); while (stream.incrementToken()) &#123; System.out.println(offsetAtt.toString()); &#125; stream.end(); stream.close(); &#125; catch (IOException e) &#123; &#125; &#125;&#125; 输出结果如下123amjavacoder! 可以看到，长度小于两个字符的文本都被过滤掉了。 Analyzer之PerFieldAnalyzerWrapperPerFieldAnalyzerWrapper的doc注释中提供了详细的说明，该类提供处理不同的Field使用不同的Analyzer的技术方案。PerFieldAnalyzerWrapper可以像其它的Analyzer一样使用，包括索引和查询分析。1234567891011121314151617181920212223242526272829303132public void testPerFieldAnalyzerWrapper() throws IOException, ParseException &#123; Map&lt;String, Analyzer&gt; fields = new HashMap&lt;&gt;(); fields.put("partnum", new KeywordAnalyzer()); //对于其他的域，默认使用SimpleAnalyzer分析器，对于指定的域partnum使用KeywordAnalyzer PerFieldAnalyzerWrapper perFieldAnalyzerWrapper = new PerFieldAnalyzerWrapper(new SimpleAnalyzer(), fields); Directory directory = new RAMDirectory(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(perFieldAnalyzerWrapper); IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); Document document = new Document(); FieldType fieldType = new FieldType(); fieldType.setStored(true); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS); document.add(new Field("partnum", "Q36", fieldType)); document.add(new Field("description", "Illidium Space Modulator", fieldType)); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); //直接使用TermQuery是可以检索到的 TopDocs search = indexSearcher.search(new TermQuery(new Term("partnum", "Q36")), 10); Assert.assertEquals(1, search.totalHits); //如果使用QueryParser，那么必须要使用PerFieldAnalyzerWrapper，否则如下所示，是检索不到的 Query description = new QueryParser("description", new SimpleAnalyzer()).parse("partnum:Q36 AND SPACE"); search = indexSearcher.search(description, 10); Assert.assertEquals(0, search.totalHits); System.out.println("SimpleAnalyzer :" + description.toString());//+partnum:q +description:space，原因是SimpleAnalyzer会剥离非字母字符并将字母小写化 //使用PerFieldAnalyzerWrapper可以检索到 //partnum:Q36 AND SPACE表示在partnum中出现Q36，在description中出现SPACE description = new QueryParser("description", perFieldAnalyzerWrapper).parse("partnum:Q36 AND SPACE"); search = indexSearcher.search(description, 10); Assert.assertEquals(1, search.totalHits); System.out.println("(SimpleAnalyzer,KeywordAnalyzer) :" + description.toString());//+partnum:Q36 +description:space&#125; 输出结果如下12SimpleAnalyzer :+partnum:q +description:space(SimpleAnalyzer,KeywordAnalyzer) :+partnum:Q36 +description:space 由结果可以看出，在索引阶段，使用KeywordAnalyzer作为partnum域的分词器，使用SimpleAnalyzer作为其它域的分词器，同样的在检索阶段，也必须这样处理，否则无法检索到结果。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 实战（3）-各种Field查询操作]]></title>
    <url>%2F2016%2F05%2F20%2FLucene-6-0-in-action-3-A-variety-of-Field-query-operation%2F</url>
    <content type="text"><![CDATA[IntPoint查询XXXPoint类中提供了一些常用的静态工厂查询方法，可以直接用来构建查询语句。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Testpublic void testIntPointQuery() throws IOException &#123; Directory directory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer())); Document document = new Document(); Field intPoint = new IntPoint("age", 11); document.add(intPoint); intPoint = new StoredField("age", 11); document.add(intPoint); indexWriter.addDocument(document); Field intPoint1 = new IntPoint("age", 22); document = new Document(); document.add(intPoint1); intPoint1 = new StoredField("age", 22); document.add(intPoint1); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); //精确查询 Query query = IntPoint.newExactQuery("age", 11); ScoreDoc[] scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("精确查询：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，不包含边界 query = IntPoint.newRangeQuery("age", Math.addExact(11, 1), Math.addExact(22, -1)); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("不包含边界：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，包含边界 query = IntPoint.newRangeQuery("age", 11, 22); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("包含边界：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，左包含，右不包含 query = IntPoint.newRangeQuery("age", 11, Math.addExact(22, -1)); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("左包含右不包含：" + indexSearcher.doc(scoreDoc.doc)); &#125; //集合查询 query = IntPoint.newSetQuery("age", 11, 22, 33); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("集合查询：" + indexSearcher.doc(scoreDoc.doc)); &#125;&#125; 输出结果如下123456精确查询：Document&lt;stored&lt;age:11&gt;&gt;包含边界：Document&lt;stored&lt;age:11&gt;&gt;包含边界：Document&lt;stored&lt;age:22&gt;&gt;左包含右不包含：Document&lt;stored&lt;age:11&gt;&gt;集合查询：Document&lt;stored&lt;age:11&gt;&gt;集合查询：Document&lt;stored&lt;age:22&gt;&gt; LongPoint查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Testpublic void testLongPointQuery() throws IOException &#123; Directory directory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer())); Document document = new Document(); Field longPoint = new LongPoint("age", 11); document.add(longPoint); longPoint = new StoredField("age", 11); document.add(longPoint); indexWriter.addDocument(document); longPoint = new LongPoint("age", 22); document = new Document(); document.add(longPoint); longPoint = new StoredField("age", 22); document.add(longPoint); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); //精确查询 Query query = LongPoint.newExactQuery("age", 11); ScoreDoc[] scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("精确查询：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，不包含边界 query = LongPoint.newRangeQuery("age", Math.addExact(11, 1), Math.addExact(22, -1)); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("不包含边界：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，包含边界 query = LongPoint.newRangeQuery("age", 11, 22); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("包含边界：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，左包含，右不包含 query = LongPoint.newRangeQuery("age", 11, Math.addExact(22, -1)); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("左包含右不包含：" + indexSearcher.doc(scoreDoc.doc)); &#125; //集合查询 query = LongPoint.newSetQuery("age", 11, 22, 33); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("集合查询：" + indexSearcher.doc(scoreDoc.doc)); &#125;&#125; 输出结果如下123456精确查询：Document&lt;stored&lt;age:11&gt;&gt;包含边界：Document&lt;stored&lt;age:11&gt;&gt;包含边界：Document&lt;stored&lt;age:22&gt;&gt;左包含右不包含：Document&lt;stored&lt;age:11&gt;&gt;集合查询：Document&lt;stored&lt;age:11&gt;&gt;集合查询：Document&lt;stored&lt;age:22&gt;&gt; FloatPoint查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Testpublic void testFloatPointQuery() throws IOException &#123; Directory directory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer())); Document document = new Document(); Field floatPoint = new FloatPoint("age", 11.1f); document.add(floatPoint); floatPoint = new StoredField("age", 11.1f); document.add(floatPoint); indexWriter.addDocument(document); floatPoint = new FloatPoint("age", 22.2f); document = new Document(); document.add(floatPoint); floatPoint = new StoredField("age", 22.2f); document.add(floatPoint); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); //精确查询 Query query = FloatPoint.newExactQuery("age", 11.1f); ScoreDoc[] scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("精确查询：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，不包含边界 query = FloatPoint.newRangeQuery("age", Math.nextUp(11.1f), Math.nextDown(22.2f)); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("不包含边界：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，包含边界 query = FloatPoint.newRangeQuery("age", 11.1f, 22.2f); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("包含边界：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，左包含，右不包含 query = FloatPoint.newRangeQuery("age", 11.1f, Math.nextDown(22.2f)); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("左包含右不包含：" + indexSearcher.doc(scoreDoc.doc)); &#125; //集合查询 query = FloatPoint.newSetQuery("age", 11.1f, 22.2f, 33.3f); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("集合查询：" + indexSearcher.doc(scoreDoc.doc)); &#125;&#125; 输出结果如下123456精确查询：Document&lt;stored&lt;age:11.1&gt;&gt;包含边界：Document&lt;stored&lt;age:11.1&gt;&gt;包含边界：Document&lt;stored&lt;age:22.2&gt;&gt;左包含右不包含：Document&lt;stored&lt;age:11.1&gt;&gt;集合查询：Document&lt;stored&lt;age:11.1&gt;&gt;集合查询：Document&lt;stored&lt;age:22.2&gt;&gt; DoublePoint查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Testpublic void testDoublePointQuery() throws IOException &#123; Directory directory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer())); Document document = new Document(); Field doublePoint = new DoublePoint("age", 11.1); document.add(doublePoint); doublePoint = new StoredField("age", 11.1); document.add(doublePoint); indexWriter.addDocument(document); doublePoint = new DoublePoint("age", 22.2); document = new Document(); document.add(doublePoint); doublePoint = new StoredField("age", 22.2); document.add(doublePoint); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); //精确查询 Query query = DoublePoint.newExactQuery("age", 11.1); ScoreDoc[] scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("精确查询：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，不包含边界 query = DoublePoint.newRangeQuery("age", Math.nextUp(11.1), Math.nextDown(22.2)); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("不包含边界：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，包含边界 query = DoublePoint.newRangeQuery("age", 11.1, 22.2); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("包含边界：" + indexSearcher.doc(scoreDoc.doc)); &#125; //范围查询，左包含，右不包含 query = DoublePoint.newRangeQuery("age", 11.1, Math.nextDown(22.2)); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("左包含右不包含：" + indexSearcher.doc(scoreDoc.doc)); &#125; //集合查询 query = DoublePoint.newSetQuery("age", 11.1, 22.2, 33.3); scoreDocs = indexSearcher.search(query, 10).scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println("集合查询：" + indexSearcher.doc(scoreDoc.doc)); &#125;&#125; 输出结果如下123456精确查询：Document&lt;stored&lt;age:11.1&gt;&gt;包含边界：Document&lt;stored&lt;age:11.1&gt;&gt;包含边界：Document&lt;stored&lt;age:22.2&gt;&gt;左包含右不包含：Document&lt;stored&lt;age:11.1&gt;&gt;集合查询：Document&lt;stored&lt;age:11.1&gt;&gt;集合查询：Document&lt;stored&lt;age:22.2&gt;&gt;]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 实战（2）-各种Field及排序操作]]></title>
    <url>%2F2016%2F05%2F20%2FLucene-6-0-in-action-2-All-kinds-of-Field-and-sort-operations%2F</url>
    <content type="text"><![CDATA[Field简述在Lucene中，各种Field都是IndexableField接口的实现，该接口中提供了一些通用的方法，用于获取Field相关的属性 12345678910111213141516public interface IndexableField &#123; //获取名称 public String name(); //获取Field的类型 public IndexableFieldType fieldType(); //获取Field的权重 public float boost(); /** Non-null if this field has a binary value */ public BytesRef binaryValue(); /** Non-null if this field has a string value */ public String stringValue(); /** Non-null if this field has a Reader value */ public Reader readerValue(); /** Non-null if this field has a numeric value */ public Number numericValue();&#125; NOTES：一般通过fieldType()方法获取到该Field对应的FieldType类型，但是却并不能去调用FieldType一系列的set方法，例如123Field field = new IntPoint(name, value);field.fieldType().setStored(true);field.fieldType().setTokenized(true); 否则会报 java.lang.IllegalStateException: this FieldType is already frozen and cannot be changed 这是因为各Field的子类中，都调用了type.freeze()方法，而该方法就可以阻止对fieldType做更改。 在Lucene 6.0中，IntField替换为IntPoint，FloatField替换为FloatPoint，LongField替换为LongPoint，DoubleField替换为DoublePoint。对Lucene常见Field总结如下 名称 说明 IntPoint 对int型字段索引，只索引不存储，提供了一些静态工厂方法用于创建一般的查询，提供了不同于文本的数值类型存储方式，使用KD-trees索引 FloatPoint 对float型字段索引，其它同上 LongPoint 对long型字段索引，其它同上 DoublePoint 对double型字段索引，其它同上 BinaryDocValuesField 只存储不共享，例如标题类字段，如果需要共享并排序，推荐使用SortedDocValuesField NumericDocValuesField 存储long型字段，用于评分、排序和值检索，如果需要存储值，还需要添加一个单独的StoredField实例 SortedDocValuesField 索引并存储，用于String类型的Field排序，需要在StringField后添加同名的SortedDocValuesField StringField 只索引但不分词，所有的字符串会作为一个整体进行索引，例如通常用于country或id等 TextField 索引并分词，不包括term vectors，例如通常用于一个body Field StoredField 存储Field的值，可以用 IndexSearcher.doc和IndexReader.document来获取存储的Field和存储的值 IntPoint的使用12345678910111213141516171819202122232425262728293031323334353637public void addIntPoint(Document document, String name, int value) &#123; Field field = new IntPoint(name, value); document.add(field); //要排序，必须添加一个同名的NumericDocValuesField field = new NumericDocValuesField(name, value); document.add(field); //要存储值，必须添加一个同名的StoredField field = new StoredField(name, value); document.add(field);&#125;@Testpublic void testIntPointSort() throws IOException &#123; Document document = new Document(); Directory directory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer())); addIntPoint(document, "intValue", 10); indexWriter.addDocument(document); document = new Document(); addIntPoint(document, "intValue", 20); indexWriter.addDocument(document); document = new Document(); addIntPoint(document, "intValue", 30); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); //第三个参数为true，表示从大到小 SortField intValues = new SortField("intValue", SortField.Type.INT, true); TopFieldDocs search = indexSearcher.search(new MatchAllDocsQuery(), 10, new Sort(intValues)); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125;&#125; 输出结果如下123Document&lt;stored&lt;intValue:30&gt;&gt;Document&lt;stored&lt;intValue:20&gt;&gt;Document&lt;stored&lt;intValue:10&gt;&gt; FloatPoint，LongPoint，DoublePoint使用方法类似，不再赘述。 BinaryDocValuesField的使用123456789101112131415161718192021222324252627282930313233public void addBinaryDocValuesField(Document document, String name, String value) &#123; Field field = new BinaryDocValuesField(name, new BytesRef(value)); document.add(field); //如果需要存储，加此句 field = new StoredField(name, value); document.add(field);&#125;@Testpublic void testBinaryDocValuesFieldSort() throws IOException &#123; Document document = new Document(); Directory directory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer())); addBinaryDocValuesField(document, "binaryValue", "1234"); indexWriter.addDocument(document); document = new Document(); addBinaryDocValuesField(document, "binaryValue", "2345"); indexWriter.addDocument(document); document = new Document(); addBinaryDocValuesField(document, "binaryValue", "12345"); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); SortField binaryValue = new SortField("binaryValue", SortField.Type.STRING_VAL, true); TopFieldDocs search = indexSearcher.search(new MatchAllDocsQuery(), 10, new Sort(binaryValue)); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125;&#125; 输出结果如下123Document&lt;stored&lt;intValue:2345&gt;&gt;Document&lt;stored&lt;intValue:12345&gt;&gt;Document&lt;stored&lt;binaryValue:1234&gt;&gt; StringField的使用1234567891011121314151617181920212223242526272829303132public void addStringField(Document document, String name, String value) &#123; Field field = new StringField(name, value, Field.Store.YES); document.add(field); field = new SortedDocValuesField(name, new BytesRef(value)); document.add(field);&#125;@Testpublic void testStringFieldSort() throws IOException &#123; Document document = new Document(); Directory directory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer())); addStringField(document, "stringValue", "1234"); indexWriter.addDocument(document); document = new Document(); addStringField(document, "stringValue", "2345"); indexWriter.addDocument(document); document = new Document(); addStringField(document, "stringValue", "12345"); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); SortField stringValue = new SortField("stringValue", SortField.Type.STRING, true); TopFieldDocs search = indexSearcher.search(new MatchAllDocsQuery(), 10, new Sort(stringValue)); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125;&#125; 输出结果如下123Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;stringValue:2345&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;stringValue:12345&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;stringValue:1234&gt;&gt; TextField的使用1234567891011121314151617181920212223242526272829303132public void addTextField(Document document, String name, String value) &#123; Field field = new TextField(name, value, Field.Store.YES); document.add(field); field = new SortedDocValuesField(name, new BytesRef(value)); document.add(field);&#125;@Testpublic void testTextFieldSort() throws IOException &#123; Document document = new Document(); Directory directory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(directory, new IndexWriterConfig(new StandardAnalyzer())); addTextField(document, "textValue", "1234"); indexWriter.addDocument(document); document = new Document(); addTextField(document, "textValue", "2345"); indexWriter.addDocument(document); document = new Document(); addTextField(document, "textValue", "12345"); indexWriter.addDocument(document); indexWriter.close(); IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory)); SortField textValue = new SortField("textValue", SortField.Type.STRING, true); TopFieldDocs search = indexSearcher.search(new MatchAllDocsQuery(), 10, new Sort(textValue)); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; System.out.println(indexSearcher.doc(scoreDoc.doc)); &#125;&#125; 输出结果如下123Document&lt;stored,indexed,tokenized&lt;textValue:2345&gt;&gt;Document&lt;stored,indexed,tokenized&lt;textValue:12345&gt;&gt;Document&lt;stored,indexed,tokenized&lt;textValue:1234&gt;&gt;]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene 6.0 实战（1）-创建索引]]></title>
    <url>%2F2016%2F05%2F20%2FLucene-6-0-in-action-1-Index-creation%2F</url>
    <content type="text"><![CDATA[引言Lucene6.0于2016年4月8日发布，要求最低Java版本是Java 8。 相信大多数公司的数据库都需要采用分库分表等一些策略，而对于某些特定的业务需求，分别从不同的库不同的表中去检索特定的数据显得比较繁琐，而Lucene正好可以解决某些特殊需求，对于不同库不同表中的数据先建立全量索引，然后将需要检索的数据写入某个单独的表中，供其它业务需求方查询，以后的每天只需要做增量索引并写入数据表即可。 鉴于最近一直在做Lucene相关方面的工作，而本人一向又比较喜欢使用最新发布的版本，而网络上这类资源极少，故将一些要点及示例整理出来，本文主要从实战角度来介绍Lucene 6.0的使用，不涉及过多原理方面的东西，但是对于一些核心点也会有所提及。 Lucene为什么这么流行Lucene是一个高效的，基于Java的全文检索库，生活中数据主要分为两种：结构化数据和非结构化数据。一般使用的XML、JSON、数据库等都是结构化数据，非结构化数据也叫全文数据，而这种全文数据正是Lucene的用武之地。全文检索主要有两个过程，索引创建（Indexing）和搜索索引（Search）。 Lucene是很多搜索引擎的一个基础实现，被很多大公司所采用，例如Netflix，MySpace，LinkedIn，Twitter，IBM等。可以通过如下几点特性对Lucene有个大概的认识 在现代的硬件上一小时可以索引150GB的数据 索引20GB的文本文件，产生的索引文件大概是4-6GB 只需要1MB的堆内存 可定制的排序模型 支持多种查询类型 通过特定的字段搜索 通过特定的字段排序 近实时的索引和搜索 Faceting，Grouping，Highlighting，Suggestions等 鉴于Lucene这么多强大的特性以及流行度，有很多种基于Lucene的搜索技术，其中最流行的两个是Apache Solr和Elastic search，当然还有许多其它不同语言的Lucene实现： CLucene: Lucene implementation in C++ (http://sourceforge.net/projects/clucene/) Lucene.Net: Lucene implementation in Microsoft.NET (http://incubator.apache.org/lucene.net/) Lucene4c: Lucene implementation in C (http://incubator.apache.org/lucene4c/) LuceneKit: Lucene implementation in Objective-C, Cocoa/GNUstep support (https://github.com/tcurdt/lucenekit) Lupy: Lucene implementation in Python (RETIRED) (http://www.divmod.org/projects/lupy) NLucene: This is another Lucene implementation in .NET (out of date) (http://sourceforge.net/projects/nlucene/) Zend Search: Lucene implementation in the Zend Framework for PHP 5 (http://framework.zend.com/manual/en/zend.search.html) Plucene: Lucene implementation in Perl (http://search.cpan.org/search?query=plucene&amp;mode=all) KinoSearch: This is a new Lucene implementation in Perl (http://www.rectangular.com/kinosearch/) PyLucene: This is GCJ-compiled version of Java Lucene integrated with Python (http://pylucene.osafoundation.org/) MUTIS: Lucene implementation in Delphi (http://mutis.sourceforge.net/) Ferret: Lucene implementation in Ruby (http://ferret.davebalmain.com/trac/) Montezuma: Lucene implementation in Common Lisp (http://www.cliki.net/Montezuma) 存储索引索引由Lucene按照特定的格式创建，而创建出来的索引必然要存储在文件系统之上，Lucene在文件系统中存储索引的最基本的抽象实现类是BaseDirectory，该类继承自Directory，BaseDirectory有两个主要的实现类： FSDirectory：在文件系统上存储索引文件，有六个子类，如下是三个常用的子类 SimpleFSDirectory：使用Files.newByteChannel实现，对并发支持不好，它会在多线程读取同一份文件时进行同步操作 NIOFSDirectory：使用Java NIO中的FileChannel去读取同一份文件，可以避免同步操作，但是由于Windows平台上存在Sun JRE bug，所以在Windows平台上不推荐使用 MMapDirectory：在读取的时候使用内存映射IO，如果你的虚拟内存足够容纳索引文件大小的话，这是一个很棒的选择 RAMDirectory：在内存中暂存索引文件，只对小索引好，大索引会出现频繁GC 通常情况下，如果索引文件存储在文件系统之上，我们无需自己选择使用FSDirectory的某个实现子类，只要使用FSDirectory中的open(Path path)方法即可，英文doc如下： Creates an FSDirectory instance, trying to pick the best implementation given the current environment. The directory returned uses the NativeFSLockFactory. The directory is created at the named location if it does not yet exist. 该方法可以自动根据当前使用的系统环境而选择一个最佳的实现子类，其选择策略是 对于Linux，MacOSX，Solaris，Windows 64-bit JREs返回MMapDirectory 对于其它非Windows上的JREs，返回NIOFSDirectory 对于其它Windows上的JREs，返回SimpleFSDirectory MMapDirectory就目前来说，是比较好的实现。它使用virtual memory和mmap来访问磁盘文件。一般的方法都是依赖系统调用在文件系统cache以及Java heap之间拷贝数据。那么怎么才能直接访问文件系统cache呢？这就是mmap的作用！ 简单说MMapDirectory就是把lucene的索引当作swap file来处理。mmap()系统调用让OS把整个索引文件映射到虚拟地址空间，这样Lucene就会觉得索引在内存中。然后Lucene就可以像访问一个超大的byte[]数据（在Java中这个数据被封装在ByteBuffer接口里）一样访问磁盘上的索引文件。Lucene在访问虚拟空间中的索引时，不需要任何的系统调用，CPU里的MMU（memory management unit）和TLB（translation lookaside buffers, 它cache了频繁被访问的page）会处理所有的映射工作。如果数据还在磁盘上，那么MMU会发起一个中断，OS将会把数据加载进文件系统Cache。如果数据已经在cache里了，MMU/TLB会直接把数据映射到内存，这只需要访问内存，速度很快。程序员不需要关心paging in/out，所有的这些都交给OS。而且，这种情况下没有并发的干扰，唯一的问题就是Java的ByteBuffer封装后的byte[]稍微慢一些，但是Java里要想用mmap就只能用这个接口。还有一个很大的优点就是所有的内存issue都由OS来负责，这样没有GC的问题。 索引核心类执行简单的索引过程需要用到以下几个类： IndexWriter：负责创建索引或打开已有索引 IndexWriterConfig：持有创建IndexWriter的所有配置项 Directory：描述了Lucene索引的存放位置，它的子类负责具体指定索引的存储路径 Analyzer：负责文本分析，从被索引文本文件中提取出语汇单元。对于文本分析器Analyzer，需要注意一点，就是使用哪种Analyzer进行索引创建，查询的时候也要使用哪种Analyzer查询，否则查询结果不正确。 Document：代表一些域（Field）的集合，你可以将Document对象理解为虚拟文档-例如Web页面、E-mail信息或者文本文件 Field：索引中的每个文档都包含一个或多个不同命名的域，每个域都有一个域名和对应的域值 FieldType：描述了Field的各种属性，在不使用某种具体的Field类型（例如StringField，TextField）时需要用到此类 创建索引索引的创建方式有三种，通过IndexWriterConfig.OpenMode进行指定，分别是 CREATE：创建一个新的索引或者覆写已经存在的索引 APPEND：打开一个已经存在的索引 CREATE_OR_APPEND：如果不存在则创建新的索引，如果存在则追加索引 123456789101112131415161718/** * 创建索引写入器 * * @param indexPath * @param create * @throws IOException */public IndexWriter getIndexWriter(String indexPath, boolean create) throws IOException &#123; IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new StandardAnalyzer()); if (create) &#123; indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE); &#125; else &#123; indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND); &#125; Directory directory = FSDirectory.open(Paths.get(indexPath)); IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); return indexWriter;&#125; 如果仅仅做测试用，还可以将索引文件存储在内存之中，此时需要使用RAMDirectory1234567891011121314151617181920212223242526272829303132public class LuceneDemo &#123; private Directory directory; private String[] ids = &#123;"1", "2"&#125;; private String[] unIndex = &#123;"Netherlands", "Italy"&#125;; private String[] unStored = &#123;"Amsterdam has lots of bridges", "Venice has lots of canals"&#125;; private String[] text = &#123;"Amsterdam", "Venice"&#125;; private IndexWriter indexWriter; private IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new StandardAnalyzer()); @Test public void createIndex() throws IOException &#123; directory = new RAMDirectory(); //指定将索引创建信息打印到控制台 indexWriterConfig.setInfoStream(System.out); indexWriter = new IndexWriter(directory, indexWriterConfig); indexWriterConfig = (IndexWriterConfig) indexWriter.getConfig(); FieldType fieldType = new FieldType(); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); fieldType.setStored(true);//存储 fieldType.setTokenized(true);//分词 for (int i = 0; i &lt; ids.length; i++) &#123; Document document = new Document(); document.add(new Field("id", ids[i], fieldType)); document.add(new Field("country", unIndex[i], fieldType)); document.add(new Field("contents", unStored[i], fieldType)); document.add(new Field("city", text[i], fieldType)); indexWriter.addDocument(document); &#125; indexWriter.commit(); &#125;&#125; NOTES：在调用IndexWriter的close()方法时会自动调用commit()方法，在调用commit()方法时会自动调用flush()方法。所以一般无需这样操作123indexWriter.flush();indexWriter.commit();indexWriter.close(); 控制台输出索引创建信息如下： IFD 0 [2016-05-19T07:10:21.127Z; main]: init: current segments file is “segments”; deletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@691a7f8fIFD 0 [2016-05-19T07:10:21.167Z; main]: delete []IFD 0 [2016-05-19T07:10:21.167Z; main]: now checkpoint “” [0 segments ; isCommit = false]IFD 0 [2016-05-19T07:10:21.167Z; main]: delete []IFD 0 [2016-05-19T07:10:21.167Z; main]: 0 msec to checkpointIW 0 [2016-05-19T07:10:21.167Z; main]: init: create=trueIW 0 [2016-05-19T07:10:21.168Z; main]:……DW 0 [2016-05-19T07:10:21.271Z; main]: main finishFullFlush success=trueIW 0 [2016-05-19T07:10:21.271Z; main]: startCommit(): startIW 0 [2016-05-19T07:10:21.271Z; main]: skip startCommit(): no changes pendingIFD 0 [2016-05-19T07:10:21.271Z; main]: delete []IW 0 [2016-05-19T07:10:21.271Z; main]: commit: pendingCommit == null; skipIW 0 [2016-05-19T07:10:21.271Z; main]: commit: took 0.4 msecIW 0 [2016-05-19T07:10:21.271Z; main]: commit: doneIW 0 [2016-05-19T07:10:21.271Z; main]: rollbackIW 0 [2016-05-19T07:10:21.271Z; main]: all running merges have abortedIW 0 [2016-05-19T07:10:21.271Z; main]: rollback: done finish mergesDW 0 [2016-05-19T07:10:21.271Z; main]: abortDW 0 [2016-05-19T07:10:21.271Z; main]: done abort success=trueIW 0 [2016-05-19T07:10:21.271Z; main]: rollback: infos=_0(6.0.0):c2IFD 0 [2016-05-19T07:10:21.271Z; main]: now checkpoint “_0(6.0.0):c2” [1 segments ; isCommit = false]IFD 0 [2016-05-19T07:10:21.272Z; main]: delete []IFD 0 [2016-05-19T07:10:21.272Z; main]: 0 msec to checkpointIFD 0 [2016-05-19T07:10:21.272Z; main]: delete []IFD 0 [2016-05-19T07:10:21.272Z; main]: delete [] 删除文档在IndexWriter中提供了从索引中删除Document的接口，分别是 deleteDocuments(Query… queries)：删除所有匹配到查询语句的Document deleteDocuments(Term… terms)：删除所有包含有terms的Document deleteAll()：删除索引中所有的Document NOTES: deleteDocuments(Term… terms)方法，只接受Term参数，而Term只提供如下四个构造函数 Term(String fld, BytesRef bytes) Term(String fld, BytesRefBuilder bytesBuilder) Term(String fld, String text) Term(String fld) 所以我们无法使用deleteDocuments(Term… terms)去删除一些非String值的Field，例如IntPoint，LongPoint，FloatPoint，DoublePoint等。这时候就需要借助传递Query实例的方法去删除包含某些特定类型Field的Document。1234567891011121314151617181920212223242526@Testpublic void testDelete() throws IOException &#123; RAMDirectory ramDirectory = new RAMDirectory(); IndexWriter indexWriter = new IndexWriter(ramDirectory, new IndexWriterConfig(new StandardAnalyzer())); Document document = new Document(); document.add(new IntPoint("ID", 1)); indexWriter.addDocument(document); indexWriter.commit(); //无法删除ID为1的 indexWriter.deleteDocuments(new Term("ID", "1")); indexWriter.commit(); DirectoryReader open = DirectoryReader.open(ramDirectory); IndexSearcher indexSearcher = new IndexSearcher(open); Query query = IntPoint.newExactQuery("ID", 1); TopDocs search = indexSearcher.search(query, 10); //命中，1，说明并未删除 System.out.println(search.totalHits); //使用Query删除 indexWriter.deleteDocuments(query); indexWriter.commit(); indexSearcher = new IndexSearcher(DirectoryReader.openIfChanged(open)); search = indexSearcher.search(query, 10); //未命中，0，说明已经删除 System.out.println(search.totalHits);&#125; 参考文献[1] http://www.cnblogs.com/huangfox/p/3616298.html]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 下使用 java jar 运行可执行 jar 包的正确方式]]></title>
    <url>%2F2016%2F05%2F11%2FThe-correct-way-to-use-java-jar-run-an-executable-jar-package-under-Linux%2F</url>
    <content type="text"><![CDATA[问题来源一般来说，一个大型的项目都会有一些依赖的JAR包（Java归档，英语：Java ARchive），而在将项目部署到服务器的过程中，如果没有持续集成环境的话，也就是说服务器不支持在线编译及打包，那么需要自己上传依赖的JAR包，然而可能服务器上已经存在了该项目所依赖的JAR包（比如项目修复BUG，重新打包上传，而依赖不变或者版本升级，修改方法等），无需再次上传所依赖的JAR包，此时只需将该项目单独打包，在运行的时候指定CLASSPATH即可。 在将JAR包部署到服务器上之后，设置CLASSPATH环境变量，运行java -jar ...命令出现ClassNotFoundException异常。之后又试用了诸多其它参数设置CLASSPATH，例如下面几个命令，同样都是报找不到类异常。12345set CLASSPATH = classpath1;classpath2...java -classpath &quot;.;D:\mylib\*&quot; -jar jar包 #Windows设置java -classpath &quot;.:/data/home/mylib/*&quot; -jar jar包 #Linux设置java -cp ...java -cp /lib/* 关于在CLASSPATH参数中使用通配符需要注意正确方式（冒号代表是Linux，Windows使用分号）1java -classpath &quot;lib/*:.&quot; my.package.Program 不正确方式1234java -classpath &quot;lib/a*.jar:.&quot; my.package.Programjava -classpath &quot;lib/a*:.&quot; my.package.Programjava -classpath &quot;lib/*.jar:.&quot; my.package.Programjava -classpath lib/*:. my.package.Program JAR的分类首先你需要知道JAR分为可执行JAR和非可执行JAR，一个可执行的JAR文件是一个自包含的Java应用程序，它存储在特别配置的JAR文件中，可以由JVM直接执行它而无需事先提取文件或者设置类路径。可执行的JAR文件中的MANIFEST.MF文件用代码Main-Class: myPrograms.MyClass指定了入口类，同时这个入口类的入口方法一定是Main方法，而不能是其它方法，注意要指明该类的全路径（另外-cp参数将被忽略，-cp 是 -classpath的缩写）。有些操作系统可以在点击后直接运行可执行JAR文件。而更典型的调用则是通过命令行执行java -jar [/data/home/java/]foo.jar。 运行存储在非可执行的JAR中的应用程序，不需要配置MANIFEST.MF文件，只要将它加入到您的类路径中，并用包名.类名这种全路径的方式指定应用程序的主类，而这个主类的入口方法也必须是Main方法，不能是其它方法。但是使用可执行的JAR文件，我们可以不用提取它或者知道主要入口点就可以运行一个应用程序。可执行JAR有助于方便发布和执行Java应用程序。典型的调用非可执行JAR包的命令是java -cp [/data/home/java/]foo.jar [多个JAR之间用;分隔] packageName.ClassName。 注意点：对于可执行JAR，在运行java -jar选项的时候，那么环境变量CLASSPATH和在命令行中指定的所有类路径都将被JVM忽略，也就是说，对于一个可执行JAR，使用java -classpath或者java -cp或者set classpath=lib/commons-io-2.4.jar等等命令指定CLASSPATH都是无效的。 正确姿势对于一个可执行的JAR必须通过MANIFEST.MF文件的头引用它所需要的所有其他从属JAR，引用方式如下1Class-Path: lib/commons-io-2.4.jar lib/commons-lang3-3.4.jar 如果有多个JAR包那么相互之间使用空格分隔。MANIFEST文件的一般格式如下123456789Manifest-Version: 1.0Archiver-Version: Plexus ArchiverBuilt-By: wangxuX-Compile-Target-JDK: 1.7X-Compile-Source-JDK: 1.7Created-By: Apache Maven 3.3.3Build-Jdk: 1.8.0_45Main-Class: com.yuewen.statistics.report.service.MainClass-Path: lib/commons-io-2.4.jar lib/commons-lang3-3.4.jar lib/guava-18.0.jar lib/junit-4.10.jar lib/log4j-api-2.0.jar lib/log4j-core-2.0.jar lib/lombok-1.16.4.jar lib/lucene-analyzers-common-5.5.0.jar lib/lucene-analyzers-smartcn-5.5.0.jar lib/lucene-core-5.5.0.jar lib/lucene-grouping-5.5.0.jar lib/lucene-queries-5.5.0.jar lib/lucene-queryparser-5.5.0.jar lib/mysql-connector-java-5.1.38-bin.jar 其中Manifest-Version表示版本号，一般由IDE工具自动生成，在编写MANIFEST文件的过程中，有如下注意点 Main-Class是JAR文件的主类，程序的入口 Class-Path指定需要依赖的JAR，多个JAR必须要在一行上，多个JAR之间以空格隔开，如果依赖的JAR在当前目录的子目录下，windows下使用\来分割，linux下用/分割 文件的冒号后面必须要空一个空格，否则会出错 文件的最后一行必须是一个回车换行符，否则也会出错 多条java jar命令的执行顺序问题通常地，我们会在服务器上配置shell脚本去定时调用自己的JAR包，但是当shell脚本中存在多条java -jar命令时，其执行情况是怎么样的呢？是同时并行执行，还是按顺序执行呢？经过测试得出，多条java -jar命令是按顺序执行的，并且只有在第一条java -jar命令执行完毕后，才会执行下一条java -jar命令，依次类推。 参考文献[1] http://stackoverflow.com/questions/219585/setting-multiple-jars-in-java-classpath/219801#219801[2] https://www.ibm.com/developerworks/cn/java/j-jar/[3] Difference between exporting as a JAR and exporting as a Runnable JAR]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA使用maven-javadoc-plugin生成Java Doc控制台乱码]]></title>
    <url>%2F2016%2F05%2F06%2FIntelliJ-IDEA-using-maven-javadoc-plugin-to-generate-the-Java-Doc-console-gibberish%2F</url>
    <content type="text"><![CDATA[问题重现在使用IDEA生成Java Doc的过程中，发现IDEA控制台乱码，作为有轻微代码强迫症的我来说，这是不可忍受的，需要鼓捣一番。先上pom.xml中的javadoc插件配置 123456789101112131415161718192021&lt;!--配置生成Javadoc包--&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.10.3&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;aggregate&gt;true&lt;/aggregate&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;docencoding&gt;UTF-8&lt;/docencoding&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 在运行mvn clean package命令进行打包之后，控制台会打印出如下信息，可以看到在使用javadoc插件的过程中，控制台输出乱码 [INFO] — maven-javadoc-plugin:2.10.3:jar (attach-javadocs) @ lucene —[INFO]���ڼ���Դ�ļ�D:\Multi-module-project\Lucene\src\main\java\AnalyzerDemo.java…���ڼ���Դ�ļ�D:\Multi-module-project\Lucene\src\main\java\BaiduAPI.java…���ڼ���Դ�ļ�D:\Multi-module-project\Lucene\src\main\java\CustomQueryParser.java……… 解决办法在IDEA中，打开File | Settings | Build, Execution, Deployment | Build Tools | Maven | Runner在VM Options中添加-Dfile.encoding=GBK，切记一定是GBK。即使用UTF-8的话，依然是乱码，这是因为Maven的默认平台编码是GBK，如果你在命令行中输入mvn -version的话，会得到如下信息，根据Default locale可以看出 Maven home:…Java version:…Java home:…Default locale: zh_CN, platform encoding: GBK…… 再次运行mvn clean package，控制台输出一切正常 [INFO] — maven-javadoc-plugin:2.10.3:jar (attach-javadocs) @ lucene —[INFO]正在加载源文件D:\Multi-module-project\Lucene\src\main\java\AnalyzerDemo.java…正在加载源文件D:\Multi-module-project\Lucene\src\main\java\BaiduAPI.java…正在加载源文件D:\Multi-module-project\Lucene\src\main\java\CustomQueryParser.java………]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>IntelliJ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之JDBC连接MySQL的一点坑]]></title>
    <url>%2F2016%2F04%2F15%2FA-small-trap-of-JDBC-connection-to-MySQL-in-java%2F</url>
    <content type="text"><![CDATA[对代码不满足，是任何真正有天才的程序员的根本特征。 Value ‘0000-00-00’ can not be represented as java.sql.Timestamp在查询MySql数据库某表的timestamp列的时候，发现此异常，问题来源表述如下 先创建一个测试表 123456create table test.mytest(id int PRIMARY KEY AUTO_INCREMENT ,createtime timestamp not null default current_timestamp , --创建时间updatetime timestamp not null , --更新时间testtime timestamp null --发布时间); 插入两条数据，将此sql执行两次即可 1insert into test.mytest(createtime,testtime) values(sysdate(),null); 查询数据 1select * from mytest; 在数据库客户端中使用SELECT查询结果如下： id createtime updatetime testtime 1 16/4/18 10:28:18 2 16/4/18 10:28:42 虽然说使用SELECT语句查询到的updatetime列和testtime列都是空，但是可以通过查看表结构从而得到各个列的默认值，在Toad工具中右击表选View Details得到如下表结构，可以看到Default列的默认值。 Column Type Collation Null Key Default Extra Privileges Comment id int(11) {null} NO PRI {null} auto_increment select,insert,update,references ID，主键 createtime timestamp {null} NO CURRENT_TIMESTAMP select,insert,update,references 创建时间 updatetime timestamp {null} NO 0000-00-00 00:00:00 select,insert,update,references 更新时间 testtime timestamp {null} YES {null} select,insert,update,references 发布时间 NOTE：在MySql中对于timestamp类型的列，如果设置not null的话（对应测试表的updatetime列），在程序中查询得其默认值是0000-00-00 00:00:00；如果timestamp的列默认值是null的话（对应测试表的testtime列），那么0000-00-00 00:00:00和null值在数据库中都是不显示（由查询结果，可知updatetime和testtime显示的结果都是空），也就是说在数据库中不管其默认值是not null还是null，该列值显示的都是空，你无法根据值去判断其类型，并且当是not null的时候，在程序中取出来的值实际上是0000-00-00 00:00:00，所以使用resultSet.getString()方法或其它方法都会报错，java.sql.SQLException: Value ‘0000-00-00’ can not be represented as java.sql.Timestamp。 另外timestamp类型要求第一个出现的timestamp列必须是not null default current_timestamp。 在Java程序中如果取到这种0000-00-00 00:00:00数据的话报错，解决办法是修改jdbc url为： jdbc:mysql://yourserver:3306/yourdatabase?zeroDateTimeBehavior=convertToNull 这样在程序中取出来的数据就是null了。 resultSet.getRow()总是返回0在使用resultSet.getRow()的时候，发现总是返回0，注意该方法返回的是数据库当前的行数，所以当你没调用resultSet.next()的时候，该方法总是返回0。 resultSet.getFetchSize()总是返回0另外使用resultSet.getFetchSize()的时候也是每次都返回0，这是怎么回事呢？getFetchSize()方法不是获得记录数，而是获得每次抓取的记录数，默认是0，也就是说不限制。可以用setFetchSize()来设置，而getFetchSize()是用来读出那个设置值。setFetchSize()最主要是为了减少网络交互次数设计的，访问ResultSet时，如果它每次只从服务器上取一行数据，则会产生大量的开销，setFetchSize()的意思是当调用resultSet.next()时，resultSet会一次性从服务器上取多行数据回来，这样在下次resultSet.next()时，它可以直接从内存中获得数据，而不需要网络交互，提高了效率。 Note：有时候觉得英文确实表述更准确，请看原汁原味的解释 The fetch size is the number of rows that should be retrieved from the database in each roundtrip. It has nothing to do with the number of rows returned. 获取查询的数据集的行数如果你是想获得符合条件的记录数目，最少有三种方法 自己维护计数器 1234int count = 0;while(resultSet.next())&#123; count++;&#125; 可以这样 12345678//需要设置结果集可滚动Statement statement = connection.createStatement(ResultSet.TYPE_SCROLL_SENSITIVE, ResultSet.CONCUR_UPDATABLE);//对于PreparedStatement也需要设置PreparedStatement preparedStatement = connection.prepareStatement(sql, ResultSet.TYPE_SCROLL_SENSITIVE, ResultSet.CONCUR_UPDATABLE);//之后在操作结果集中就可以移动游标了resultSet.last();rowCount = resultSet.getRow();resultSet.beforeFirst();//将数据集重新归位，这样就获取到行数了 有关ResultSet设置结果集类型的参数说明 ResultSet.TYPE_FORWARD_ONLY - 结果集不能滚动，这是默认值 ResultSet.TYPE_SCROLL_INSENSITIVE - 结果集可以滚动，但ResuleSet对数据库中发生的数据改变不敏感 ResultSet.TYPE_SCROLL_SENSITIVE - 结果集可以滚动，并且ResuleSet对数据库中发生的改变敏感 ResultSet.CONCUR_READ_ONLY - 只读结果集，不能用于更新数据库 ResultSet.CONCUR_UPDATABLE - 可更新结果集，可以用于更新数据库 当使用TYPE_SCROLL_INSENSITIVE或者TYPE_SCROLL_SENSITIVE来创建Statement对象时，可以使用ResultSet的first()/last()/beforeFirst()/afterLast()/relative()/absolute()等方法在结果集中随意前后移动 即使使用了CONCUR_UPDATABLE参数来创建Statement，得到的记录集也并非一定是“可更新的”，如果你的记录集来自于合并查询，即该查询的结果来自多个表格，那么这样的结果集就可能不是可更新的结果集。可以使用ResuleSet类的getConcurrency()方法来确定是否为可更新的的结果集。如果结果集是可更新的，那么可使用ResultSet的updateRow()，insertRow()，moveToCurrentRow()，deleteRow()，cancelRowUpdates()等方法来对数据库进行更新。 利用SQL查询12String sql = "select count(*) totalCount from table";rowCount = resultSet.getInt("totalCount"); Note：在JDBC中使用可更新的结果集来更新数据库，不能使用select * from table方式的SQL语句，必须将它写成如下两种形式之一： select table.* from table select column1,column2… from table 参考文献[1] http://blog.sina.com.cn/s/blog_50267f510100dj1m.html]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Log4j打印异常堆栈信息到日志]]></title>
    <url>%2F2016%2F04%2F15%2FLog4j-print-exception-stack-information-to-the-log%2F</url>
    <content type="text"><![CDATA[把辛勤的耕作当做生命的必要，即使没有收获的指望依然心平气和的继续耕种。路遥 在Java中，通常情况下，需要将异常堆栈信息输出到日志中，这样便于纠错及修正Bug，而多数情况下，大家最常用的是使用e.printStackTrace()直接打印堆栈信息完事，这并不是值的推荐的做法。 当出现异常时，调用e.printStackTrace();其实相当于什么都没做，同时也不会把异常信息输出到日志文件中 使用log.error(e.getMessage());只能够输出异常信息，但是并不包括异常堆栈，所以无法追踪出错的源点 使用log.error(e);除了输出异常信息外，还能输出异常类型，但是同样不包括异常堆栈，该方法doc说明为：Logs a message object with the ERROR level.显然并不会记录异常堆栈信息 当然也可以自己手动写个工具类，来挨个输出e.getStackTrace();获得的堆栈信息，显然繁琐麻烦 其实在log4j中只需要这样调用，就可以获得异常及堆栈信息log.error(Object var1, Throwable var2);，该方法doc说明为：Logs a message at the ERROR level including the stack trace of the Throwable t passed as parameter. 例如：12345678910@Testpublic void testError() &#123; String s = null; try &#123; s.length(); &#125; catch (Exception e) &#123; log.error(s, e); //当然如果你懒得想提示信息的话，直接这样log.error("", e); &#125;&#125; 输出： 2016.04.14 12:41:56 GMT+08:00 ERROR XX XX testError - null java.lang.NullPointerException at XX(XX.java:YY) [classes/:?]…………]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Log4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《程序员，你伤不起》]]></title>
    <url>%2F2016%2F03%2F27%2FRead-the-programmer-you-can-not-afford-injuries%2F</url>
    <content type="text"><![CDATA[作者简介：吉日嘎拉（蒙古族）2000年毕业于黑龙江大学计算机软件专业。微软MVP。博客园知名博主。争议性人物。现定居杭州。13年软件开发经验。外企5年。上市公司3年。独立经营软件公司2年。主持研发部门管理工作5年以上。行走江湖安身立命的作品是历经十余年不断完善的通用权限管理系统组件。该产品于2003年发布，目前已是国内用户最多的权限管理系统，被广大支持者称为权限管理系统中的“走火入魔级权限管理系统”精心维护10余年，不懈的推广，20万行经典业务逻辑积累，上万次的调试修正，将权限管理与工作流程管理做到我能力的极致，就是我的目标。 我相信，一个人的一生只能做好很少的几件事。 你不能老是换方向，一方面你啥都不精，日常工作也很累，东一榔头，西一棒子，十年后，结果你啥都懂，啥都没有，别人需要啥，你都需要从零开发，你俩手空空进入IT行业，又俩手空空离开IT行业。我很害怕这个，所以，从2002年开始写的代码，几乎都在电脑里，2003年写的系统，也经常在维护着，与时俱进。 我：想想自己不也是这样吗？一直都在重复造轮子，学习计算机也有七年了，本科四年，研究生三年，拿得出手的作品还真是没有，如果让我做东西的话也能做，只是一切从零开始，这想来是极其耗费精力的事情，好在我以前的代码都还留着，不管多烂也舍不得丢弃，只是一直没有整理维护之，这也正是我害怕的地方，随着时间的流逝，越来越没有心思与劲头去完善它们，谁让咱温饱都没解决呢，谁有精力去经营那些虚无的东西，人这一辈子，尤其是底层的人可能时间都用来挣钱去解决最基本的衣食住行了，这是多么可悲的事情。 开发软件，本来就是人来一拨走一拨，春风吹又生，一年换一拨人，一拨又一拨。所以你必须要事先想好人员的变动因素，自己如何积累经验成果，公司如何积累经验成果，都是很深的学问。 我：说的很对，不是技术导向型的公司往往无法理解技术人员真正的价值，以及技术人员之间的本质区别，顶级程序员与平庸程序员之间的差距是极大的，这是普通人所无法理解的，就像世界上跑的最快的人和平常人百米赛跑，他们花费的时间差距也不会超过两倍，这就是体力劳动的先天限制，而脑力劳动则全然不是，世界上最聪明的人和平常人的智力差距，必然是指数级的，这毫无疑问。我并不想说技术人员多牛逼多屌，而想说的是智力劳动，千兵易得，一将难求，要善于发现并珍惜重视人才。 大家都讲，做日本外包学不到知识，只是低级的编码工作，我从来不认同这个观念。我做日本外包大概有一年多时间，这期间也是我提高非常快的一段时间。自从接触日本外包后，我才觉得自己终于变成软件人才了，脑子里也懂了点东西，有些内容了，知道什么叫规范，什么叫质量，什么叫规模化生产，什么样的人才是软件人才，当然也见到了管理类软件开发领域的顶尖人物。 我：给那些不断打压贬低外包的人一记响亮的耳光，我常说人与人最本质的区别就在于人的能动性，无论在什么境遇下，都要充分发挥自己的能动性，在现有条件或水平内，做到最好，你就是最棒的，英雄不问出处，没人能左右自己的出身，不论出身如何，我只顾风雨兼程。 在日本，一个人的工资换算成人民币有两三万元，加班还要有加班工资，一个员工一年下来，公司可能要支出20万元给他，所以公司肯花10万元来购买软件，这样不仅仅是省了费用，而且软件可以24小时不间断工作，效率也提高了很多。但是在国内，两万就可以搞定一个人干一年的费用，而且还可以安排他干其他事情，所以国内的软件市场，跟我们的人力成本是有些关系的，人力成本越高，软件越有竞争力。 我：这段话的重点不是工资，重点是这个观点很新颖，原来我一直没注意到，人力成本越高，软件越有竞争力。想想现在的机器人代替工人其实不正是这个原理吗？机器人本质也都靠软件驱动，常年无休、可靠、任劳任怨，可以大大的降低人工成本，这必然会是未来的一大趋势。 做公司不是做技术，更多的是做人，我做人本就不够到位，做公司是必败的。 我：技术人员创业确有成功案例，小扎、马化腾、李彦宏都是技术出身，但是可能存在更多的技术人员创业失败的案例，只是不为人知晓而已，如果可以的话，技术人员最好找个合作伙伴，大家一起创业。 人没必要野心太大，做事业没有那么容易，特别是在IT行业，成功概率太低了。并且就算做大了，公司上市了，其实大多都是在亏损的，最终玩的就是资本运作。所以想靠公司的技术或者产品来赢得胜利是很难的，只能走资本运作路线。 我：从2015年的诸多互联网第一第二名并购案例就可以看出，最后玩的不是情怀、不是梦想、不是技术、不是产品，而是市场占有率、盈利能力以及背后对应的资本。 程序是技术（说的是后台开发），设计是艺术（说的是前台开发）。技术可以批量生产，艺术无法批量制造啊。在没有高科技的开发里，还是艺术更值钱一些。有艺术细胞，还会一点儿技术，那才是人才！ 我：其实之所以出现这种状况，还是因为常人都是非技术人员，他们所能理解的仅仅限于前台的设计，对于后台的架构技术则是完全不懂的，所以导致现在重前台而抑后台，对应的导致前端开发的工资超越了后端开发的工资。只有上了一定量级的公司，例如天猫淘宝，才会极度的重视后台的架构设计，因为不论前台多漂亮，都是难以支撑秒杀的。 日程生活中，多关注人情世故，多关心我的同事、我的家人、我的朋友，与他们保持联系，努力建设和谐社会。工作不是生活的全部，写程序更是很小的一部分。做人比做事更重要，人脉比技术更重要，有个良好的人脉，干啥都容易，说俗了人脉就是金钱。 我：虽然这个话题很俗，但是在天朝，这确实是一个很大的问题，无论怎样，你都免不了花费精力去处理人际关系问题。 晚上回家吃饭、上网娱乐、看电影、运动健身、看电视、睡觉，第二天起床上班，周末陪老婆看电影、逛街散步。 我：看似很简单的事情，估计天朝有很多人是享受不到这种待遇的，你是否享受到了呢？ 不要做过于廉价的劳动力，那是在破坏市场，尊重自己尊重别人，免费给别人做当然可以，但如果那样做，IT软件就更不值钱了。 我：我觉得咨询也是一种服务，所以就有了ThoughtWorks这类公司，但是这类毕竟是极其专业的咨询。日常开发生活中，我们其实也会碰到诸多的咨询，但是国人往往忽视了咨询的代价，代码调不通、环境不会配、测试通不过……从来不会尝试着自己先解决一下或者Google一下，而是第一就想到了去找别人咨询或者帮助，张口即问，伸手即来，殊不知，鲁迅先生说过，浪费别人的时间无异于谋财害命。 与领导沟通好，报告写好，比拼命干活效果好几十倍。别总是埋头苦干，干得累死累活，没人知道你多么辛苦，及时向领导汇报情况，周报月报下足功夫写。平时跟领导走得近一些，领导在想啥，你在想啥，及时沟通，这样被提拔的机会多。被提拔了，发展的空间也就大了。地里干活的牛辛苦不？光干活不行，性格倔强更没前途，就别提了。 我：算是厚黑学吗？ 开源一方面是好事情，另一方面是害死一大片的，微软很多东西都可以开源，它为什么不搞开源的操作系统？为什么不搞开源的办公软件？论坛现在几乎是一分钱不值了，大家也不用太费心机折腾论坛了。用友财务软件开源了，那谁还会花钱去买金蝶？还会去买其他小财务软件？那用友也赚不到啥钱，金蝶也可能会倒闭，做财务软件的人就有可能失业了，跟做论坛一样。商业化的东西，别乱搞开源，学习入门级别的东西，你开源是可以的，小技巧你开源可以，自己辛苦积累别轻易搞开源，那是损人害己。公司的成果更不能搞开源，那是老板烧了很多钱积累下来的成果，你三下两下就开源了，别以为你开源了，就为社会做贡献了，你很可能是在给自己挖掘坟墓，在污染自己的生存行业，恶化生存环境，破坏商业规则。 我：说的很好，现在大家有点盲目开源了，貌似不开源技术就不行，貌似不开源就没人知道自己的牛逼之处，貌似不开源就是软件世界里的恶势力一样，这些都是错误的思想，任何事情都没有绝对的，开源同样不是百利而无一害。 软件是个庞大的工程，不像盖小民房，看得见摸的着，软件是一个看不见的庞大工程，所以一个像样的项目，一般没那么容易就能搞定的。 我：虽说现在有了软件工程的概念，但是对于大多数人来说，并没有真正意识到一个软件的复杂度。就我而言，可能有的软件项目成功了，但是根本谈不上完美的程度，可能其扩展性、复用性、维护性各方面都存在大量问题，而有的项目只是要当前上线运行即可，至于以后，谁管呢？说不定项目经理早已跳槽，开发人员早已换人。 公司里往往耕地的牛是不讨主人喜欢的，汪汪乱叫的小狗却会讨主人的疼爱，主人会更喜好小狗。但是做软件项目是需要牛的，不是靠小狗叫叫就可以了。 我：至于做牛做狗，每个人都有自己的选择权，但是我觉得这和人的秉性有很大的关系，就我来说，情商不够高，做狗可能做不来，那么我只能被动的选择去做一头牛了。但是做牛的好处就是踏实干活，把本事学到手，这样不论到哪里，不论什么境地，哪怕经济危机也罢，至少我有谋生的手段，饿不死，积财千万不如薄技在身。 其实打工也是创业的开始，每个月都能拿到钱，还可以学到知识。公司的股份都是空话，没几个小公司能成功，开起来了也过不了三年就得分家。除非真的有科技含量或者是客户资源的，否则股份一文钱不值，每个月能按时拿到工资才是硬道理。 我：有些人总是想着期权，还是too young、too simple啊，并不是说期权不好，而是真正能让期权有着其对应价值的公司很少。可能大多数的公司都等不到IPO的那天，就因为各种各样的原因而中途寿终正寝了。即便真的给了你期权，也会有诸多限制条件，比如分五年领取，或者IPO之后三年的禁售期等等。 哪个都懂一些，就不容易成为专才。所以，我没有被各种诱惑误导，放弃.NET奔向Java，只做.NET就可以了，Java的不关我的事，我精力也有限，没必要接那么多项目，我也消化不了。 我：跟我的方向也是一致的，我就不做C++/C，谈不上讨厌或其它，只是精力有限，不可能全部精通，而且不会C++/C也并不影响你成为全栈工程师。不管是.NET或者Java，找准方向，深钻下去，都会有所收获。 当上级与下属发生争执后，公司解决问题的方法很简单：开除下属。因为公司没办法开除上级，开除上级带来的损失会更大。集团公司需要有一定的规矩，有一定的执行力度，就是靠下属服从上级领导为原则的。 我：很正常，毕竟哪个最容易替换就换哪个，哪个更换成本最低就换哪个，所以你要尽力做那个并不容易替代的人。 打工，有时候需要按部就班。你想出人头地，就要做得非常优秀。不只是技术上，还要在为人处世上。要不然，你想做出点成绩，不是老板压制你，就是身边的人看你不顺眼，是很难成功的。要想提高，不只是需要有量的变化，还需要有质的变化。量的变化可以靠蛮力，质的变化需要有头脑。只有蛮力是不会带来质的飞跃的，顶多只是量的积累。 我：对于多数人来说，都可以认为是一个打工者，我们能做的有限，除了做好本职工作之外，可能很多事情都是难以控制的，你想往上爬，别人也想往上爬，反而有时候是心理最阴暗的人爬上去了，因为搞技术的都不是人际关系的能手。唯有默默的积累吧，待量变到一定地步或许会带来质变。其实老板也不是那么好当的，别只看到花开后的灿烂，而忽视了为花开的那一刻所做的努力。要想人前显贵，必须人后受罪。 创业更需要的不是技术、干劲，而是商业模式。你想在IT行业创业，最好别干靠人的买卖。人是最不可靠的，最好是靠产品。创业也要讲个层次，越是高层次的创业越会顺利一些，越是层次低的越累，越容易失败。因为竞争对手太多，低层次的创业，别人可能会因为多给一千、两千，员工就会出卖你，签订的劳动合同，就当是废纸一张了，啥合同、啥信用都不讲了。 我：其实说的就是门槛嘛，门槛越高能进入的人必然越少，而且相对来说都是高素质的竞争中；门槛越低，必然竞争者遍地，难免鱼龙混杂，低素质者也大量涌入。 即使一个人写代码，也建议用版本管理器。最明显的好处就是操作失误便于恢复代码。 我：废话不说，举双手赞同。 关于朋友借钱的原则，整理如下 不是非常好的朋友我是理也不理。借是不可能的事情，因为我自身还有银行的几十万债务要还 若真是亲人要借钱，比如自己家里人，能给多少给多少，毕竟一家人需要患难与共 若是好朋友借钱，遇到苦难了，向我借十万，我白送五千元，这钱不用还，我表示一下心意，希望能对你有所帮助 有时候钱借出去了，收不回来，还会损失一个朋友 若是一般的朋友，当你借出去了，就要做好要不回来的打算 吉日嘎拉博客园地址：http://www.cnblogs.com/jirigala/]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven项目中Lucene集成中文分词工具Jcseg和Ansj]]></title>
    <url>%2F2016%2F03%2F23%2FMaven-project-integrating-Lucene-Chinese-Segmentation-tools-Jcseg-and-Ansj%2F</url>
    <content type="text"><![CDATA[开发环境Windows：Win7 64 bitJava：java version “1.8.0_45”；Java HotSpot(TM) 64-Bit Server VMLucene：5.5Jcseg：1.9.7Ansj_seg：3.7.1Ansj_lucene5_plug：3.0Maven：3.3.3 Lucene集成JcsegJcseg简介Jcseg是使用Java开发的一个开源中文分词器，使用流行的mmseg算法实现，有兴趣的可以参考算法原文，并且提供了最高版本的lucene、solr、elasticsearch的分词接口。 Google Code最新版V1.9.6：https://code.google.com/archive/p/jcseg/Git OSChina最新版V1.9.7：http://git.oschina.net/lionsoul/jcsegGitHub地址：https://github.com/lionsoul2014/jcseg Maven编译Jcseg项目源码从OSChina上下载ZIP压缩包之后，解压即可，文件夹重命名为jcseg，进入jcseg根目录，使用mvn clean package命令打包，前提是已经配置过maven环境变量，否则mvn无法识别。成功打包之后如下所示 [INFO] Reactor Summary:[INFO][INFO] jcseg-core ………………………………….. SUCCESS [ 13.469 s][INFO] jcseg-analyzer ………………………………. SUCCESS [ 4.601 s][INFO] jcseg-elasticsearch ………………………….. SUCCESS [ 4.355 s][INFO] jcseg-server ………………………………… SUCCESS [ 7.346 s][INFO] jcseg ………………………………………. SUCCESS [ 0.113 s][INFO] ————————————————————————[INFO] BUILD SUCCESS[INFO] ————————————————————————[INFO] Total time: 30.134 s[INFO] Finished at: 2016-03-22T16:47:59+08:00[INFO] Final Memory: 31M/282M[INFO] ———————————————————————— Maven将Jar包安装到本地仓库打开maven的settings.xml文件，配置本地仓库路径123&lt;settings&gt; &lt;localRepository&gt;D:/apache-maven-3.3.3/repo&lt;/localRepository&gt;&lt;/settings&gt; 使用maven命令mvn clean install将之前打包的jar文件安装到本地maven仓库中，成功安装如下所示 [INFO] — maven-install-plugin:2.4:install (default-install) @ jcseg-server —[INFO] Installing D:\jcseg\jcseg-server\target\jcseg-server-1.9.7.jar to D:\apache-maven-3.3.3\repo\org\lionsoul\jcseg\jcseg-server\1.9.7\jcseg-server-1.9.7.jar[INFO] Installing D:\jcseg\jcseg-server\dependency-reduced-pom.xml to D:\apache-maven-3.3.3\repo\org\lionsoul\jcseg\jcseg-server\1.9.7\jcseg-server-1.9.7.pom[INFO] Installing D:\jcseg\jcseg-server\target\jcseg-server-1.9.7-sources.jar to D:\apache-maven-3.3.3\repo\org\lionsoul\jcseg\jcseg-server\1.9.7\jcseg-server-1.9.7-sources.jar[INFO][INFO] ————————————————————————[INFO] Building jcseg 1.9.7[INFO] ————————————————————————[INFO][INFO] — maven-clean-plugin:2.5:clean (default-clean) @ jcseg —[INFO][INFO] — maven-enforcer-plugin:1.0:enforce (enforce-maven) @ jcseg —[INFO][INFO] — maven-install-plugin:2.4:install (default-install) @ jcseg —[INFO] Installing D:\jcseg\pom.xml to D:\apache-maven-3.3.3\repo\org\lionsoul\jcseg\jcseg\1.9.7\jcseg-1.9.7.pom[INFO] ————————————————————————[INFO] Reactor Summary:[INFO][INFO] jcseg-core ………………………………….. SUCCESS [ 10.195 s][INFO] jcseg-analyzer ………………………………. SUCCESS [ 6.793 s][INFO] jcseg-elasticsearch ………………………….. SUCCESS [ 3.815 s][INFO] jcseg-server ………………………………… SUCCESS [ 5.745 s][INFO] jcseg ………………………………………. SUCCESS [ 0.016 s][INFO] ————————————————————————[INFO] BUILD SUCCESS[INFO] ————————————————————————[INFO] Total time: 26.687 s[INFO] Finished at: 2016-03-22T16:53:28+08:00[INFO] Final Memory: 31M/253M[INFO] ———————————————————————— Maven项目配置Jcseg如果按照默认添加依赖方式如下所示12345&lt;dependency&gt; &lt;groupId&gt;org.lionsoul.jcseg&lt;/groupId&gt; &lt;artifactId&gt;jcseg-core&lt;/artifactId&gt; &lt;version&gt;1.9.7&lt;/version&gt;&lt;/dependency&gt; 是无法成功引用到本地仓库中的jcseg的，这是因为，如果依赖的版本是RELEASE或者LATEST，则Maven基于更新策略读取所有远程仓库的元数据groupId/artifactId/maven-metadata.xml，将其与本地仓库的对应元数据合并，如果不指定依赖版本，也就是说当依赖版本不明晰的时候，如RELEASE、LATEST、SNAPSHOT，Maven就需要基于更新远程仓库的更新策略来检查更新。而我们知道jcseg是我们自己安装到本地仓库中的，远程仓库必然不存在对应的元数据文件，所以这时maven会报错，那么如何解决呢？ 这时候就需要指定scope属性，scope用来指定依赖的范围，当scope指定的值是system的时候，Maven直接从本地文件系统解析构建，而不会去远程仓库查询。 scope还有其它几种取值，说明如下 compile：编译依赖范围，如果没有指定，就会默认使用该依赖范围 test：测试依赖范围，使用此依赖范围的Maven依赖，只对于测试classpath有效，典型的例子是JUnit，它只在编译测试代码及运行测试的时候才需要 provided：已提供依赖范围，使用此依赖范围的Maven依赖，对于编译和测试classpath有效，但在运行时无效，典型的例子是servlet-api，编译和测试项目的时候需要该依赖，但在运行项目的时候，由于容器已经提供，就不需要Maven重复地引入一遍 runtime：运行时依赖范围，使用此依赖范围的Maven依赖，对于测试和运行classpath有效，但在编译主代码时无效。典型的例子是JDBC驱动实现，项目主代码的编译只需要JDK提供的JDBC接口，只有在执行测试或者运行项目的时候才需要实现上述接口的具体JDBC驱动 system：系统依赖范围，该依赖与三种classpath的关系，和provided依赖范围完全一致。但是，在使用system范围的依赖时必须通过systemPath元素显式地指定依赖文件路径。通常此依赖与本机系统绑定，造成构建的不可移植，应该谨慎使用。 import：导入依赖范围，该依赖范围不会对三种classpath产生实际的影响，三种classpath指的是，编译classpath、测试classpath、运行时classpath 所以解决方案已经有了，如下所示，其它依赖本地仓库中的jar包配置类似：123456789101112131415&lt;!--在systemPath中亦可指定其它位置，包括相对路径--&gt;&lt;dependency&gt; &lt;groupId&gt;org.lionsoul.jcseg&lt;/groupId&gt; &lt;artifactId&gt;jcseg-core&lt;/artifactId&gt; &lt;version&gt;1.9.7&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;D:/apache-maven-3.3.3/repo/org/lionsoul/jcseg/jcseg-core/1.9.7/jcseg-core-1.9.7.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.lionsoul.jcseg&lt;/groupId&gt; &lt;artifactId&gt;jcseg-analyzer&lt;/artifactId&gt; &lt;version&gt;1.9.7&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;D:/apache-maven-3.3.3/repo/org/lionsoul/jcseg/jcseg-analyzer/1.9.7/jcseg-analyzer-1.9.7.jar&lt;/systemPath&gt;&lt;/dependency&gt; Lucene集成Jcseg的测试代码将jcseg源码包中的lexicon和jcseg.properties两个文件复制到src/main/resources下，并修改jcseg.properties中的lexicon.path = src/main/resources/lexicon1234567891011121314151617181920212223242526272829303132333435363738@Testpublic void test() throws IOException, ParseException &#123; String text = "jcseg是使用Java开发的一款开源的中文分词器, 基于流行的mmseg算法实现，分词准确率高达98.4%, 支持中文人名识别, 同义词匹配, 停止词过滤等。并且提供了最新版本的lucene,solr,elasticsearch分词接口。"; //如果不知道选择哪个Directory的子类，那么推荐使用FSDirectory.open()方法来打开目录 Analyzer analyzer = new JcsegAnalyzer5X(JcsegTaskConfig.COMPLEX_MODE); //非必须(用于修改默认配置): 获取分词任务配置实例 JcsegAnalyzer5X jcseg = (JcsegAnalyzer5X) analyzer; JcsegTaskConfig config = jcseg.getTaskConfig(); //追加同义词, 需要在 jcseg.properties中配置jcseg.loadsyn=1 config.setAppendCJKSyn(true); //追加拼音, 需要在jcseg.properties中配置jcseg.loadpinyin=1 config.setAppendCJKPinyin(true); //更多配置, 请查看 org.lionsoul.jcseg.tokenizer.core.JcsegTaskConfig Directory directory = new RAMDirectory(); indexWriterConfig = new IndexWriterConfig(analyzer); indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND); indexWriter = new IndexWriter(directory, indexWriterConfig); Document document = new Document(); document.add(new StringField("id", "1000", Field.Store.YES)); document.add(new TextField("text", text, Field.Store.YES)); indexWriter.addDocument(document); indexWriter.commit(); IndexReader indexReader = DirectoryReader.open(directory); IndexSearcher indexSearcher = new IndexSearcher(indexReader); String key = "中文分词器"; QueryParser queryParser = new QueryParser("text", analyzer); queryParser.setDefaultOperator(QueryParser.Operator.AND); Query parse = queryParser.parse(key); System.out.println(parse); TopDocs search = indexSearcher.search(parse, 10); System.out.println("命中数：" + search.totalHits); ScoreDoc[] scoreDocs = search.scoreDocs; for (ScoreDoc sd : scoreDocs) &#123; Document doc = indexSearcher.doc(sd.doc); System.out.println("得分：" + sd.score); System.out.println(doc.get("text")); &#125;&#125; 结果输出 +text:中文 +text:zhong wen +text:国语 +text:分词器 +text:fen ci qi命中数：1得分：0.080312796jcseg是使用Java开发的一款开源的中文分词器, 基于流行的mmseg算法实现，分词准确率高达98.4%, 支持中文人名识别, 同义词匹配, 停止词过滤等。并且提供了最新版本的lucene,solr,elasticsearch分词接口。 Lucene集成AnsjAnsj简介Ansj是一个ICTCLAS的Java实现。基本上重写了所有的数据结构和算法。词典是用的开源版的ICTCLAS所提供的。并且进行了部分的人工优化。 还是一个基于n-Gram+条件随机场模型的中文分词的Java实现。分词速度达到每秒钟大约200万字左右（Mac Air下测试），准确率能达到96%以上。目前实现了中文分词、中文姓名识别、用户自定义词典。可以应用到自然语言处理等方面，适用于对分词效果要求高的各种项目。 GitHub项目地址：https://github.com/NLPchina/ansj_segAnsj的仓库地址，包括针对Lucene的插件：http://maven.nlpcn.org/org/ansj/ Maven项目配置Ansj根据官方手册，在pom.xml文件中加入依赖，如下所示12345678910&lt;dependency&gt; &lt;groupId&gt;org.ansj&lt;/groupId&gt; &lt;artifactId&gt;ansj_seg&lt;/artifactId&gt; &lt;version&gt;3.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.ansj&lt;/groupId&gt; &lt;artifactId&gt;ansj_lucene5_plug&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt;&lt;/dependency&gt; 但是你应该能想到，中央仓库或者镜像仓库中并没有Ansj啊，那上述依赖必然报错。对的，所以还需要在settings.xml加入Ansj的仓库地址1234567&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;mvn-repo&lt;/id&gt; &lt;name&gt;ansj maven repo&lt;/name&gt; &lt;url&gt;http://maven.nlpcn.org/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 如果你使用了中央仓库的镜像请注意如下内容，如果你没使用镜像请忽略之。 一般在天朝，访问maven中央仓库速度是很慢的，所以国内一般会用某个中央仓库的镜像，而我用的是OSChina的镜像，在镜像的配置中，如果你使用了通配符，那么需要注意该通配符同样会屏蔽掉Ansj仓库的地址，所以需要在通配符之后排除掉Ansj的仓库地址123456789101112131415&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc&lt;/id&gt; &lt;mirrorOf&gt;*,!mvn-repo&lt;/mirrorOf&gt; &lt;!--&lt;mirrorOf&gt;*&lt;/mirrorOf&gt;如果使用这种方式会屏蔽Ansj仓库--&gt; &lt;name&gt;Nexus osc&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc-thirdparty&lt;/id&gt; &lt;mirrorOf&gt;thirdparty&lt;/mirrorOf&gt; &lt;name&gt;Nexus osc thirdparty&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/repositories/thirdparty/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 当然如果你没使用通配符，而是指定对Maven中央仓库做镜像的话，就无需使用!mvn-repo进行排除了，其中mvn-repo是Ansj仓库的ID。指定对Maven中央仓库做镜像配置如下：1234567891011121314&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus osc&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc-thirdparty&lt;/id&gt; &lt;mirrorOf&gt;thirdparty&lt;/mirrorOf&gt; &lt;name&gt;Nexus osc thirdparty&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/repositories/thirdparty/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; Maven之镜像如果仓库X可以提供仓库Y存储的所有内容，那么就可以认为X是Y的一个镜像。换句话说，任何一个可以从仓库Y获得的构建，都能够从它的镜像中获取。 关于镜像的一个更为常见的用法是结合私服。由于私服可以代理任何外部的公共仓库，因此，对于组织内部的Maven用户来说，使用一个私服地址就等于使用了所有需要的外部仓库，这可以将配置集中到私服，从而简化Maven本身的配置。 有关镜像的一些配置说明如下：1234&lt;mirrorOf&gt;*&lt;/mirrorOf&gt;：匹配所有远程仓库&lt;mirrorOf&gt;external:*&lt;/mirrorOf&gt;：匹配所有远程仓库，使用localhost的除外，使用file://协议的除外。也就是说，匹配所有不在本机上的远程仓库&lt;mirrorOf&gt;repo1,repo2&lt;/mirrorOf&gt;：匹配仓库repo1和repo2，使用逗号分隔多个远程仓库&lt;mirrorOf&gt;*,!repo1&lt;/mirrorOf&gt;：匹配所有远程仓库，repo1除外，使用感叹号将仓库从匹配中排除 需要注意的是，由于镜像仓库完全屏蔽了被镜像仓库，当镜像仓库不稳定或者停止服务的时候，Maven仍将无法访问被镜像仓库，因而将无法下载构建。 Maven中自定义变量通常在依赖一个项目多个组件的时候，为每一个组件单独指定版本号是可以的，但是当升级版本号的时候，就需要对每个组件都做升级，很麻烦，这时就需要自定义变量了。在pom.xml中定义如下123&lt;properties&gt; &lt;spring.version&gt;5.2.0&lt;/spring.version&gt;&lt;/properties&gt; 使用时，直接在version中引用标签名即可，例如12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt; Maven内置变量Maven本身就内置了很多预定义变量，可以直接引用，选取一些举例如下 内置属性 ${basedir} represents the directory containing pom.xml ${version} equivalent to ${project.version} or ${pom.version} ${project.basedir} 同 ${basedir} ${project.baseUri} 表示项目文件地址 ${maven.build.timestamp} 表示项目构件开始时间; ${maven.build.timestamp.format} 表示属性 ${maven.build.timestamp} 的展示格式，默认值为yyyyMMdd-HHmm POM属性 ${project.build.directory} results in the path to your “target” dir, this is the same as ${pom.project.build.directory} ${project.build.outputDirectory} results in the path to your “target/classes” dir ${project.name} refers to the name of the project ${project.version} refers to the version of the project ${project.build.finalName} refers to the final name of the file created when the built project is packaged Settings文件属性 ${settings.localRepository} refers to the path of the user’s local repository Java系统属性 使用mvn help:system命令可以查看所有的Java系统属性 System.getProperties()可以得到所有的Java属性 ${user.home} 表示用户目录 ${java.home} specifies the path to the current JRE_HOME environment use with relative paths to get for example:&lt;jvm&gt;${java.home}../bin/java.exe&lt;/jvm&gt; 环境变量属性 ${env.M2_HOME} returns the Maven2 installation path. ${env.JAVA_HOME} 表示JAVA_HOME环境变量的值; 自定义属性 &lt;properties&gt;&lt;my.version&gt;hello&lt;/my.version&gt;&lt;/properties&gt;则引用 ${my.version}就会得到值hello Lucene集成Ansj的测试代码Ansj In Lucene的官方参考文档：http://nlpchina.github.io/ansj_seg/ 到https://github.com/NLPchina/ansj_seg 下载ZIP压缩文件，解压，将其中的library文件夹和library.properties文件拷贝到maven项目下的src/main/resources中，修改library.properties内容如下12345678#redress dic file pathambiguityLibrary=src/main/resources/library/ambiguity.dic#path of userLibrary this is default libraryuserLibrary=src/main/resources/library/default.dic#path of crfModelcrfModel=src/main/resources/library/crf.model#set real nameisRealName=true Lucene集成Ansj测试代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import org.ansj.library.UserDefineLibrary;import org.ansj.lucene5.AnsjAnalyzer;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.index.CorruptIndexException;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.Highlighter;import org.apache.lucene.search.highlight.InvalidTokenOffsetsException;import org.apache.lucene.search.highlight.QueryScorer;import org.apache.lucene.search.highlight.SimpleHTMLFormatter;import org.apache.lucene.store.Directory;import org.apache.lucene.store.RAMDirectory;import org.junit.Test;import org.tartarus.snowball.ext.PorterStemmer;import java.io.IOException;import java.io.Reader;import java.io.StringReader;import java.util.Date;public class IndexTest &#123; @Test public void test() throws IOException &#123; Analyzer ca = new AnsjAnalyzer(); Reader sentence = new StringReader( "\n\n\n\n\n\n\n我从小就不由自主地认为自己长大以后一定得成为一个象我父亲一样的画家, 可能是父母潜移默化的影响。其实我根本不知道作为画家意味着什么，我是否喜欢，最重要的是否适合我，我是否有这个才华。其实人到中年的我还是不确定我最喜欢什么，最想做的是什么？我相信很多人和我一样有同样的烦恼。毕竟不是每个人都能成为作文里的宇航员，科学家和大教授。知道自己适合做什么，喜欢做什么，能做好什么其实是个非常困难的问题。"); TokenStream ts = ca.tokenStream("sentence", sentence); System.out.println("start: " + (new Date())); long before = System.currentTimeMillis(); while (ts.incrementToken()) &#123; System.out.println(ts.getAttribute(CharTermAttribute.class)); &#125; ts.close(); long now = System.currentTimeMillis(); System.out.println("time: " + (now - before) / 1000.0 + " s"); &#125; @Test public void indexTest() throws IOException, ParseException &#123; Analyzer analyzer = new AnsjAnalyzer(); Directory directory; IndexWriter iwriter; String text = "季德胜蛇药片 10片*6板 "; UserDefineLibrary.insertWord("蛇药片", "n", 1000); IndexWriterConfig ic = new IndexWriterConfig(analyzer); // 建立内存索引对象 directory = new RAMDirectory(); iwriter = new IndexWriter(directory, ic); addContent(iwriter, text); iwriter.commit(); iwriter.close(); System.out.println("索引建立完毕"); System.out.println("index ok to search!"); search(analyzer, directory, "\"季德胜蛇药片\""); &#125; private void search(Analyzer queryAnalyzer, Directory directory, String queryStr) throws IOException, ParseException &#123; IndexSearcher isearcher; DirectoryReader directoryReader = DirectoryReader.open(directory); // 查询索引 isearcher = new IndexSearcher(directoryReader); QueryParser tq = new QueryParser("text", queryAnalyzer); Query query = tq.parse(queryStr); System.out.println(query); TopDocs hits = isearcher.search(query, 5); System.out.println(queryStr + ":共找到" + hits.totalHits + "条记录!"); for (int i = 0; i &lt; hits.scoreDocs.length; i++) &#123; int docId = hits.scoreDocs[i].doc; Document document = isearcher.doc(docId); System.out.println(toHighlighter(queryAnalyzer, query, document)); &#125; &#125; private String toHighlighter(Analyzer analyzer, Query query, Document doc) &#123; String field = "text"; try &#123; SimpleHTMLFormatter simpleHtmlFormatter = new SimpleHTMLFormatter("&lt;font color=\"red\"&gt;", "&lt;/font&gt;"); Highlighter highlighter = new Highlighter(simpleHtmlFormatter, new QueryScorer(query)); TokenStream tokenStream1 = analyzer.tokenStream("text", new StringReader(doc.get(field))); String highlighterStr = highlighter.getBestFragment(tokenStream1, doc.get(field)); return highlighterStr == null ? doc.get(field) : highlighterStr; &#125; catch (IOException | InvalidTokenOffsetsException e) &#123; &#125; return null; &#125; private void addContent(IndexWriter iwriter, String text) throws CorruptIndexException, IOException &#123; Document doc = new Document(); doc.add(new Field("text", text, Field.Store.YES, Field.Index.ANALYZED)); iwriter.addDocument(doc); &#125; @Test public void poreterTest() &#123; PorterStemmer ps = new PorterStemmer(); System.out.println(ps.stem()); &#125;&#125; 本以为大功告成，怀着激动的心情运行Junit单元测试，但是天雷滚滚啊，报错啊，想想这可是官方给的测试Demo啊，居然报错！！！ 废话不说，上结果： java.lang.AssertionError: TokenStream implementation classes or at least their incrementToken() implementation must be final Notes 2016/3/27：经反馈，作者已fix此Bug，但本文不再做更新。Issue编号：#249，反馈地址：https://github.com/NLPchina/ansj_seg/issues/249#event-604309598 说的很清楚啊，Ansj作者自己测试通过否？要么将类修饰为final，要么将incrementToken()方法修饰为final，这是为啥哩？ 查源码，从AnsjAnalyzer追踪到AnsjTokenizer，再追踪到Tokenizer，别停，继续追踪TokenStream，该类位于org.apache.lucene.analysis包下，在该类的Doc注释中，终于发现如下说明 The {@code TokenStream}-API in Lucene is based on the decorator pattern. Therefore all non-abstract subclasses must be final or have at least a final implementation of {@link #incrementToken}! This is checked when Java assertions are enabled. 意思是说所有的非抽象子类必须是final的或者至少有一个final修饰的incrementToken()覆写方法。但是Ansj针对Lucene的插件中，这两者都没有做！！！ Lucene集成Ansj报错解决方案 既然Ansj两者都没做，那么一种方法就是修改Ansj的源码，但是我们使用的是Ansj仓库中提供的Jar包，修改源码之后，只能本地引用修改后的Jar包，不方便项目的迁移，所以不采用 提供一个final修饰的覆写方法incrementToken()，通过实现两个内部类，分别继承自AnsjAnalyzer和AnsjTokenizer，在使用的时候调用自己实现的内部类 修复BUG后的代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161import org.ansj.library.UserDefineLibrary;import org.ansj.lucene.util.AnsjTokenizer;import org.ansj.lucene5.AnsjAnalyzer;import org.ansj.splitWord.Analysis;import org.ansj.splitWord.analysis.IndexAnalysis;import org.ansj.splitWord.analysis.ToAnalysis;import org.ansj.splitWord.analysis.UserDefineAnalysis;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.Tokenizer;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.index.CorruptIndexException;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.Highlighter;import org.apache.lucene.search.highlight.InvalidTokenOffsetsException;import org.apache.lucene.search.highlight.QueryScorer;import org.apache.lucene.search.highlight.SimpleHTMLFormatter;import org.apache.lucene.store.Directory;import org.apache.lucene.store.RAMDirectory;import org.junit.Test;import org.nlpcn.commons.lang.tire.domain.Forest;import org.tartarus.snowball.ext.PorterStemmer;import java.io.BufferedReader;import java.io.IOException;import java.io.Reader;import java.io.StringReader;import java.util.Date;import java.util.Set;public class IndexTest &#123; @Test public void test() throws IOException &#123; Analyzer ca = new MyAnsjAnalyzer(); Reader sentence = new StringReader( "\n\n\n\n\n\n\n我从小就不由自主地认为自己长大以后一定得成为一个象我父亲一样的画家, 可能是父母潜移默化的影响。其实我根本不知道作为画家意味着什么，我是否喜欢，最重要的是否适合我，我是否有这个才华。其实人到中年的我还是不确定我最喜欢什么，最想做的是什么？我相信很多人和我一样有同样的烦恼。毕竟不是每个人都能成为作文里的宇航员，科学家和大教授。知道自己适合做什么，喜欢做什么，能做好什么其实是个非常困难的问题。"); TokenStream ts = ca.tokenStream("sentence", sentence); System.out.println("start: " + (new Date())); long before = System.currentTimeMillis(); while (ts.incrementToken()) &#123; System.out.println(ts.getAttribute(CharTermAttribute.class)); &#125; ts.close(); long now = System.currentTimeMillis(); System.out.println("time: " + (now - before) / 1000.0 + " s"); &#125; static class MyAnsjTokenizer extends AnsjTokenizer &#123; public MyAnsjTokenizer(Analysis ta, Set&lt;String&gt; filter) &#123; super(ta, filter); &#125; @Override public final boolean incrementToken() throws IOException &#123; return super.incrementToken(); &#125; &#125; static class MyAnsjAnalyzer extends AnsjAnalyzer &#123; private Set&lt;String&gt; filter; private String type; public MyAnsjAnalyzer() &#123; &#125; public MyAnsjAnalyzer(String type, Set&lt;String&gt; filter) &#123; super(type, filter); &#125; @Override protected TokenStreamComponents createComponents(String text) &#123; BufferedReader reader = new BufferedReader(new StringReader(text)); Tokenizer tokenizer = getTokenizer(reader, this.type, this.filter); return new TokenStreamComponents(tokenizer); &#125; public static Tokenizer getTokenizer(BufferedReader reader, String type, Set&lt;String&gt; filter) &#123; AnsjTokenizer tokenizer; if ("user".equalsIgnoreCase(type)) &#123; tokenizer = new MyAnsjTokenizer(new UserDefineAnalysis(reader, new Forest[0]), filter); &#125; else if ("index".equalsIgnoreCase(type)) &#123; tokenizer = new MyAnsjTokenizer(new IndexAnalysis(reader, new Forest[0]), filter); &#125; else &#123; tokenizer = new MyAnsjTokenizer(new ToAnalysis(reader, new Forest[0]), filter); &#125; return tokenizer; &#125; &#125; @Test public void indexTest() throws IOException, ParseException &#123; Analyzer analyzer = new MyAnsjAnalyzer(); Directory directory; IndexWriter iwriter; String text = "季德胜蛇药片 10片*6板 "; UserDefineLibrary.insertWord("蛇药片", "n", 1000); IndexWriterConfig ic = new IndexWriterConfig(analyzer); // 建立内存索引对象 directory = new RAMDirectory(); iwriter = new IndexWriter(directory, ic); addContent(iwriter, text); iwriter.commit(); iwriter.close(); System.out.println("索引建立完毕"); System.out.println("index ok to search!"); search(analyzer, directory, "\"季德胜蛇药片\""); &#125; private void search(Analyzer queryAnalyzer, Directory directory, String queryStr) throws IOException, ParseException &#123; IndexSearcher isearcher; DirectoryReader directoryReader = DirectoryReader.open(directory); // 查询索引 isearcher = new IndexSearcher(directoryReader); QueryParser tq = new QueryParser("text", queryAnalyzer); Query query = tq.parse(queryStr); System.out.println(query); TopDocs hits = isearcher.search(query, 5); System.out.println(queryStr + ":共找到" + hits.totalHits + "条记录!"); for (int i = 0; i &lt; hits.scoreDocs.length; i++) &#123; int docId = hits.scoreDocs[i].doc; Document document = isearcher.doc(docId); System.out.println(toHighlighter(queryAnalyzer, query, document)); &#125; &#125; private String toHighlighter(Analyzer analyzer, Query query, Document doc) &#123; String field = "text"; try &#123; SimpleHTMLFormatter simpleHtmlFormatter = new SimpleHTMLFormatter("&lt;font color=\"red\"&gt;", "&lt;/font&gt;"); Highlighter highlighter = new Highlighter(simpleHtmlFormatter, new QueryScorer(query)); TokenStream tokenStream1 = analyzer.tokenStream("text", new StringReader(doc.get(field))); String highlighterStr = highlighter.getBestFragment(tokenStream1, doc.get(field)); return highlighterStr == null ? doc.get(field) : highlighterStr; &#125; catch (IOException | InvalidTokenOffsetsException e) &#123; &#125; return null; &#125; private void addContent(IndexWriter iwriter, String text) throws CorruptIndexException, IOException &#123; Document doc = new Document(); doc.add(new Field("text", text, Field.Store.YES, Field.Index.ANALYZED)); iwriter.addDocument(doc); &#125; @Test public void poreterTest() &#123; PorterStemmer ps = new PorterStemmer(); System.out.println(ps.stem()); &#125;&#125; 运行部分结果如下： 信息: init user userLibrary ok path is : D:\Multi-module-project\Lucene\src\main\resources\library\default.dic信息: init ambiguityLibrary ok!信息: init core library ok use time :281信息: init ngram ok use time :668索引建立完毕index ok to search!text:”季德胜 蛇药片”“季德胜蛇药片”:共找到1条记录! 季德胜蛇药片 10片*6板 Ansj设置词典路径 正规方式创建library.properties中增加 123#path of userLibrary 自定义词典路径userLibrary=library/userLibrary/userLibrary.dicambiguityLibrary=library/ambiguity.dic 在用词典未加载前可以通过，代码方式方式来加载 1MyStaticValue.userLibrary=[你的路径] 调用api加载。在程序运行的任何时间都可以。动态加载。 1loadLibrary.loadLibrary(String path)方式加载 路径可以是具体文件也可以是一个目录，如果是一个目录，那么会扫描目录下的dic文件自动加入词典。 Lucene集成Ansj添加自定义词典如果需要添加自己的自定义词典，参考default.dic格式即可。用户自定义词典的格式是word[tab]nature[tab]freq，example: 小李子 nr 100 分布式分词组件WordNotes：另外发现了一个分布式中文分词组件，希望以后有机会可以深入研究，地址：https://github.com/ysc/word Word分词是一个Java实现的分布式的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除歧义。能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。能通过自定义配置文件来改变组件行为，能自定义用户词库、自动检测词库变化、支持大规模分布式环境，能灵活指定多种分词算法，能使用refine功能灵活控制分词结果，还能使用词频统计、词性标注、同义标注、反义标注、拼音标注等功能。提供了10种分词算法，还提供了10种文本相似度算法，同时还无缝和Lucene、Solr、ElasticSearch、Luke集成。注意：word1.3需要JDK1.8。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下搭建Solr5.5搜索服务]]></title>
    <url>%2F2016%2F03%2F21%2FWindows-build-Solr5-5-search-services%2F</url>
    <content type="text"><![CDATA[机器环境Windows：Win7 64 bitJava：java version “1.8.0_45”；Java HotSpot(TM) 64-Bit Server VMSolr：5.5Lucene：5.5Tomcat：8.0.32 Lucene和Solr下载地址：http://lucene.apache.org/Windows选择下载zip压缩包，Linux选择下载tgz压缩包Tomcat下载地址：http://tomcat.apache.org/ ，选择Binary Distributions下的Core中的64-bit Windows zip (pgp, md5, sha1)下载之后文件名称是：apache-tomcat-8.0.32-windows-x64 Windows下启动Tomcat并添加服务配置环境变量注意JDK和Tomcat的目录中最好别有中文，首先在系统环境变量中配置JAVA_HOME，值是C:/Program Files/Java/jdk1.8.0_45，然后在Path中添加;%JAVA_HOME%/bin。 添加环境变量CATALINA_HOME值是D:/apache-tomcat-8.0.32，然后点击 tomcat的bin下的startup.bat就应该可以启动tomcat了。 添加Tomcat到Windows服务如果想把tomcat添加到Windows服务中，需要打开cmd运行Tomcat的bin目录下的service.bat脚本，安装命令如下，其中tomcat8是自己起的服务名称，当然也可以使用其它名称 D:\apache-tomcat-8.0.32&gt;cd binD:\apache-tomcat-8.0.32\bin&gt;service.bat install tomcat8Installing the service ‘tomcat8’ …Using CATALINA_HOME: “D:\apache-tomcat-8.0.32”Using CATALINA_BASE: “D:\apache-tomcat-8.0.32”Using JAVA_HOME: “C:\Program Files\Java\jdk1.8.0_45”Using JRE_HOME: “C:\Program Files\Java\jdk1.8.0_45\jre”Using JVM: “C:\Program Files\Java\jdk1.8.0_45\jre\bin\server\jvm.dll”The service ‘tomcat8’ has been installed. 卸载服务命令如下 D:\apache-tomcat-8.0.32\bin&gt;service.bat remove tomcat8Removing the service ‘tomcat8’ …Using CATALINA_BASE: “D:\apache-tomcat-8.0.32”The service ‘tomcat8’ has been removed 配置Tomcat管理员在Tomcat启动之后，可以在浏览器中输入http://localhost:8080/ 查看是否配置成功，如果出现Tomcat主界面，点击Manager App，提示权限不足，此时需要在D:/apache-tomcat-8.0.32/conf/tomcat-users.xml文件中添加如下内容12&lt;role rolename="manager-gui"/&gt;&lt;user username="admin" password="admin" roles="manager-gui"/&gt; 重启Tomcat，打开浏览器点击Manager App会弹出登录框，使用配置的用户名和密码登录即可。 Tomcat启动Solr 解压从官网下载的solr-5.5.0.zip压缩包，解压到D:/solr-5.5.0 复制solr-5.5.0/server/solr-webapp/webapp到tomcat下的webapps目录下，改名为solr 将solr-5.5.0/server/lib/ext/目录下的所有jar包复制到tomcat/webapps/solr/WEB-INF/lib/下 将solr-5.5.0/server/solr目录复制到D:/apache-tomcat-8.0.32/bin目录下，这个就是传说中的solr/home(存放检索数据) 设置solr/home：编辑D:/apache-tomcat-8.0.32/webapps/solr/WEB-INF/web.xml文件，以下部分是注释的，打开该注释，并修改env-entry-value。solr在启动的时候会去这个根目录下加载配置信息。 12345&lt;env-entry&gt; &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt; &lt;env-entry-value&gt;D:/apache-tomcat-8.0.32/bin/solr&lt;/env-entry-value&gt; &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt;&lt;/env-entry&gt; 将solr-5.5.0/server/resources下的log4j.properties文件复制到tomcat/webapps/solr/WEB-INF/classes目录下，如果该目录不存在则新建。 将solr-5.5.0/dist目录下的solr-dataimporthandler-5.5.0.jar和solr-dataimporthandler-extras-5.5.0.jar复制到tomcat/webapps/solr/WEB-INF/lib/下，这个是为了以后导入数据库表数据。配置完成之后，Solr环境搭建完毕，重启Tomcat，在浏览器中输入http://localhost:8080/solr/index.html#/ 就可以看到Solr的控制台了 在tomcat/solrhome/目录下创建core(自定义)，在其目录下创建data文件夹，并将D:/apache-tomcat-8.0.32/bin/solr/configsets/basic_configs/目录下的conf文件夹复制到core下。然后在solr控制台点击Add Core即可完成创建。 Jetty启动SolrSolr自带Jetty服务器，并且提供了可运行的jar包，该jar包存在于D:/solr-5.5.0/server目录中，但是不能直接双击该jar包运行，需要在命令行中进行启动。打开cmd命令行，按照网上提示，输入启动命令 d:\solr-5.5.0\server&gt;java -jar start.jarWARNING: Nothing to start, exiting …Usage: java -jar start.jar [options] [properties] [configs] java -jar start.jar –help # for more information 根据提示信息，该命令并不能启动jar包，原因是在新版的Solr中并不能以这种方式启动，具体细节可以参见Solr Quick Start以及StackOverflow上面的问答“Nothing to start” when trying to start Apache Solr，根据以上信息，我们采用如下方式启动 d:\solr-5.5.0\bin&gt;solr start -e cloud -nopromptWelcome to the SolrCloud example!Starting up 2 Solr nodes for your example SolrCloud cluster.Creating Solr home directory d:\solr-5.5.0\example\cloud\node1\solrCloning d:\solr-5.5.0\example\cloud\node1 into d:\solr-5.5.0\example\cloud\node2Starting up Solr on port 8983 using command:d:\solr-5.5.0\bin\solr.cmd start -cloud -p 8983 -s “d:\solr-5.5.0\example\cloud\node1\solr”Waiting up to 30 to see Solr running on port 8983Starting up Solr on port 7574 using command:d:\solr-5.5.0\bin\solr.cmd start -cloud -p 7574 -s “d:\solr-5.5.0\example\cloud\node2\solr” -z localhost:9983Started Solr server on port 8983. Happy searching!Waiting up to 30 to see Solr running on port 7574Started Solr server on port 7574. Happy searching!Connecting to ZooKeeper at localhost:9983 …Uploading d:\solr-5.5.0\server\solr\configsets\data_driven_schema_configs\conf for config gettingstarted to ZooKeeper at localhost:9983Creating new collection ‘gettingstarted’ using command:http://localhost:8983/solr/admin/collections?action=CREATE&amp;name=gettingstarted&amp;numShards=2&amp;replicationFactor=2&amp;maxShardsPerNode=2&amp;collection.configName=gettingstarted{ “responseHeader”:{ “status”:0, “QTime”:6080}, “success”:{“”:{ “responseHeader”:{ “status”:0, “QTime”:5576}, “core”:”gettingstarted_shard2_replica2”}}}Enabling auto soft-commits with maxTime 3 secs using the Config APIPOSTing request to Config API: http://localhost:8983/solr/gettingstarted/config{“set-property”:{“updateHandler.autoSoftCommit.maxTime”:”3000”}}Successfully set-property updateHandler.autoSoftCommit.maxTime to 3000SolrCloud example running, please visit: http://localhost:8983/solr 成功启动之后，打开浏览器，访问http://localhost:8983/solr 即可看到Solr的控制台界面。 如果不想使用默认的端口8983，那么使用如下命令通过-p参数指定端口号同样可以启动Solr Server，访问http://localhost:8080/solr/index.html#/ 就可以看到主控界面了12345678D:\solr-5.5.0\bin&gt;solr.cmd start -cloud -p 8080 -s "D:\solr-5.5.0\example\cloud\node1\solr"Backing up D:\solr-5.5.0\example\cloud\node1\solr\..\logs\solr.log移动了 1 个文件。Backing up D:\solr-5.5.0\example\cloud\node1\solr\..\logs\solr_gc.log移动了 1 个文件。Waiting up to 30 to see Solr running on port 8080Started Solr server on port 8080. Happy searching! 或者直接修改jetty的配置文件，修改D:/solr-5.5.0/server/etc目录下jetty-http.xml中的8983改为自己希望的端口号即可。 参考文献[1] http://blog.csdn.net/luenxin/article/details/50838895]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客主题从Jacman切换到NexT.Mist]]></title>
    <url>%2F2016%2F03%2F20%2FHexo-blog-theme-switching-from-Jacman-to-NexT-Mist%2F</url>
    <content type="text"><![CDATA[序言难得一个周末，经过五天的工作之后，本来是打算周六周日在睡觉中度过，因为最近工作确实挺累，倒不是工作有多繁忙，而是路途遥远，每天上下班路上花费三个小时，特别是早高峰挤地铁，那真是个体力活啊，每天都是七号线静安寺转二号线，换乘之前，并不能太深入地铁车厢内部，否则到了换乘站你可能根本就挤不下车去了。 最近博客流量有渐增趋势，可能是之前未被百度收录，而近期被百度收录的缘故吧，可见我天朝还是百度的天下，哎，万恶的墙啊（Great Firewall of China）！流量渐增，访客自然增加，所以也会有猎头加我微信，我也在考虑要不要把我的个人信息撤掉，貌似泄露的有点多了。同时也会有一些人因为博客的内容加我，请教谈不上，可能是一些同学在做相同的事情的过程中遇到了一些问题，而我可能刚好做过这方面的事情而已，希望有事的同学或朋友直接电联我即可，因为这是我认为最节约我们双方时间的沟通方式，确实没有太多时间去和你在微信上慢慢切磋。 貌似也有段时间没有发文了，主要是希望以后可以进一步提高博文质量，尽量发表一些互联网上还不存在或者没有解决方案类的文章，这样不至于人云亦云，还可以给别人提供比较多的参考价值，至于互联网上已经泛滥成灾的一些信息，以后在博文中尽量不再予以提供，不做重复发明轮子的工作。 扯远了，本来是打算睡觉度周末来着，但是博客的Jacman主题越看越觉得不顺眼，始终觉得有一种too young、too simple、sometimes naive的感觉。而我素来想要一个简洁的主题，终于发现NexT.Mist主题足够优雅、足够简洁、是我喜爱的风格，于是果断手贱，开始折腾起来，还好在有以前折腾的基础之上，再次折腾就比较简单了，好多东西已经驾熟就轻了，本着能省力就省力的原则，很快更改完毕，主要还有一些细节在调整中。 说一点最近有待提高的地方就是对时间的把控，每天都有大把的时间花费在了各种软文之中，越来越觉得实在是一种浪费。自从微信火了之后，相信大家越来越觉得时间不够用了，到处都是软文，到处都是鸡汤，到处都是某人生活中的苦难，我不是说苦难不好，也不是说你的苦难不应该公之于众，而是在这个社会，苦难太多，我们不能靠着每天去消费苦难而存活，你有时间把你的苦难公之于众供大家消费，倒不如去干点活来的实在，这就是我的观点，也是我本人行动的格言，所以在这个大家即将毕业的季节，同学们旅游的旅游，回家的回家，玩的玩，泡妞的泡妞，憋在宿舍整天打游戏的也大有人在，而我还是整天苦逼哈哈，一天不拉的跑去公司上班，赶着来回三个多小时的地铁，每天挤得像狗一样累的半死，不为别的，只因为我知道，我不努力没人替我努力，而且还有那么好的媳妇在等着我去给她幸福呢。 所以我还是那句话，人生的竞赛没有公平可言，但是只要起步了，就只顾风雨兼程。在不忘记享受沿途风景的时候，大家也要记得赶路哦。 最后把我本次更改主题参考的文章放在末尾，有需要的自行点击，不谢。 NexT.Mist主题相关本次更改主题主要参考如下内容 NexT官方指南 在博文标题下面添加文章热度方法参考这里 在站点概览中添加友链参考这里 设置社交链接，参考这里 需要添加音乐播放器和High一下小玩意参考这里 有关Hexo更详细的配置参考这里 Hexo更改默认Google字体库，参考这里 利用Git解决Hexo博客多PC间同步问题，参考这里 关于多说评论框的美化以及评论用户的操作系统和浏览器型号，参考这里 使用多说给博客添加最近访客功能，参考这里 给博客创建留言页面，参考这里 开启打赏功能，参考这里 多说评论显示UA，即用户操作系统和浏览器版本参考这里 Favicon设置后没有生效？将你的favicon放置到站点的source目录下，如我的站点D:\hexo\themes\next\source\favicon.ico，并在D:\hexo\themes\next\_config.yml中启用favicon: /favicon.ico即可 如何添加非默认菜单的Menu icon呢？例如我的菜单中加入了简历和友链，但是并不知道这两者对应的图标是什么，很简单，点击Awesome Icons 去挑选你自己喜爱的图标即可，然后去掉图标名称前面的icon-，只利用后面的名字即可，例如我的配置： 1234567891011menu_icons: enable: true # Icon Mapping. home: home about: user categories: th tags: tags archives: archive commonweal: heartbeat links: link resume: book 修改侧边栏头像下面的字体及颜色参考这里，修改的文件具体位置是在D:\hexo\themes\next\source\css\_common\components\sidebar\sidebar-author.styl中的font-size，将其调大即可。 123456789.site-description &#123; margin-top: 5px; font-size: 16px; color: $site-author-name-color; /* font-size: $site-description-font-size; color: $site-description-color; */&#125; 修改代码块中字体大小，主题默认的代码块字体偏小，找到D:\hexo\themes\next\source\css\_common\components\highlight\highlight.styl，修改font-size属性即可 12345678$code-block background: highlight-background margin: 20px 0 padding: 15px overflow: auto font-size 15px color: highlight-foreground line-height: $line-height-code-block 为博客添加搜索框，进入swiftype，点击自己的Engine-&gt;Install Search-&gt;Search Field后的edit，选择Element ID，填#st-search-input之后选Save，继续跳转后点Activate Swiftype按钮即可完成swiftype的所有配置了。拷贝你的Engine的Install Search中得到的代码如下 123456(function(w,d,t,u,n,s,e)&#123;w['SwiftypeObject']=n;w[n]=w[n]||function()&#123;(w[n].q=w[n].q||[]).push(arguments);&#125;;s=d.createElement(t);e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);&#125;)(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');_st('install','这里是KEY','2.0.0'); 然后在Next.Mist主题下的_config.xml中启用即可如下：swiftype_key: 这里是KEY 把侧边栏头像变成圆形，并且鼠标停留在上面发生旋转效果，参考这里，具体修改文件的位置是D:\hexo\themes\next\source\css\_common\components\sidebar\sidebar-author.styl中的内容如下 12345678910111213141516171819202122232425.site-author-image &#123; display: block; margin: 0 auto; max-width: 96px; height: auto; border: 2px solid #333; padding: 2px; /* start*/ border-radius: 50% webkit-transition: 1.4s all; moz-transition: 1.4s all; ms-transition: 1.4s all; transition: 1.4s all; /* end */&#125;/* start */.site-author-image:hover &#123; background-color: #55DAE1; webkit-transform: rotate(360deg) scale(1.1); moz-transform: rotate(360deg) scale(1.1); ms-transform: rotate(360deg) scale(1.1); transform: rotate(360deg) scale(1.1);&#125;/* end */ 添加右上角Fork me on GitHub，将如下代码添加到D:\hexo\themes\next\layout\_layout.swig底部的body标签之内即可，注意修改href为你自己的链接 12345&lt;a href="https://github.com/shijiebei2009"&gt;&lt;img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/365986a132ccd6a44c23a9169022c0b5c890c387/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f7265645f6161303030302e706e67"alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png"&gt;&lt;/a&gt; mathjax数学公式后面有竖线的问题（在chrome中显示有竖线，在猎豹、Firefox中显示正常）这个问题已经发了Issue，可以参考官方提供的解决方案。首先，编辑next的主题配置文件_config.yml，在里面找到mathjax的部分，替换为以下内容： 123mathjax: enable: true cdn: //cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML 打开\themes\next\layout\_scripts\third-party\mathjax.swig，将其内容替换为： 1234567891011121314151617181920&#123;% if theme.mathjax.enable %&#125; &lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Config(&#123; tex2jax: &#123; inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125; &#125;); &lt;/script&gt; &lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Queue(function() &#123; var all = MathJax.Hub.getAllJax(), i; for (i=0; i &lt; all.length; i += 1) &#123; all[i].SourceElement().parentNode.className += ' has-jax'; &#125; &#125;); &lt;/script&gt; &lt;script type="text/javascript" src="&#123;&#123; theme.mathjax.cdn &#125;&#125;"&gt;&lt;/script&gt;&#123;% endif %&#125; 之后清理，重新发布即可去除数学公式后面的竖线。 为Next主题添加版权信息对于Hexo Next主题而言先找到/themes/next/layout/_macro/post.swig，再找到其中的打赏部分代码，如下所示 12345&lt;div&gt; &#123;% if ! is_index %&#125; &#123;% include 'reward.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 然后直接在其上面添加如下代码段： 123456789&lt;div align="center"&gt; &#123;% if not is_index %&#125; &lt;div class="copyright"&gt; &lt;p&gt;&lt;span&gt; &lt;b&gt;本文地址：&lt;/b&gt;&lt;a href="&#123;&#123; url_for(page.path) &#125;&#125;" title="&#123;&#123; page.title &#125;&#125;"&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt;&lt;br/&gt;&lt;b&gt;转载请注明出处，谢谢！&lt;/b&gt; &lt;/span&gt;&lt;/p&gt; &lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 为Next博客添加广告位如果你的博客访问量比较大的话，添加一个广告位也未尝不可，若有收入则可以用来抵扣域名续费或者服务器的费用，首先找到D:/hexo/themes/next/layout/_macro下的post.swig文件，在该文件中找到你认为合适放广告的位置添加如下代码 123456&lt;!-- 添加广告位 --&gt;&lt;div&gt; &#123;% if ! is_index %&#125; &#123;% include 'advertisement.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 该段代码会引入存有广告代码的文件，新建advertisement.swig文件，将你的广告代码置于其中即可。 为Next博客添加访客统计使用不蒜子网页计数器，统计代码非常简单，参考这里 为Next博客添加Font Awesome图标不知道你有没有注意到我的网站底部的那个小人和眼睛呢？其实这是Font Awesome图标，只需要结合不蒜子网页计数器即可实现，非常简单，直接贴出代码，这段代码在D:/hexo/themes/next/layout/_partials/footer.swig中，只需要将你需要的部分添加到你的模板对应文件中即可，如下所示 1234567891011121314151617&lt;div class="theme-info"&gt; &#123;&#123; __('footer.theme') &#125;&#125; - &lt;a class="theme-link" href="https://github.com/iissnan/hexo-theme-next"&gt; NexT.&#123;&#123; theme.scheme &#125;&#125; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;span id="busuanzi_container_site_pv"&gt; &lt;i class="fa fa-user" aria-hidden="true"&gt;&lt;/i&gt; &lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt; &lt;/span&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;span id="busuanzi_container_site_uv"&gt; &lt;i class="fa fa-eye" aria-hidden="true"&gt;&lt;/i&gt; &lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt; &lt;/span&gt;&lt;/div&gt;&lt;script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 修改首页、归档页、分类页、Tag页显示文章数量默认地首页归档页仅显示10篇文章，然后要不断向后翻页，用起来非常不爽，经本人测试，设置数量在20篇比较合适，这个可以由hexo的D:/hexo/_config.yml来控制，修改如下 12per_page: 20pagination_dir: page 修改文章底部的前一篇、下一篇为两端对齐在Next主题的5.0.0版本中，下一篇默认为靠右对齐，看起来非常丑陋，因为前一篇已经设置为靠右对齐了，对称地，这时候，下一篇应该设置为靠左对齐才对。修改D:/hexo/themes/next/source/css/_common/components/post/post-nav.styl文件，拉到末尾，设置如下： 12.post-nav-next &#123; text-align: left; &#125;.post-nav-prev &#123; text-align: right; &#125; 这样看起来就非常对称啦。]]></content>
      <categories>
        <category>Git/GitHub</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Java NIO2实现的异步非阻塞消息通信框架]]></title>
    <url>%2F2016%2F02%2F26%2FAsynchronous-non-blocking-message-communication-framework-based-on-Java-NIO2%2F</url>
    <content type="text"><![CDATA[前奏因为NIO并不容易掌握，所以这注定会是一篇长文，而且即便篇幅很大，亦难以把很多细节解释清楚，只能侧重于从整体上进行把握，并实现一个简单的客户端服务端消息通信框架作为例子，以便有需要的开发人员参考之。借用淘宝伯岩给出的忠告就是 尽量不要尝试实现自己的NIO框架，除非有经验丰富的工程师 尽量使用经过广泛实践的开源NIO框架Mina/Netty/xSocket 尽量使用最新版稳定版JDK 遇到问题的时候，可以先看下Java的Bug Database Asynchronous I/O是在JDK7中提出的异步非阻塞I/O，习惯上称之为NIO2，也叫AIO，AIO是对JDK1.4中提出的同步非阻塞I/O的进一步增强，主要包括 更新的Path类，该类在NIO里对文件系统进行了进一步的抽象，用来替换原来的java.io.File，可以通过File.toPath()和Path.toFile()将File和Path进行相互转换 File Attributes，java.nio.file.attribute针对文件属性提供了各种用户所需的元数据，不同操作系统使用的类不太一样，支持的属性分类有 BasicFileAttributeView DosFileAttributeView PosixFileAttributeView FileOwnerAttributeView AclFileAttributeView UserDefinedFileAttributeView Symbolic and Hard Links，相当于用Java程序实现Linux中的ln命令 Watch Service API，作为一个线程安全的服务用于监控对象的变化和事件，以前直接用Java监控文件系统的变化是不可能的，只能通过JNI的方式调用操作系统的API，而在JDK7中这部分被加入到了标准库里 Random Access Files主要提供了一个SeekableByteChannel接口，配合ByteBuffer使得随机访问文件更加方便 Sockets API主要是NIO1中的Selector模式实现同步非阻塞 Asynchronous Channel API由NIO1中的Selector模式变成方法回调模式，使用更加方便，主要是可以异步实现文件的读写了 AIO应用开发Future方式Future是在JDK1.5中加入Java并发包的，该接口提供get()方法用于获取任务完成之后的处理结果。在AIO中，可以接受一个I/O连接请求，返回一个Future对象，然后可以基于该返回对象进行后续的操作，包括使其阻塞、查看是否完成、超时异常，使用方式如下。服务端代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.channels.AsynchronousServerSocketChannel;import java.nio.channels.AsynchronousSocketChannel;import java.nio.charset.Charset;import java.util.concurrent.ExecutionException;import java.util.concurrent.Future;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/24 16:57 * &lt;/p&gt; * &lt;p&gt; * ClassName:ServerOnFuture * &lt;/p&gt; * &lt;p&gt; * Description:基于Future的NIO2服务端实现，此时的服务端还无法实现多客户端并发，如果有多个客户端并发连接该服务端的话， * 客户端会出现阻塞，待前一个客户端处理完毕，服务端才会接受下一个客户端的连接并处理 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2public class ServerOnFuture &#123; static final int DEFAULT_PORT = 7777; static final String IP = "127.0.0.1"; static ByteBuffer buffer = ByteBuffer.allocateDirect(1024); public static void main(String[] args) &#123; try (AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open()) &#123; if (serverSocketChannel.isOpen()) &#123; serverSocketChannel.setOption(StandardSocketOptions.SO_REUSEADDR, true); serverSocketChannel.bind(new InetSocketAddress(IP, DEFAULT_PORT)); log.info("Waiting for connections..."); while (true) &#123; Future&lt;AsynchronousSocketChannel&gt; channelFuture = serverSocketChannel.accept(); try (AsynchronousSocketChannel socketChannel = channelFuture.get()) &#123; log.info("Incoming connection from : " + socketChannel.getRemoteAddress()); while (socketChannel.read(buffer).get() != -1) &#123; buffer.flip(); // Java NIO2或者Java AIO报： java.util.concurrent.ExecutionException: java.io.IOException: 指定的网络名不再可用。 // 此处要注意，千万不能直接操作buffer，否则客户端会阻塞并报错，“java.util.concurrent.ExecutionException: java.io.IOException: 指定的网络名不再可用。” ByteBuffer duplicate = buffer.duplicate(); showMessage(duplicate); socketChannel.write(buffer).get(); if (buffer.hasRemaining()) &#123; buffer.compact(); &#125; else &#123; buffer.clear(); &#125; &#125; log.info(socketChannel.getRemoteAddress() + " was successfully served!"); &#125; catch (InterruptedException | ExecutionException e) &#123; log.error(e); &#125; &#125; &#125; else &#123; log.warn("The asynchronous server-socket channel cannot be opened!"); &#125; &#125; catch (IOException e) &#123; log.error(e); &#125; &#125; protected static void showMessage(ByteBuffer buffer) &#123; CharBuffer decode = Charset.defaultCharset().decode(buffer); log.info(decode.toString()); &#125;&#125; 客户端代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.channels.AsynchronousSocketChannel;import java.nio.charset.Charset;import java.util.Random;import java.util.concurrent.ExecutionException;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/24 17:48 * &lt;/p&gt; * &lt;p&gt; * ClassName:ClientOnFuture * &lt;/p&gt; * &lt;p&gt; * Description:基于Future的NIO2客户端实现 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2public class ClientOnFuture &#123; static final int DEFAULT_PORT = 7777; static final String IP = "127.0.0.1"; static ByteBuffer buffer = ByteBuffer.allocateDirect(1024); public static void main(String[] args) &#123; try (AsynchronousSocketChannel socketChannel = AsynchronousSocketChannel.open()) &#123; if (socketChannel.isOpen()) &#123; //设置一些选项，非必选项，可使用默认设置 socketChannel.setOption(StandardSocketOptions.SO_RCVBUF, 128 * 1024); socketChannel.setOption(StandardSocketOptions.SO_SNDBUF, 128 * 1024); socketChannel.setOption(StandardSocketOptions.SO_KEEPALIVE, true); Void aVoid = socketChannel.connect(new InetSocketAddress(IP, DEFAULT_PORT)).get(); //返回null表示连接成功 if (aVoid == null) &#123; Integer messageLength = socketChannel.write(ByteBuffer.wrap("Hello Server！".getBytes())).get(); log.info(messageLength); while (socketChannel.read(buffer).get() != -1) &#123; buffer.flip();//写入buffer之后，翻转，之后可以从buffer读取，或者将buffer内容写入通道 CharBuffer decode = Charset.defaultCharset().decode(buffer); log.info(decode.toString()); if (buffer.hasRemaining()) &#123; buffer.compact(); &#125; else &#123; buffer.clear(); &#125; int r = new Random().nextInt(1000); if (r == 50) &#123; log.info("Client closed!"); break; &#125; else &#123; // Java NIO2或者Java AIO报： Exception in thread "main" java.nio.channels.WritePendingException // 此处注意，如果在频繁调用write()的时候，在上一个操作没有写完的情况下，调用write会触发WritePendingException异常， // 所以此处最好在调用write()之后调用get()以便明确等到有返回结果 socketChannel.write(ByteBuffer.wrap("Random number : ".concat(String.valueOf(r)).getBytes())).get(); &#125; &#125; &#125; else &#123; log.warn("The connection cannot be established!"); &#125; &#125; else &#123; log.warn("The asynchronous socket-channel cannot be opened!"); &#125; &#125; catch (IOException | InterruptedException | ExecutionException e) &#123; log.error(e); &#125; &#125;&#125; Future方式实现为多客户端并发服务如何让服务端同时可以接受多个客户端的连接呢？一个简单的处理方法就是使用ExecutorService。每次新建一个连接，并且获得返回值之后，这个返回值就是一个AsynchronousSocketChannel的通道，将其提交给线程池，由一个工作线程进行后续处理。然后一个新的线程准备好在等待接受下一个连接。代码示例如下。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.channels.AsynchronousServerSocketChannel;import java.nio.channels.AsynchronousSocketChannel;import java.nio.charset.Charset;import java.util.concurrent.*;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/24 17:38 * &lt;/p&gt; * &lt;p&gt; * ClassName:ServerOnFutureForMultiClients * &lt;/p&gt; * &lt;p&gt; * Description:基于Future实现的可以接受多客户端并发的Java NIO2服务端实现 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2public class ServerOnFutureForMultiClients &#123; static final int DEFAULT_PORT = 7777; static final String IP = "127.0.0.1"; static ExecutorService taskExecutorService = Executors.newCachedThreadPool(Executors.defaultThreadFactory()); static ByteBuffer buffer = ByteBuffer.allocateDirect(1024); public static void main(String[] args) &#123; try (AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open()) &#123; if (serverSocketChannel.isOpen()) &#123; serverSocketChannel.setOption(StandardSocketOptions.SO_REUSEADDR, true); serverSocketChannel.bind(new InetSocketAddress(IP, DEFAULT_PORT)); log.info("Waiting for connections..."); while (true) &#123; Future&lt;AsynchronousSocketChannel&gt; socketChannelFuture = serverSocketChannel.accept(); try &#123; final AsynchronousSocketChannel socketChannel = socketChannelFuture.get(); Callable&lt;String&gt; worker = new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; String s = socketChannel.getRemoteAddress().toString(); log.info("Incoming connection from : " + s); while (socketChannel.read(buffer).get() != -1) &#123; buffer.flip(); ByteBuffer duplicate = buffer.duplicate(); showMessage(duplicate); socketChannel.write(buffer).get(); if (buffer.hasRemaining()) &#123; buffer.compact(); &#125; else &#123; buffer.clear(); &#125; &#125; socketChannel.close(); log.info(s + " was successfully served!"); return s; &#125; &#125;; taskExecutorService.submit(worker); &#125; catch (InterruptedException | ExecutionException e) &#123; log.error(e); taskExecutorService.shutdown(); while (!taskExecutorService.isTerminated()) &#123; &#125; break; &#125; &#125; &#125; else &#123; log.warn("The asynchronous server-socket channel cannot be opened!"); &#125; &#125; catch (IOException e) &#123; log.error(e); &#125; &#125; protected static void showMessage(ByteBuffer buffer) &#123; CharBuffer decode = Charset.defaultCharset().decode(buffer); log.info(decode.toString()); &#125;&#125; Callback方式方法回调模式，即提交一个I/O操作请求，并且指定一个CompletionHandler。当异步操作完成时，便会发一个通知，此时该CompletionHandler对象覆写的方法将被调用，如果成功调用completed方法，如果失败调用failed方法，首先看下Java API。12345678910111213141516171819202122public interface CompletionHandler&lt;V,A&gt; &#123; /** * Invoked when an operation has completed. * * @param result 操作结果 * The result of the I/O operation. * @param attachment 提交请求时的参数，通常会封装一个连接环境 * The object attached to the I/O operation when it was initiated. */ void completed(V result, A attachment); /** * Invoked when an operation fails. * * @param exc * The exception to indicate why the I/O operation failed * @param attachment * The object attached to the I/O operation when it was initiated. */ void failed(Throwable exc, A attachment);&#125; AIO提供了四种类型的异步通道以及不同的I/O操作可以接收一个CompletionHandler对象，分别是： AsynchronousSocketChannel：connect，read，write AsynchronousFileChannel：lock，read，write AsynchronousServerSocketChannel：accept AsynchronousDatagramChannel：read，write，send，receive 服务端示例代码如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.channels.AsynchronousServerSocketChannel;import java.nio.channels.AsynchronousSocketChannel;import java.nio.channels.CompletionHandler;import java.nio.charset.Charset;import java.util.concurrent.ExecutionException;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/24 20:14 * &lt;/p&gt; * &lt;p&gt; * ClassName:ServerOnCompletionHandler * &lt;/p&gt; * &lt;p&gt; * Description:基于CompletionHandler实现NIO2的服务端 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2public class ServerOnCompletionHandler &#123; static final int DEFAULT_PORT = 7777; static final String IP = "127.0.0.1"; public static void main(String[] args) &#123; try (final AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open()) &#123; if (serverSocketChannel.isOpen()) &#123; serverSocketChannel.setOption(StandardSocketOptions.SO_RCVBUF, 4 * 1024); serverSocketChannel.setOption(StandardSocketOptions.SO_REUSEADDR, true); serverSocketChannel.bind(new InetSocketAddress(IP, DEFAULT_PORT)); log.info("Waiting for connections..."); serverSocketChannel.accept(null, new CompletionHandler&lt;AsynchronousSocketChannel, Void&gt;() &#123; final ByteBuffer buffer = ByteBuffer.allocateDirect(1024); @Override public void completed(AsynchronousSocketChannel socketChannel, Void attachment) &#123; //注意接收一个连接之后，紧接着可以接收下一个连接，所以必须再次调用accept方法 serverSocketChannel.accept(null, this); try &#123; log.info("Incoming connection from : " + socketChannel.getRemoteAddress()); while (socketChannel.read(buffer).get() != -1) &#123; buffer.flip(); final ByteBuffer duplicate = buffer.duplicate(); final CharBuffer decode = Charset.defaultCharset().decode(duplicate); log.info(decode.toString()); socketChannel.write(buffer).get(); if (buffer.hasRemaining()) &#123; buffer.compact(); &#125; else &#123; buffer.clear(); &#125; &#125; &#125; catch (InterruptedException | ExecutionException | IOException e) &#123; log.error(e); &#125; finally &#123; try &#123; socketChannel.close(); &#125; catch (IOException e) &#123; log.error(e); &#125; &#125; &#125; @Override public void failed(Throwable exc, Void attachment) &#123; serverSocketChannel.accept(null, this); throw new UnsupportedOperationException("Cannot accept connections!"); &#125; &#125;); //主要是阻塞作用，因为AIO是异步的，所以此处不阻塞的话，主线程很快执行完毕，并会关闭通道 System.in.read(); &#125; else &#123; log.warn("The asynchronous server-socket channel cannot be opened!"); &#125; &#125; catch (IOException e) &#123; log.error(e); &#125; &#125;&#125; 客户端代码如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.channels.AsynchronousSocketChannel;import java.nio.channels.CompletionHandler;import java.nio.charset.Charset;import java.util.Random;import java.util.concurrent.ExecutionException;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/25 10:13 * &lt;/p&gt; * &lt;p&gt; * ClassName:ClientOnCompletionHandler * &lt;/p&gt; * &lt;p&gt; * Description:基于匿名内部类形式的CompletionHandler客户端实现 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2public class ClientOnCompletionHandler &#123; static final int DEFAULT_PORT = 7777; static final String IP = "127.0.0.1"; public static void main(String[] args) &#123; try (final AsynchronousSocketChannel socketChannel = AsynchronousSocketChannel.open()) &#123; if (socketChannel.isOpen()) &#123; socketChannel.setOption(StandardSocketOptions.SO_RCVBUF, 128 * 1024); socketChannel.setOption(StandardSocketOptions.SO_SNDBUF, 128 * 1024); socketChannel.setOption(StandardSocketOptions.SO_KEEPALIVE, true); socketChannel.connect(new InetSocketAddress(IP, DEFAULT_PORT), null, new CompletionHandler&lt;Void, Void&gt;() &#123; final ByteBuffer buffer = ByteBuffer.allocateDirect(1024); @Override public void completed(Void result, Void attachment) &#123; try &#123; log.info("Successfully connected at : " + socketChannel.getRemoteAddress()); socketChannel.write(ByteBuffer.wrap("Hello Server！".getBytes())).get(); while (socketChannel.read(buffer).get() != -1) &#123; buffer.flip(); ByteBuffer duplicate = buffer.duplicate(); CharBuffer decode = Charset.defaultCharset().decode(duplicate); log.info(decode.toString());// 只要还有多余位置就可以继续从通道读入buffer，但是其实没必要，除非你要保留上一次通信的信息，一般全清空即可// if (buffer.hasRemaining()) &#123;// buffer.compact();// &#125; else &#123; buffer.clear();// &#125; int r = new Random().nextInt(1000); if (r == 50) &#123; log.warn("Client closed!"); break; &#125; else &#123; socketChannel.write(ByteBuffer.wrap("Random number ".concat(String.valueOf(r)).getBytes())).get(); &#125; &#125; &#125; catch (IOException | InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; socketChannel.close(); &#125; catch (IOException e) &#123; log.error(e); &#125; &#125; &#125; @Override public void failed(Throwable exc, Void attachment) &#123; log.error("Connection cannot be established!"); throw new UnsupportedOperationException("Connection cannot be established!"); &#125; &#125;); //如果没有可读取的数据，那么返回-1，该方法阻塞直到有可读取数据 System.in.read(); &#125; else &#123; log.warn("The asynchronous socket channel cannot be opened!"); &#125; &#125; catch (IOException e) &#123; log.error(e); &#125; &#125;&#125; Reader/Writer方式实现其实除了使用匿名内部类的形式外，还有可以指定读写者的read和write方法，另外你还可以指定超时时间，这种实现方式相对来说比匿名内部类形式看起来代码解耦合更好，代码更简洁。抽象的接口12345678910111213141516171819202122232425262728import java.nio.channels.AsynchronousSocketChannel;import java.nio.channels.CompletionHandler;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/19 17:58 * &lt;/p&gt; * &lt;p&gt; * ClassName:Callback * &lt;/p&gt; * &lt;p&gt; * Description:回调接口，顶层抽象，主要是设定两个泛型参数 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */public interface Callback extends CompletionHandler&lt;Integer, AsynchronousSocketChannel&gt; &#123; // 某种程度上说，AIO编程其实是attachment编程 @Override void failed(Throwable exc, AsynchronousSocketChannel socketChannel); @Override void completed(Integer result, AsynchronousSocketChannel socketChannel);&#125; 12345678910111213141516171819/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/25 11:19 * &lt;/p&gt; * &lt;p&gt; * ClassName:ReaderCallback * &lt;/p&gt; * &lt;p&gt; * Description:回调接口的下一层抽象，针对读操作 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */public interface ReaderCallback extends Callback &#123;&#125; 12345678910111213141516171819/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/19 18:09 * &lt;/p&gt; * &lt;p&gt; * ClassName:WriterCallback * &lt;/p&gt; * &lt;p&gt; * Description:回调接口的下一层抽象，负责写操作 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */public interface WriterCallback extends Callback &#123;&#125; 读者123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import lombok.NoArgsConstructor;import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.SocketAddress;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.channels.AsynchronousSocketChannel;import java.nio.charset.Charset;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/16 14:05 * &lt;/p&gt; * &lt;p&gt; * ClassName:Reader * &lt;/p&gt; * &lt;p&gt; * Description:负责服务端的读消息 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2@NoArgsConstructorpublic class Reader implements ReaderCallback &#123; private ByteBuffer byteBuffer; public Reader(ByteBuffer byteBuffer) &#123; log.info("An reader has been created!"); this.byteBuffer = byteBuffer; &#125; @Override public void completed(Integer result, AsynchronousSocketChannel socketChannel) &#123; log.info(String.format("Reader name : %s ", Thread.currentThread().getName())); byteBuffer.flip(); log.info("Message size : " + result); if (result != null &amp;&amp; result &lt; 0) &#123; try &#123; socketChannel.close(); &#125; catch (IOException e) &#123; log.error(e); &#125; return; &#125; try &#123; SocketAddress localAddress = socketChannel.getLocalAddress(); SocketAddress remoteAddress = socketChannel.getRemoteAddress(); log.info("localAddress : " + localAddress.toString()); log.info("remoteAddress : " + remoteAddress.toString()); socketChannel.write(byteBuffer, socketChannel, new Writer(byteBuffer)); &#125; catch (IOException e) &#123; log.error(e); &#125; ByteBuffer duplicate = byteBuffer.duplicate(); CharBuffer decode = Charset.defaultCharset().decode(duplicate); log.info("Receive message from client : " + decode.toString()); &#125; @Override public void failed(Throwable exc, AsynchronousSocketChannel attachment) &#123; log.error(exc); throw new RuntimeException(exc); &#125;&#125; 写者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import lombok.NoArgsConstructor;import lombok.extern.log4j.Log4j2;import java.nio.ByteBuffer;import java.nio.channels.AsynchronousSocketChannel;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/16 14:05 * &lt;/p&gt; * &lt;p&gt; * ClassName:Writer * &lt;/p&gt; * &lt;p&gt; * Description:负责服务端的写操作 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2@NoArgsConstructorpublic class Writer implements WriterCallback &#123; private ByteBuffer byteBuffer; public Writer(ByteBuffer byteBuffer) &#123; this.byteBuffer = byteBuffer; log.info("A writer has been created!"); &#125; @Override public void completed(Integer result, AsynchronousSocketChannel socketChannel) &#123; log.debug("Message write successfully, size = " + result); log.info(String.format("Writer name : %s ", Thread.currentThread().getName())); byteBuffer.clear(); socketChannel.read(byteBuffer, socketChannel, new Reader(byteBuffer)); &#125; @Override public void failed(Throwable exc, AsynchronousSocketChannel socketChannel) &#123; log.error(exc); throw new RuntimeException(exc); &#125;&#125; 服务端代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.channels.AsynchronousServerSocketChannel;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/25 10:53 * &lt;/p&gt; * &lt;p&gt; * ClassName:ServerOnReaderAndWriter * &lt;/p&gt; * &lt;p&gt; * Description:基于读写类实现的NIO2服务端 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2public class ServerOnReaderAndWriter &#123; static final int DEFAULT_PORT = 7777; static final String IP = "127.0.0.1"; public static void main(String[] args) &#123; try (AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open()) &#123; if (serverSocketChannel.isOpen()) &#123; //服务端的通道支持两种选项SO_RCVBUF和SO_REUSEADDR，一般无需显式设置，使用其默认即可，此处仅为展示设置方法 //在面向流的通道中，此选项表示在前一个连接处于TIME_WAIT状态时，下一个连接是否可以重用通道地址 serverSocketChannel.setOption(StandardSocketOptions.SO_REUSEADDR, true); //设置通道接收的字节大小 serverSocketChannel.setOption(StandardSocketOptions.SO_RCVBUF, 8 * 1024); serverSocketChannel.bind(new InetSocketAddress(IP, DEFAULT_PORT)); log.info("Waiting for connections..."); serverSocketChannel.accept(serverSocketChannel, new Acceptor()); System.in.read(); &#125; else &#123; log.warn("The connection cannot be opened!"); &#125; &#125; catch (IOException e) &#123; log.error(e); &#125; &#125;&#125; 客户端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.channels.AsynchronousSocketChannel;import java.nio.charset.Charset;import java.util.Random;import java.util.concurrent.ExecutionException;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/25 15:15 * &lt;/p&gt; * &lt;p&gt; * ClassName:ClientOnReaderAndWriter * &lt;/p&gt; * &lt;p&gt; * Description:基于读写类的NIO2客户端实现 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2public class ClientOnReaderAndWriter &#123; static final int DEFAULT_PORT = 7777; static final String IP = "127.0.0.1"; static ByteBuffer buffer = ByteBuffer.allocateDirect(1024); public static void main(String[] args) &#123; try (AsynchronousSocketChannel socketChannel = AsynchronousSocketChannel.open()) &#123; Void aVoid = socketChannel.connect(new InetSocketAddress(IP, DEFAULT_PORT)).get(); if (socketChannel.isOpen()) &#123; if (aVoid == null) &#123; socketChannel.setOption(StandardSocketOptions.SO_RCVBUF, 128 * 1024); socketChannel.setOption(StandardSocketOptions.SO_SNDBUF, 128 * 1024); socketChannel.setOption(StandardSocketOptions.SO_KEEPALIVE, true); socketChannel.write(ByteBuffer.wrap("Hello server".getBytes())).get(); while (socketChannel.read(buffer).get() != -1) &#123; buffer.flip(); CharBuffer decode = Charset.defaultCharset().decode(buffer); log.info(decode.toString());// 如果调用的是clear()方法，position将被设回0，limit被设置成capacity的值。换句话说，Buffer被清空了。// Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。// 如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。// 如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。// compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。// limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。 if (buffer.hasRemaining()) &#123; buffer.compact(); &#125; else &#123; buffer.clear(); &#125; int r = new Random().nextInt(10000); if (r == 50) &#123; break; &#125; else &#123; socketChannel.write(ByteBuffer.wrap("Random number ".concat(String.valueOf(r)).getBytes())).get(); log.info("Client send successfully!"); &#125; &#125; &#125; else &#123; log.warn("The connection cannot be established!"); &#125; &#125; else &#123; log.warn("The asynchronous socket-channel cannot be opened!"); &#125; &#125; catch (IOException | InterruptedException | ExecutionException e) &#123; log.error(e); &#125; &#125;&#125; Reader/Writer方式实现支持多客户端并发服务想要使服务端支持多并发，必须要使用到AsynchronousChannelGroup，有关细节在下一节详述，AsynchronousChannelGroup用于管理异步通道资源，封装一个处理I/O完成的机制。该组对象关联一个线程池，可以将处理任务提交到线程池，这个组对象相当于是一个Dispatcher。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import lombok.extern.log4j.Log4j2;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.channels.AsynchronousChannelGroup;import java.nio.channels.AsynchronousServerSocketChannel;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;/** * &lt;p&gt; * Created with IntelliJ IDEA. 16/2/25 10:53 * &lt;/p&gt; * &lt;p&gt; * ClassName:ServerOnReaderAndWriterForMultiClients * &lt;/p&gt; * &lt;p&gt; * Description:基于读写类实现的服务端，并同时支持多客户端并发 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 * WebSite: http://codepub.cn * Licence: Apache v2 License */@Log4j2public class ServerOnReaderAndWriterForMultiClients &#123; static final int DEFAULT_PORT = 7777; static final String IP = "127.0.0.1"; static AsynchronousChannelGroup threadGroup = null; static ExecutorService executorService = Executors.newCachedThreadPool(Executors.defaultThreadFactory()); public static void main(String[] args) &#123; try &#123; threadGroup = AsynchronousChannelGroup.withCachedThreadPool(executorService, 5); //或者使用指定数量的线程池 //threadGroup = AsynchronousChannelGroup.withFixedThreadPool(6, Executors.defaultThreadFactory()); &#125; catch (IOException e) &#123; log.error(e); &#125; try (AsynchronousServerSocketChannel serverSocketChannel = AsynchronousServerSocketChannel.open(threadGroup)) &#123; if (serverSocketChannel.isOpen()) &#123; //服务端的通道支持两种选项SO_RCVBUF和SO_REUSEADDR，一般无需显式设置，使用其默认即可，此处仅为展示设置方法 //在面向流的通道中，此选项表示在前一个连接处于TIME_WAIT状态时，下一个连接是否可以重用通道地址 serverSocketChannel.setOption(StandardSocketOptions.SO_REUSEADDR, true); //设置通道接收的字节大小 serverSocketChannel.setOption(StandardSocketOptions.SO_RCVBUF, 8 * 1024); serverSocketChannel.bind(new InetSocketAddress(IP, DEFAULT_PORT)); log.info("Waiting for connections..."); serverSocketChannel.accept(serverSocketChannel, new Acceptor()); threadGroup.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);// System.in.read(); &#125; else &#123; log.warn("The connection cannot be opened!"); &#125; &#125; catch (IOException | InterruptedException e) &#123; log.error(e); &#125; &#125;&#125; 线程池和Group四种异步通道的open方法可以指定group参数，或者不指定。每个异步通道都必须关联一个组，要么是系统默认组，要么是用户创建的组。如果不使用group参数，java使用一个默认的系统范围的组对象。系统默认的组对象的线程池参数可以使用两个属性进行配置： java.nio.channels.DefaultThreadPool.threadFactory 默认组对象不会将其关联的线程池中的线程进行额外的配置，因此，这些线程都是daemon线程。 java.nio.channels.DefaultThreadPool.initialSize: 处理I/O事件的最大线程数量。 组与ExecutorService类似，这意味着关闭过程通常是两步关闭方法。在多层次Client结构（例如FTP的控制通道需要衍生新的数据传输通道）中，如果要使用group，很讨厌的一点就是group参数传递。没有环境编程之类的工具进行辅助的话，使用者必须考虑如何有效传递group参数。 不使用group，最大的好处是不用传递group参数。缺点是：必须注意处理非daemon线程的完成和退出，不小心的话，将会导致异步通道的工作丢失；同时还需要处理线程工厂和最大线程数的配置。 PendingException 和 AsynchronousChannel如果一个读写操作没有完成，程序又发送一个读写操作命令，则导致ReadPendingException或者WritePendingException。如果你的程序非要这样的话，只有一个解决办法，将读写操作的命令使用队列排队进行。通常应该不会出现这种需求，如果有的话，很有可能是设计上的缺陷。 读写超时。AsynchronousChannel的读写操作可以指定超时参数，但是超时发生之后，传递给读写操作的ByteBuffer参数不应该向正常读写完成一样进行处理。通常设计如果超时发生，一般应该丢弃当前期望数据结果。 ByteBufferAIO鼓励使用DirectByteBuffer。就算应用程序代码中不使用DirectByteBuffer，AIO内核实现也会使用DirectByteBuffer来复制外部传入的HeadByteBuffer内容。 ByteBuffer主要有两个继承的类分别是：HeapByteBuffer和MappedByteBuffer。他们的不同之处在于HeapByteBuffer会在JVM的堆上分配内存资源，而MappedByteBuffer的资源则会由JVM之外的操作系统内核来分配。DirectByteBuffer继承了MappedByteBuffer，采用了直接内存映射的方式，将文件直接映射到虚拟内存，同时减少在内核缓冲区和用户缓冲区之间的调用，尤其在处理大文件方面有很大的性能优势。但是在使用内存映射的时候会造成文件句柄一直被占用而无法删除的情况，网上也有很多介绍。 Netty中使用ChannelBuffer来处理读写，之所以废弃ByteBuffer，官方说法是ChannelBuffer简单易用并且有性能方面的优势。在ChannelBuffer中使用ByteBuffer或者byte[]来存储数据。同样的，ChannelBuffer也提供了几个标记来控制读写并以此取代ByteBuffer的position和limit，分别是：0 &lt;= readerIndex &lt;= writerIndex &lt;= capacity，同时也有类似于mark的markedReaderIndex和markedWriterIndex。当写入buffer时，writerIndex增加，从buffer中读取数据时readerIndex增加，而不能超过writerIndex。有了这两个变量后，就不用每次写入buffer后调用flip()方法，方便了很多。 参考文献[1] https://www.ibm.com/developerworks/cn/java/j-lo-nio2/[2] https://github.com/redkale/redkale[3] http://colobu.com/2014/11/13/java-aio-introduction/[4] http://zjumty.iteye.com/blog/1896350[5] http://stevex.blog.51cto.com/4300375/1581701[6] 《Pro Java 7 NIO2》]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程之Callable接口及线程池]]></title>
    <url>%2F2016%2F02%2F01%2FJava-multi-thread-Callable-interface-and-thread-pool%2F</url>
    <content type="text"><![CDATA[Java实现多线程的三种方式 继承Thread类 1234567891011public class Test extends Thread &#123; public static void main(String[] args) &#123; Thread t = new Test(); t.start(); &#125; @Override public void run() &#123; System.out.println("Override run() ..."); &#125;&#125; 实现Runnable接口，并覆写run方法 1234567891011public class Test implements Runnable &#123; public static void main(String[] args) &#123; Thread t = new Thread(new Test()); t.start(); &#125; @Override public void run() &#123; System.out.println("Override run() ..."); &#125;&#125; 实现Callable接口，并覆写call方法 1234567891011121314151617181920public class Test implements Callable &#123; public static void main(String[] args) &#123; FutureTask futureTask = new FutureTask(new Test()); Thread thread = new Thread(futureTask); thread.start(); try &#123; Object o = futureTask.get(); System.out.println(o); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public Object call() throws Exception &#123; return "Override call() ..."; &#125;&#125; 前两种方式应该很熟悉了，不再赘述，本文主要介绍第三种方式。 Executor框架Executor框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。无限制的创建线程会引起应用程序内存溢出。所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。Executor框架包括：线程池，Executor，Executors，ExecutorService，CompletionService，Future，Callable等。 什么是ExecutorExecutor仅仅是一个接口，只有一个方法execute(Runnable command)，是在JDK1.5中引入的，主要是用来运行提交的可运行的任务。一般我们并不直接使用该接口，而是使用其不同的子接口，主要是ExecutorService，而通常情况，与ExecutorService一起使用的是Executors类，该类由著名的并发编程大师Doug Lea实现。Executor框架可以用来控制线程的启动、执行和关闭，可以简化并发编程的操作。 什么是Callable和FutureCallable接口使用泛型去定义它的返回类型。Executors类提供了一些有用的方法去在线程池中执行Callable内的任务。由于Callable任务是并行的，我们必须等待它返回的结果。java.util.concurrent.Future对象为我们解决了这个问题。在线程池提交Callable任务后返回了一个Future对象，使用它我们可以知道Callable任务的状态和得到Callable返回的执行结果。Future提供了get()方法让我们可以等待Callable结束并获取它的执行结果。 什么是FutureTaskFutureTask是Future的一个基础实现，我们可以将它同Executors一起使用处理异步任务。通常我们不需要使用FutureTask类，单当我们打算重写Future接口的一些方法并保持原来基础的实现时，它就变得非常有用。我们可以仅仅继承于它并重写我们需要的方法。 什么是ExecutorsExecutors提供了一系列工厂方法用于创建线程池，返回的线程池都实现了ExecutorService接口： newCachedThreadPool可缓存线程池，对于每个线程，如果有空闲线程可用，立即让它执行，如果没有，则创建一个新线程 newFixedThreadPool具有固定大小的线程池，如果任务数大于空闲的线程数，则把它们放进队列中等待 newSingleThreadPool大小为1的线程池，任务一个接着一个完成 newScheduledThreadPool调度型线程池，可控制线程最大并发数，支持定时及周期性任务执行，用来代替Timer 什么是ExecutorService在ExecutorService中提供了重载的submit()方法，该方法既可以接收Runnable实例又能接收Callable实例。对于实现Callable接口的类，需要覆写call()方法，并且只能通过ExecutorService的submit()方法来启动call()方法。那么既然存在Runnable接口，为什么还要添加Callable接口呢？这是因为Runnable不会返回结果，并且无法抛出经过检查的异常而Callable会返回结果，而且当获取返回结果时可能会抛出异常。Callable中的call()方法类似Runnable的run()方法，区别同样前者有返回值，而后者没有。 使用示例获取线程的返回值通过FutureTask包装一个Callable的实例，再通过Thread包装FutureTask的实例，然后调用Thread的start()方法，且看示例如下12345678910111213141516171819202122232425262728import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;import java.util.concurrent.atomic.AtomicInteger;public class ThreadTest implements Callable &#123; private AtomicInteger atomicInteger = new AtomicInteger(0); @Override public Object call() throws Exception &#123; for (int i = 0; i &lt;= 100; i++) &#123; //Do something what U want atomicInteger.set(atomicInteger.get() + 1); &#125; return atomicInteger.get(); &#125; @org.junit.Test public void test() throws ExecutionException, InterruptedException &#123; FutureTask future = new FutureTask(new ThreadTest()); Thread thread = new Thread(future); thread.start(); if (future.isDone()) &#123; future.cancel(true); &#125; System.out.println(future.get()); &#125;&#125; 通过ExecutorService执行线程12345678910111213141516171819202122import java.util.concurrent.*;import java.util.concurrent.atomic.AtomicInteger;public class ThreadTest implements Callable &#123; private AtomicInteger atomicInteger = new AtomicInteger(0); @Override public Object call() throws Exception &#123; for (int i = 0; i &lt;= 100; i++) &#123; //Do something what U want atomicInteger.set(atomicInteger.get() + 1); &#125; return atomicInteger.get(); &#125; @org.junit.Test public void test() throws ExecutionException, InterruptedException &#123; ExecutorService executorService = Executors.newSingleThreadExecutor(); Future submit = executorService.submit(new ThreadTest()); System.out.println(submit.get()); &#125;&#125; 批量提交任务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.ArrayList;import java.util.List;import java.util.concurrent.*;import java.util.concurrent.atomic.AtomicInteger;public class ThreadTest&lt;T&gt; implements Callable &#123; private AtomicInteger atomicInteger = new AtomicInteger(0); @Override public Object call() throws Exception &#123; synchronized (this) &#123; for (int i = 0; i &lt;= 100; i++) &#123; //Do something what U want atomicInteger.set(atomicInteger.get() + 1); &#125; TimeUnit.SECONDS.sleep(1); &#125; return atomicInteger.get(); &#125; @org.junit.Test public void test() throws ExecutionException, InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(5); List&lt;Callable&lt;T&gt;&gt; tasks = new ArrayList&lt;&gt;(); ThreadTest threadTest = new ThreadTest(); for (int i = 0; i &lt; 5; i++) &#123; tasks.add(threadTest); &#125; //该方法返回某个完成的任务 Object o = executorService.invokeAny(tasks); System.out.println(o); System.out.println("One completed!"); long start = System.currentTimeMillis(); threadTest = new ThreadTest(); tasks.clear(); for (int i = 0; i &lt; 5; i++) &#123; tasks.add(threadTest); &#125; List&lt;Future&lt;T&gt;&gt; futures = executorService.invokeAll(tasks); long end = System.currentTimeMillis(); System.out.println(end - start + " ms之后，返回运行结果！"); for (int i = 0; i &lt; 5; i++) &#123; System.out.println(futures.get(i).get()); &#125; &#125;&#125; 运行结果 184One completed!5001 ms之后，返回运行结果！101505404404202 invokeAny方法提交所有任务到一个Callable对象的集合中，并且返回某个已经完成了的任务的结果，返回的任务是不确定的。invokeAll方法则返回所有任务的结果，但是其一个弊端是，如果第一个任务花费了很长时间，则不得不等待，待所有任务都完成之后，才返回。在某些情况下，可能只需要一个任务出了结果就可以中止所有任务，这样就得不偿失。将结果按照可获得的顺序保存起来可能更好，这时可以使用ExecutorCompletionService，其中的take()方法会移除下一个已经完成的结果（Future），如果没有可用结果则阻塞。 通过CompletionService提交多组任务并获取返回值1234567891011121314151617181920212223242526272829303132333435import java.util.concurrent.*;import java.util.concurrent.atomic.AtomicInteger;public class ThreadTest implements Callable &#123; private AtomicInteger atomicInteger = new AtomicInteger(0); @Override public Object call() throws Exception &#123; synchronized (this) &#123; for (int i = 0; i &lt;= 100; i++) &#123; //Do something what U want atomicInteger.set(atomicInteger.get() + 1); &#125; TimeUnit.SECONDS.sleep(1); &#125; return atomicInteger.get(); &#125; @org.junit.Test public void test() throws InterruptedException, ExecutionException &#123; ExecutorService executorService = Executors.newFixedThreadPool(5); CompletionService&lt;Integer&gt; completionService = new ExecutorCompletionService(executorService); ThreadTest threadTest = new ThreadTest(); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 5; ++i) &#123; completionService.submit(threadTest);//提交五组任务 &#125; long end = System.currentTimeMillis(); System.out.println(end - start + " ms之后，返回运行结果！"); for (int i = 0; i &lt; 5; ++i) &#123; Integer res = completionService.take().get();//通过take System.out.println(res); &#125; &#125;&#125; 输出结果： 1 ms之后，返回运行结果！101202303404505 由运行结果可知，ExecutorCompletionService并不会阻塞，在提交任务之后，继续向下运行，哪个任务完成即返回，并不受任务提交顺序的影响。 通过自维护列表管理多组任务并获取返回值123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;import java.util.Iterator;import java.util.List;import java.util.concurrent.*;import java.util.concurrent.atomic.AtomicInteger;public class ThreadTest implements Callable &#123; private AtomicInteger atomicInteger = new AtomicInteger(0); @Override public Object call() throws Exception &#123; for (int i = 0; i &lt;= 100; i++) &#123; //Do something what U want atomicInteger.set(atomicInteger.get() + 1); &#125; return atomicInteger.get(); &#125; @org.junit.Test public void test() throws ExecutionException, InterruptedException &#123; ExecutorService executorService = Executors.newSingleThreadExecutor(); List&lt;Future&gt; futures = new ArrayList&lt;&gt;(); ThreadTest threadTest = new ThreadTest(); for (int i = 0; i &lt; 5; ++i) &#123; Future submit = executorService.submit(threadTest); futures.add(submit); &#125; Iterator&lt;Future&gt; iterator = futures.iterator(); while (iterator.hasNext()) &#123; Future next = iterator.next(); System.out.println(next.get()); &#125; &#125;&#125; 采用自维护Future集合方法，submit的task不一定是按照加入自己维护的列表顺序完成的。从list中遍历的每个Future对象并不一定处于完成状态，这时调用get()方法就会被阻塞住，如果系统是设计成每个线程完成后就能根据其结果继续做后面的事，这样对于处于list后面的但是先完成的线程就会增加了额外的等待时间。 而CompletionService的实现是维护一个保存Future对象的BlockingQueue。只有当这个Future对象状态是结束的时候，才会加入到这个Queue中，take()方法其实就是Producer-Consumer中的Consumer。它会从Queue中取出Future对象，如果Queue是空的，就会阻塞在那里，直到有完成的Future对象加入到Queue中。 所以，先完成的必定先被取出。这样就减少了不必要的等待时间。 ScheduledExecutor任务调度ScheduledExecutor提供了基于开始时间与重复间隔的任务调度，可以实现简单的任务调度需求。每一个被调度的任务都会由线程池中一个线程去执行，因此任务是并发执行的，相互之间不会受到干扰。需要注意的是，只有当任务的执行时间到来时，ScheduedExecutor才会真正启动一个线程，其余时间ScheduledExecutor都是在轮询任务的状态。 12345678910111213141516171819202122232425262728293031323334import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ScheduledExecutorTest implements Runnable &#123; private String jobName = ""; public ScheduledExecutorTest(String jobName) &#123; super(); this.jobName = jobName; &#125; @Override public void run() &#123; System.out.println("execute " + jobName); &#125; public static void main(String[] args) &#123; ScheduledExecutorService service = Executors.newScheduledThreadPool(10); long initialDelay1 = 1; long period1 = 1; // 从现在开始1秒钟之后，每隔1秒钟执行一次job1 service.scheduleAtFixedRate( new ScheduledExecutorTest("job" + period1), initialDelay1, period1, TimeUnit.SECONDS); long initialDelay2 = 2; long period2 = 2; // 从现在开始2秒钟之后，每隔2秒钟执行一次job2 service.scheduleWithFixedDelay( new ScheduledExecutorTest("job" + period2), initialDelay2, period2, TimeUnit.SECONDS); &#125;&#125; 运行结果 execute job1execute job1execute job2execute job1execute job1execute job2execute job1execute job1execute job2 取消向线程池提交的某个任务在ExecutorService中提供了submit()方法，用于向线程池提交任务，该方法返回一个包含结果集的Future实例。而Future提供了cancel(boolean mayInterruptIfRunning)方法用于取消提交的运行任务，如果向该函数传递true，那么不管该任务是否运行结束，立即停止，如果向该函数传递false，那么等待该任务运行完成再结束之。同样Future还提供了isDone()用于测试该任务是否结束，isCancelled()用于测试该任务是否在运行结束前已取消。 关闭线程池在ExecutorService中提供了shutdown()和List&lt;Runnable&gt; shutdownNow()方法用来关闭线程池，前一个方法将启动一次顺序关闭，有任务在执行的话，则等待该任务运行结束，同时不再接受新任务运行。后一个方法将取消所有未开始的任务并且试图中断正在执行的任务，返回从未开始执行的任务列表，不保证能够停止正在执行的任务，但是会尽力尝试。例如，通过Thread.interrupt()来取消正在执行的任务，但是对于任何无法响应中断的任务，都可能永远无法终止。]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO开发注意点]]></title>
    <url>%2F2016%2F02%2F01%2FJava-NIO-development-Precautions%2F</url>
    <content type="text"><![CDATA[Java NIO的介绍首先要搞清楚两个概念，一个是NIO，另一个是NIO 2。NIO = New I/O是在JDK1.4中引入，也就是同步非阻塞I/O，简称NIO；而NIO 2是NIO的升级版，在JDK1.7中引入，也就是异步非阻塞I/O，简称AIO；而最早最传统的I/O属于同步阻塞I/O，简称BIO。 Java-NIO的优势 事件驱动模型 避免多线程 单线程处理多任务 非阻塞I/O，I/O读写不再阻塞，而是返回0 基于block的传输，通常比基于流的传输更高效 更高级的I/O函数，zero-copy I/O多路复用大大提高了Java网络应用的可伸缩性和实用性 NIO工作原理 由一个专门的线程来处理所有的I/O事件，并负责分发 事件驱动机制：事件到的时候触发，而不是同步的去监视事件 线程通讯：线程之间通过wait，notify等方式通讯。保证每次上下文切换都是有意义的，减少无谓的线程切换 NIO开发相关模糊知识点 首先Selector.open()并不是单例模式，当你每次调用该静态方法时候，都返回一个全新的Selector实例 configureBlocking()方法用来设置通道的阻塞模式，该方法会调用implConfigureBlocking方法，implConfigureBlocking方法会更改阻塞模式为新传入的值，如：默认为true，传入false，那么该通道将调整为非阻塞，可以通过调用isBlocking()方法来判断某个socket通道当前处于哪种模式 NIO的最大优势就是其是非阻塞模型，所以一般来说都需要设置SocketChannel.configureBlocking(false); 传统的Java IO中通道是阻塞的，那么NIO提供了非阻塞的通道到底有什么作用呢？非阻塞I/O是许多复杂的、高性能的程序构建的基础 remove()方法的作用 ① Set selectedKeys = selector.selectedKeys(); Iterator iter = selectedKeys.iterator(); ② SelectionKey key = iter.next(); ③ iter.remove(); 注意每次迭代之后要移除当前迭代的对象，原因是Selector不会自己从已选择键集中移除SelectionKey实例。必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中 服务端和客户端是否维护着同一份Selector，答案是否定的，服务端和客户端各自维护着一个Selector对象，并且注意在多线程并发的时候，不要让多个客户端共享Selector ByteBuffer byteBuffer = ByteBuffer.allocate(1);在从通道往buffer中读入之后，使用byteBuffer.get()获取的时候，不可重复调用，因为get()方法会移动position，使得多次调用get()方法获取的内容是不同的 在ByteBuffer中，put(int index, byte b)方法不会移动position，但是put(byte b)会移动position channel.read()函数会返回-1，那么什么时候会读到-1呢？针对服务器端而言，当客户端调用了channel.close()关闭连接时，这时候服务器端返回的读取数是-1，表示已经到了末尾。那么此时需要把对应的SelectionKey给cancel掉，表示selector不再监听这个channel上的读事件，并且关闭channel ByteBuffer.allocate(int capacity)和ByteBuffer.allocateDirect(int capacity)的区别：使用allocate来创建缓冲区，并不是一下子就分配给缓冲区capacity大小的空间，而是根据缓冲区中存储数据的情况来动态分配缓冲区的大小（实际上，在底层Java采用了数据结构中的堆来管理缓冲区的大小），因此，这个capacity可以是一个很大的值，如1024*1024（1M）。使用allocateDirect方法可以一次性分配capacity大小的连续字节空间。通过allocateDirect方法来创建具有连续空间的ByteBuffer对象虽然可以在一定程度上提高效率，但这种方式并不是平台独立的。也就是说，在一些操作系统平台上使用allocateDirect方法来创建ByteBuffer对象会使效率大幅度提高，而在另一些操作系统平台上，性能会表现得非常差。而且allocateDirect方法需要较长的时间来分配内存空间，在释放空间时也较慢。因此，在使用allocateDirect方法时应谨慎 与缓冲区不同，通道不能被重复使用，一个打开的通道即代表与一个特定I/O服务的特定连接并封装该连接的状态。当通道关闭时，那个连接会丢失，然后通道将不再连接任何东西 通道的read()和write()方法的数据流向是怎么样的？read()表示该通道读就绪，可以从通道中读取内容到缓冲区，而wirte()表示该通道写就绪，可以将缓冲区中的内容写入通道 虽然说一个通道可以被注册到多个选择器上，但对每个选择器而言只能被注册一次 选择键封装了特定的通道与特定的选择器的注册关系。选择键对象被SelectableChannel.register(Selector sel, int ops)返回并提供一个表示这种注册关系的标记。选择键包含了两个比特集（以整数的形式进行编码），指示了该注册关系所关心的通道操作，以及通道已经准备好的操作 注意select()是一个阻塞操作，那么如何停止select()操作所在的线程呢？主要有三种方法 ①使用volatile boolean变量来标识线程是否停止 ②停止线程时，需要调用停止线程的interrupt()方法，因为线程有可能在wait()或sleep()，提高停止线程的及时性 ③处于Blocking IO的处理，尽量使用InterruptibleChannel来代替Blocking IO，对于NIO来说，如果线程处于select()阻塞状态，这时候无法及时的检测到条件变量的变化，那么需要人工调用wakeup()方法，唤醒线程，使得其可以检测到条件变量 当通道关闭时，所有相关的键会自动取消；当选择器关闭时，所有被注册到该选择器的通道都将被注销，并且相关的键将立即被无效化 注意select()操作返回值不是已经准备好的通道的总数，而是从上一个select()调用之后进入就绪状态的通道的数量。之前的调用中就绪的，并且在本次调用中仍然就绪的通道不会被计入，而那些在前一次调用中已经就绪但已经不再处于就绪状态的通道也不会计入。这些通道可能仍然在已选择的键的集合中，但不会被计入返回值中，返回值可能是0 推荐使用内部的已取消的键的集合来延迟注销，是一种防止线程在取消键时阻塞，并防止与正在进行的选择操作冲突的优化 Selector选择器对象是线程安全的，但它们包含的键集合不是。通过keys()和selectKeys()返回的键的集合是Selector对象内部的私有的Set对象集合的直接引用。这些集合可能在任意时间被改变。已注册的键的集合是只读的 如果在多个线程并发地访问一个选择器的键的集合的时候存在任何问题，可以采用同步的方式进行访问，在执行选择操作时，选择器在Selector对象上进行同步，然后是已注册的键的集合，最后是已选择的键的集合 在并发量大的时候，使用同一个线程处理连接请求以及消息服务，可能会出现拒绝连接的情况，这是因为当该线程在处理消息服务的时候，可能会无法及时处理连接请求，从而导致超时；一个更好的策略是对所有的可选择通道使用一个选择器，并将对就绪通道的服务委托给其它线程。只需一个线程监控通道的就绪状态并使用一个协调好的的工作线程池来处理接收及发送数据 Selector.wakeup()的主要作用：①解除阻塞在Selector.select()/select(long)上的线程，立即返回 ②两次成功的select之间多次调用wakeup等价于一次调用 ③如果当前没有阻塞在select上，则本次wakeup调用将作用于下一次select操作]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2015年终总结]]></title>
    <url>%2F2015%2F12%2F31%2F2015-year-end-summary%2F</url>
    <content type="text"><![CDATA[公元2015年就这么过去了，我们都是不知不觉也极不情愿地又向坟墓迈进了一步。也许多年以后，当2015年已成为历史长河中的一粒沙，偶有后人回望历史，我想他们也会觉得这一年注定是多事之秋吧。 这一年，掀起了规模庞大的O2O公司创业潮，同时到了年终又有大量O2O公司掀起了倒闭潮，这也真的验证了雷军的那句话，站在风口上，猪都能飞起来。但是飞起来又怎样？猪就是猪，它不会在空中长出翅膀，所以它终归是要落地的，只不过飞的越高跌的越疼而已。伴随着年初VC们满世界的投资，他们早晚也有勒紧裤腰带，安心过日子的那一天，可不，这一天其实并不远，就在2015年，58赶集合并了、滴滴快滴合并了、美团点评合并了、携程去哪儿也合并了。大量的烧钱亏损终归不是长久之计，在资本的左右下，该合并的合并，该收的收，做起来丝毫没有半点含糊。俗话说胳膊拧不过大腿，纵然你是老板也一样拧不过投资人。BAT常年霸占互联网前三甲，如今各行各业都能看到BAT的身影，这对中国的创业环境来说未必是好事，无论你做什么，如果你愿意BAT就可以收购或投资你，如果你不愿意BAT就出钱自己搞一个玩死你。嗟乎普天之下莫非王土，率土之滨莫非王民。互联网这个世界难道始终都是BAT的帝国？ 好吧，扯远了，既然是我的年终总结，当然要说说我的事才对。这一年我也算是一枚屌丝驾驶员了，为啥哩，因为我也拿到驾照了，但是我并没有车哦。算下来，前后不到一个月驾照到手，四门科目全部一次性通过，而且还有满分哦，小小的自信心爆棚一下。考驾照这事说起来并不难，难的是根本没有上车机会，因为十多个人一辆车，一人跑个两圈，一天最多也就轮流俩次。当然你说我们车咋没那么多人嘞，因为你是土豪呀，我报的驾校是在一个小地方，价格便宜，只需三千块，所以每次上车人都特多。当然不如大上海一万多，每次就三四个人练车啦。 这一年，我也有了个人博客，采用Hexo搭建，托管在Github Pages上，只需买个域名足矣。如果你也想自己搞个博客，请移步这里。之所以建立个人博客，原因很简单，主要是为了个人的知识积累，以及个人品牌的塑造，之前写博客常驻CSDN，但是在上面发表的文章杂而多，广而浅，这不是我想要的。所以在四月份就打造一个独立域名的博客，并且使用Markdown进行写作，而Markdown的最大的优势就是像写代码那样写博客，这个才是我所喜欢的方式。 这一年，感情生活依然不多，在大多数的日子里，还是单身汪一枚，当然这也符合程序猿的风格，虽然不是所有的码猿都是屌丝，但是屌丝确实挺多，这是我的切身体会。本来在去年的时候，我还算一个有点小幸运的码猿，因为我有个女朋友，而且她也是我喜欢的类型，我也希望我们能一直走下去，直到步入婚姻的殿堂，可是天不遂人愿，在交往没多久之后我就被女票果断的甩掉了，重新回归单身汪。虽然后来我们和好了，但是没多久我又被第二次甩了，是的，就是这么狗血，本以为电视剧中才有的SB剧情，竟然神奇的就发生在我的身上。可是爱情是诡异的，没人能够左右自己的心，很多事情不是像说的那样简单，如今她又有了想要和好的迹象，对于这一次，我不确定结局若何，但是我一直坚信感情的世界里，两个人一定是平等的，即便现在男多女少，作为一个爷们，我也不会丢弃我的尊严去赢得一个女人的芳心。希望各位单身的屌丝们，即使你现在还没有女票，但是也一定不要做一些放下人格、放下尊严、恬不知耻去求女生同意做你女票之类的事，毕竟在你们父母的眼里，你就是个土皇帝，你有这些心思去孝敬你们的女票，倒不如拿来好好孝敬父母。古人有云，百善孝为先，这是永远不会错的话。 这一年，趁着五一黄金周，我也算走出了大上海，因为我去了趟杭州，潇洒的玩了四五天。在这几天里，天天都是在外奔波，丝毫不留喘息的机会，因为机会难得啊，要逛的地方实在太多了。马不停蹄也好，快马加鞭也罢，总之这几天彻底把杭州逛了个遍，虽然并没有深入到每一个角落，但是该逛的都逛了，该玩的都玩了。在此给想要去杭州玩的朋友强烈推荐宋城，以及宋城千古情的表演，另外还有河坊街，吃的东西是真特么多啊，而且尽是各种特色小吃，记得最重要一点哦，去之前别吃饭别吃饭别吃饭，重三完毕。最不推荐的就是西溪湿地国家公园，太大太大，上午从正门进，晚上从偏门出，这一天就完了。 这一年，暑假后我就研三了，所以自然而然的我开始实习了，记得是六月靠底吧，进入大众点评，做的是POI相关的业务开发。以前的我还有点小小的代码洁癖，可是进了公司才知道，尤其是做业务，那遗留的历史代码真的是一坨坨烂的可以。因为做业务的话，完全没人去管什么软件工程、设计模式之类的，至于重构那就算了，想想还是可以的，真的重构的话，并没有那么多精力，而且就在你重构的过程中，你会发现为什么我重构出来的代码也是烂的可以，如果你洁癖严重的话，那么在重构的过程中你就会想着再次重构。所以业务开发的重点是解决业务需求，对于公司来说，最终的评价标准其实就一个，解决问题否？没解决，那你水平不够啊。解决了，代码很烂吗？没关系，能用即可，所以只要解决问题，不管你的代码是烂的一坨翔还是貌美如花，他们根本不关心。虽然我最终离开了点评，但是还是在点评有所收获，那就是千万不要知无不言，言无不尽，你以为你所知道的问题，领导不知道，你以为你所想到的解决方法，领导没想到，那你就错了。其实你的领导都知道，否则他怎么会是你的领导呢？所以不要妄图期望你说了一大通对公司有好处的话之后，领导目光如炬如获至宝一般。其实并没有多少人在乎哪些对公司好，哪些对公司不好，以及公司是否能做大做强或是迅速衰落，这些都是真的。 这一年，在导师的带领下，我也完成了两篇小论文，达到了学校毕业的硬性要求，一篇中文一篇英文，中文投到了《中文信息学报》并录用，英文的投的是Frontier Computing 2015，会议地点锁定在曼谷。所以在会议的那几天，我也办了护照，去了趟曼谷，并做了报告。剩下的你都知道了，做完报告，顺便把曼谷玩了一遍，其实曼谷并没有多好玩，也并不像有些人吹嘘的到处都是中国人。原因很简单，我是自己玩的，并没有跟团，所以当然不是到处都是中国人了，如果你是跟团的话，那么这些旅游团的地点都是固定的，有你一个团，必然还有好多其他几个团，所以让你觉得到处都是中国人。玩过的都觉得不好玩，听说清迈不错，芭提雅也好玩，可是并没有去，希望下次有机会能再去这些地方欣赏一番。 这一年，我也面临着校招季，虽说这一年的校招有点冷，但是我觉得还算不上寒冬。在我的眼里，计算机专业的同学，尤其是做软件的，那真的是满大街都是钱啊。难道你看不到吗？从前端到中间件到服务端、从安卓到IOS、从运维到DBA，从大数据到机器学习等。真的是到处都是钱，主要的就看你有没有本事而已。在十月初的时候，从点评离职了，正式开始我的校招生涯，虽然好多公司这时候校招结束了，但是毕竟还留下不少。从此开启了一段很长的面试之旅，前后面了有十几家公司，多数都是互联网公司，拿了七八个OFFER，薪资全集中在13万到21万区间。最后的结果就是，如果你看了我的Resume的话那么你就知道了，我去了腾讯文学，在腾讯文学收购盛大文学之后就变得一家独大了，并且改名为阅文集团。和百度文学、阿里文学完全不在一个量级了，基本上网络文学这块市场都是属于阅文集团的天下了。遗憾的一点就是我并没有拿到最核心的BAT的OFFER吧（百度两面被拒，阿里笔试未过，腾讯没投，因为腾讯上海主要是游戏部门招C++，而我主攻Java），虽然阅文也是腾讯系，但毕竟感觉是全资子公司，还不是真正的BAT核心。最后说一下为什么没留在点评，原因很多，不能一一赘述，我自身也有原因。虽然我也走了点评的校招流程，但是面了一面之后就没消息了，因为美团收了点评之后，掌握了完全的话语权，所以到后期点评的校招和社招都冻结了，这是点评HR告诉我的，这也和滴滴收购快滴之后情形一样，这一年快滴并没有校招名额。在点评还是属于业务开发，做业务真的挺苦逼，在阅文现在是平台架构中心，和业务没半毛钱关系，可以慢慢折腾属于我自己的一块代码自留地，完全不受业务上线等日程的催促，谁说留下了就一定幸福呢？最后面了这么多公司，收获还是很多的，发现了自己知识掌握的不扎实不牢靠不深入不细致，以及算法的薄弱，所以就有了我的Github上的Algorithms项目了，虽然目前还不是很全，但是有时间的话，我还是会去更新的，如果大家发现存有BUG的话，请积极提出哦。另外的一个收获就是多数公司的HR并不靠谱，一般我有时候会在面试之前打听打听年薪，少的话就不去面了，居然有的HR大摇大摆的说，同学，我们一定不会让你因为年薪而拒绝我们，不好意思，面完了，拿到OFFER了，我最后就是因为年薪而拒绝了他们。 最近也正在看《删除-大数据取舍之道》，诸多的记忆如果没有存储下来，终究会变成一段模糊的存在或者不存在，因为我们自己也无法判断是否有过该段记忆所承载的历史事件，希望我这篇总结没有记成流水账，此非我所欲也。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CCF第六次CSP认证考试题解]]></title>
    <url>%2F2015%2F12%2F21%2FCCF-sixth-CSP-certification-exams-Solutions%2F</url>
    <content type="text"><![CDATA[CCF计算机职业资格认证CCF（China Computer Federation）是计算机领域内一个权威的学术组织，具有高端定位、崇高的价值追求、先进的治理架构和制度规范，拥有众多资深的学者和企业家为骨干会员。CCF开展的该认证工作，具有客观公正及很强的专业性，将解决企业及高校界普遍关心的软件开发人才评价问题，便于有关单位了解求职或求学者的实际开发能力，可有助甄别及吸纳具有真才实学的技术人才，有效地减轻企业与高校在人才选择过程中组织大量上机考核的成本投入。为了更加贴近用人单位的需要，使得该认证有更强的针对性，CCF会同处于行业领先地位的高校与知名企业的专家，共同制定认证标准，审查考题内容。更多详情点我。 CSP认证第六次考试本次考试时间四小时，共五道题。分别是： 数位之和（难度-易） 消除类游戏（难度-中） 画图（难度-中） 送货（难度-难） 矩阵（难度-难） 数位之和题目说明：输入任意一个整数，输出各位数位之和。输入的整数不会超过整型的最大表示范围。解答：123456789101112131415161718192021import java.util.Scanner;public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); while (in.hasNext()) &#123; int num = in.nextInt(); int sum = getSum(num); System.out.println(sum); &#125; &#125; public static int getSum(int num) &#123; int sum = 0; while (num &gt; 0) &#123; int temp = num % 10; sum += temp; num = num / 10; &#125; return sum; &#125;&#125; 消除类游戏题目说明：解答：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import java.util.Scanner;public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int[][] arrs = null;//原始数组 int[][] dest = null;//消除之后的数组 while (in.hasNext()) &#123; int row = in.nextInt(); int col = in.nextInt(); in.nextLine(); arrs = new int[row][col]; dest = new int[row][col]; for (int i = 0; i &lt; row; i++) &#123; for (int j = 0; j &lt; col; j++) &#123; arrs[i][j] = in.nextInt(); dest[i][j] = arrs[i][j]; &#125; in.nextLine(); &#125; //处理行 for (int i = 0; i &lt; row; i++) &#123; int start = 0; int end = 0; for (int j = 0; j &lt; col - 1; j++) &#123; if (arrs[i][j] == arrs[i][j + 1]) &#123; end = j + 1; &#125; else &#123; end = j + 1; if (start == 0) &#123; if (end - start &gt;= 3) &#123; for (int k = start; k &lt; end; k++) &#123; dest[i][k] = 0; &#125; &#125; &#125; else &#123; if (end - start + 1 &gt;= 3) &#123; for (int k = start; k &lt; end; k++) &#123; dest[i][k] = 0; &#125; &#125; &#125; start = end; &#125; &#125; if (start == 0) &#123; if (end - start &gt;= 3) &#123; for (int k = start; k &lt;= end; k++) &#123; dest[i][k] = 0; &#125; &#125; &#125; else &#123; if (end - start + 1 &gt;= 3) &#123; for (int k = start; k &lt;= end; k++) &#123; dest[i][k] = 0; &#125; &#125; &#125; &#125; //处理列 for (int i = 0; i &lt; col; i++) &#123; int start = 0; int end = 0; for (int j = 0; j &lt; row - 1; j++) &#123; if (arrs[j][i] == arrs[j + 1][i]) &#123; end = j + 1; &#125; else &#123; end = j + 1; if (start == 0) &#123; if (end - start &gt;= 3) &#123; for (int k = start; k &lt; end; k++) &#123; dest[k][i] = 0; &#125; &#125; &#125; else &#123; if (end - start + 1 &gt;= 3) &#123; for (int k = start; k &lt; end; k++) &#123; dest[k][i] = 0; &#125; &#125; &#125; start = end; &#125; &#125; if (start == 0) &#123; if (end - start &gt;= 3) &#123; for (int k = start; k &lt;= end; k++) &#123; dest[k][i] = 0; &#125; &#125; &#125; else &#123; if (end - start + 1 &gt;= 3) &#123; for (int k = start; k &lt;= end; k++) &#123; dest[k][i] = 0; &#125; &#125; &#125; &#125; for (int[] arr : dest) &#123; for (int a : arr) &#123; System.out.print(a + " "); &#125; System.out.println(); &#125; &#125; &#125;&#125; 画图题目说明：解答：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139import java.util.Scanner;public class Main &#123; static int COL = 0; static int ROW = 0; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); while (in.hasNext()) &#123; COL = in.nextInt(); ROW = in.nextInt(); int num = in.nextInt(); String[][] arrs = new String[ROW][COL]; for (int i = 0; i &lt; arrs.length; i++) &#123; for (int j = 0; j &lt; arrs[i].length; j++) &#123; arrs[i][j] = "·"; &#125; &#125; in.nextLine(); for (int i = 0; i &lt; num; i++) &#123; int first = in.nextInt(); if (first == 0) &#123;//表示画线段操作 int x1 = in.nextInt(); int y1 = in.nextInt(); int x2 = in.nextInt(); int y2 = in.nextInt(); if (x1 == x2) &#123; // | int big = y1 &gt; y2 ? y1 : y2; int small = y1 &lt; y2 ? y1 : y2; for (int start = small; start &lt;= big; start++) &#123; if (arrs[ROW - start - 1][x1] == "-") &#123; arrs[ROW - start - 1][x1] = "+"; &#125; else &#123; arrs[ROW - start - 1][x1] = "|"; &#125; &#125; &#125; if (y1 == y2) &#123; // - int big = x1 &gt; x2 ? x1 : x2; int small = x1 &lt; x2 ? x1 : x2; for (int start = small; start &lt;= big; start++) &#123; if (arrs[ROW - y1 - 1][start] == "|") &#123; arrs[ROW - y1 - 1][start] = "+"; &#125; else &#123; arrs[ROW - y1 - 1][start] = "-"; &#125; &#125; &#125; &#125; else if (first == 1) &#123;//表示填充操作 int x = in.nextInt(); int y = in.nextInt(); String c = in.next(); arrs[ROW - y - 1][x] = c; move(arrs, ROW - y - 1, x, c); &#125; in.nextLine(); &#125; print(arrs); &#125; &#125; public static void move(String[][] arrs, int row, int col, String c) &#123; if (col - 1 &gt;= 0) &#123; moveLeft(arrs, row, col, c); &#125; if (row + 1 &lt; ROW) &#123; moveDown(arrs, row, col, c); &#125; if (col + 1 &lt; COL) &#123; moveRight(arrs, row, col, c); &#125; if (row - 1 &gt;= 0) &#123; moveUp(arrs, row, col, c); &#125; &#125; public static void moveLeft(String[][] arrs, int row, int col, String c) &#123; col = col - 1; if (col &gt;= 0 &amp;&amp; arrs[row][col] != "-" &amp;&amp; arrs[row][col] != "|" &amp;&amp; arrs[row][col] != "+" &amp;&amp; arrs[row][col] != c) &#123; arrs[row][col] = c; moveLeft(arrs, row, col, c); moveUp(arrs, row, col, c); moveDown(arrs, row, col, c); &#125; &#125; public static void moveDown(String[][] arrs, int row, int col, String c) &#123; row = row + 1; if (row &lt; ROW &amp;&amp; arrs[row][col] != "-" &amp;&amp; arrs[row][col] != "|" &amp;&amp; arrs[row][col] != "+" &amp;&amp; arrs[row][col] != c) &#123; arrs[row][col] = c; moveLeft(arrs, row, col, c); moveRight(arrs, row, col, c); moveDown(arrs, row, col, c); &#125; &#125; public static void moveRight(String[][] arrs, int row, int col, String c) &#123; col = col + 1; if (col &lt; COL &amp;&amp; arrs[row][col] != "-" &amp;&amp; arrs[row][col] != "|" &amp;&amp; arrs[row][col] != "+" &amp;&amp; arrs[row][col] != c) &#123; arrs[row][col] = c; moveRight(arrs, row, col, c); moveUp(arrs, row, col, c); moveDown(arrs, row, col, c); &#125; &#125; public static void moveUp(String[][] arrs, int row, int col, String c) &#123; row = row - 1; if (row &gt;= 0 &amp;&amp; arrs[row][col] != "-" &amp;&amp; arrs[row][col] != "|" &amp;&amp; arrs[row][col] != "+" &amp;&amp; arrs[row][col] != c) &#123; arrs[row][col] = c; moveUp(arrs, row, col, c); moveLeft(arrs, row, col, c); moveRight(arrs, row, col, c); &#125; &#125; //打印数组工具函数 public static void print(String[][] arrs) &#123; for (String[] arr : arrs) &#123; for (String a : arr) &#123; System.out.print(a); &#125; System.out.println(); &#125; &#125;&#125; 送货（回忆版）题目说明：街道是边，圆圈是交叉路口，从路口1出发，一次性遍历所有街道；如果不存在可以一次性遍历所有街道的路径，则输出 -1；输入：路口数m 街道数nN行街道输出：如果有多条路径则输出序号依次最小的一条路径，例如demo1存在1,4,3,1,2,3和1,2,3,1,4,3，输出后者样例输入：4 51 21 41 32 33 4样例输出：1 2 3 1 4 3样例输入：4 61 21 41 32 33 42 4样例输出：-1解答：1待补充 矩阵（回忆版）题目说明：小明想重新定义世界，初始状态为n维度的列向量b0（注意是列向量，只不过用行方式输入），状态转移矩阵为n维度的方阵A, 向量b1 = A*b0为第一个时刻的状态，bn = A^m*b0为第m时刻的状态，小明据此预测未来；重新定义了向量运算中的求积为逻辑运算与，重定义求和为逻辑运算异或；其中b0 = A^0*b0输入：向量维度n转移矩阵的第一行（数字间没有空格）……转移矩阵的第n行初始向量要预测的状态个数mM行预测数字输出：M个预测状态样例输入：3110011111101100217……(省略了六个数字)样例输出：101…110…(省略了六个输出)解答：举例，求1时刻的状态，其实就是使用新的运算法则计算矩阵相乘的结果123┌ 1 1 0 ┐ ┌ 1 ┐ ┌ 1 ┐| 0 1 1 | | 1 | = | 1 |└ 1 1 1 ┘ └ 0 ┘ └ 0 ┘ 1待补充 啥？还有两题？对的，还有俩，不过第四道不会，第五道没写完就自动交卷了。不提也罢，码字速度还要跟上，虽然说机房键盘差劲，也没有我喜爱的IntelliJ，但是要从自身找原因。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>CCF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA中Maven插件无法更新索引之解决办法]]></title>
    <url>%2F2015%2F12%2F09%2FIntelliJ-IDEA-in-Maven-plugin-could-not-update-the-index-of-the-solution%2F</url>
    <content type="text"><![CDATA[Maven的仓库、索引中央仓库：目前来说，http://repo1.maven.org/maven2/ 是真正的Maven中央仓库的地址，该地址内置在Maven的源码中，其它地址包括著名的ibiblio.org，都是镜像。 索引：中央仓库带有索引文件以方便用户对其进行搜索，完整的索引文件至2015年12月8日大小约为1.11G，索引每周更新一次。 本地仓库：是建立在本地机器上的Maven仓库，本地仓库是中央仓库（或者说远程仓库）的一个缓冲和子集，当你构建Maven项目的时候，首先会从本地仓库查找资源，如果没有，那么Maven会从远程仓库下载到你本地仓库。这样在你下次使用的时候就不需要从远程下载了。如果你所需要的Jar包版本在本地仓库没有，而且也不存在于远程仓库，Maven在构建的时候会报错，这种情况可能发生在有些Jar包的新版本没有在Maven仓库中及时更新。Maven缺省的本地仓库地址为${user.home}/.m2/repository。也就是说，一个用户会对应的拥有一个本地仓库。当然你可以通过修改${user.home}/.m2/settings.xml配置这个地址：12345&lt;settings&gt; ··· &lt;localRepository&gt; D:/java/repository&lt;/localRepository&gt; ...&lt;/settings&gt; 提交内容：只要你的项目是开源的，而且你能提供完备的POM等信息，你就可以提交项目文件至中央仓库，这可以通过Sonatype提供的开源Maven仓库托管服务实现。 IntelliJ IDEA利用索引实现自动补全众所周知，由于伟大的中国防火墙，所以在使用IDEA下载Maven仓库索引的时候，要么无法访问，要么就是速度极慢，这对开发人员带来了极大的不便，所以一般公司都用Nexus搭建一个公司内部的私服。同时利用私服更有利于对公司内部开发人员依赖的Jar包版本进行控制。 也许你会问，中央仓库带有索引，为什么本地的IDEA也需要下载索引呢？那么直接看下图你就明白了，如果本地没有下载索引的话，在pom.xml文件中添加依赖是得不到任何提示的。 IntelliJ IDEA中Maven插件配置IntelliJ已经内置了对Maven插件的支持，当然你也可以配置自己的Maven，只需要进入Settings-&gt;Maven-&gt;Maven home directory|User settings file|Local repository配置即可。注意如果使用自己配置的Maven，那么一定要勾选Override，否则配置不生效。 IntelliJ14.1更新索引失败原因在使用14.1.X版本的IntelliJ时，更新Maven索引出现如下错误Indexed Maven Repositories - type remore - Error - Idea 14.1.5，根据该链接内所述原因为：这是IntelliJ14.1.X版本中的一个BUG，并且会在下一个发布版本中进行修复，推荐将IntelliJ升级到版本15。 使用国内Maven仓库的镜像鉴于伟大的防火墙，所以推荐使用国内的镜像资源作为Maven中央仓库。推荐使用开源中国Maven库使用帮助，配置很简单就不详述了，有两种方式，其一打开settings.xml文件，加入12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;&lt;!--用一个简单的*号会把所有的仓库地址屏蔽掉--&gt; &lt;name&gt;Nexus osc&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 当然还有第二种方式，就是屏蔽指定的中央仓库，并且还可以加入OSChina的第三方镜像仓库或者多个仓库，配置如下1234567891011121314&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;!--这里指定只屏蔽central仓库--&gt; &lt;name&gt;Nexus osc&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-osc-thirdparty&lt;/id&gt; &lt;mirrorOf&gt;thirdparty&lt;/mirrorOf&gt; &lt;name&gt;Nexus osc thirdparty&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/repositories/thirdparty/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 最后，在执行Maven命令的时候，Maven还需要安装一些插件包，这些插件包的下载地址也让其指向OSChina的Maven地址。修改如下所示1234567891011121314151617181920212223242526272829303132&lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.8&lt;/jdk&gt;&lt;!--指定JDK版本是1.8时自动激活--&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;local private nexus&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;local private nexus&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/profile&gt; 另外你也可以下载开源中国提供的官方纯净版settings.xml文件。 下载Maven仓库的索引在配置完成之后就可以下载仓库索引了，注意这是一个非常耗时的过程，建议利用晚上或者出去午饭时间下载。下载过程及下载完成之后状态如下图所示。本次下载整体耗时在一个小时左右。另外我在思考既然下载一次这么麻烦，那么下载下来的索引存放在哪里呢？我能否将其拷贝到其他机器重复利用呢？于是经过一番搜索我发现了索引的存放位置，并且将其打包拷贝到其他机器的同样位置，但未做测试，不知能否重复利用，如有网友测试完毕，可以告诉我，感谢之。 利用本地Tomcat作为索引下载服务器 首先下载如下两个文件：http://repo1.maven.org/maven2/.index/nexus-maven-repository-index.propertieshttp://repo1.maven.org/maven2/.index/nexus-maven-repository-index.gz 启动一个Apache Tomcat服务器，在其根目录下建立一个/maven2/.index的虚拟目录（注意：如果你使用的是Windows系统，可能无法建立.index件夹，必须使用DOS命令：mkdir .index），把上述两个文件拷贝至该虚拟目录下 编辑C:/WINDOWS/system32/drivers/etc/hosts文件，在文件中加入:127.0.0.1 repo1.maven.org注意：127.0.0.1为步骤2的Apache Tomcat服务器IP地址。 在IDEA的maven插件中更新索引 移除步骤3中在hosts文件中添加的内容 备注：其实该解决办法的总体思路就是先将索引文件整体下载，然后利用本地的Tomcat作为服务器，再从Tomcat上更新索引。 最后如果你想自己配置一个私服，可以参考Maven仓库管理之Nexus。 开源中国镜像存在的问题 开源中国镜像不是很稳定，有时候很快下载完成有时候一直处于Resolving dependencies of ...状态而无法下载 在配置了开源中国第三方库镜像之后，发现一个问题，该库内容更新不及时，很多第三方库中的Jar包版本都非常陈旧。 开源中国的中央仓库与第三方库中存在很多交叉的情况，也就是说中央仓库包括了第三方库中的内容，而且在下载jar文件的时候，默认就是直接从开源中国的中央仓库镜像下载，而不是开源中国的第三方仓库镜像下载。 我给出的建议是，如无必要，移除开源中国的第三方库镜像地址，移除的内容如下 123456&lt;mirror&gt; &lt;id&gt;nexus-osc-thirdparty&lt;/id&gt; &lt;mirrorOf&gt;thirdparty&lt;/mirrorOf&gt; &lt;name&gt;Nexus osc thirdparty&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/repositories/thirdparty/&lt;/url&gt;&lt;/mirror&gt; 针对以上问题，有时候还是需要从国外Maven官方的仓库下载，方法是只需要修改settings.xml文件为官方默认版本即可。现将Maven默认settings.xml贴出 123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;localRepository&gt;D:/apache-maven-3.3.1/repository&lt;/localRepository&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;/profiles&gt;&lt;/settings&gt;]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>IntelliJ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读《黑天鹅》]]></title>
    <url>%2F2015%2F11%2F30%2FRead-the-black-swan%2F</url>
    <content type="text"><![CDATA[作者简介：纳西姆·尼古拉斯·塔勒布，证券交易员，目前担任马塞诸塞大学阿默斯特分校随机科学教授。他在“9·11”之前大量买入行权价格很低，看似毫无价值的认沽权证，用一种独特的方式做空美国股市，直到恐怖分子劫持飞机撞向纽约世贸大楼，他由此获利丰厚，一举成名。此次美国次贷危机爆发之前，他又先知先觉重仓做空，大赚一笔。 我：我一直认为真正能看清社会走向及科学经济等发展趋势的人，才是真正的天才。然而我们平常所见识到的大多是那些整天高谈阔论，到处指指点点，喜好纸上谈兵的人。作者显然是属于前者。 科普黑天鹅事件：不可预测的重大事件。它罕有发生，但一旦出现，就具有很大的影响力。几乎一切重要的事情都逃不过黑天鹅的影响，而现代世界正是被黑天鹅所左右。认识黑天鹅，才能更深刻地认识世界的复杂性，并从不可预知的未来中获益。 我们不会自然而然地认识到自己不会学习。这个问题产生于我们的思维结构：我们不学习规律，而是学习事实，而且只学习事实。最好的例子就是第一次世界大战之后，法国人沿德国人曾经入侵的路线修建了一条防御工事，而希特勒毫不费力地绕过了它。 我：这让我不禁想到了911恐袭，如果可以的话为什么不在驾驶舱安置防弹门，并且只有机长才能从外面打开进入驾驶舱，其它工作人员都是只能从里面打开而不能从外面打开，这样就可以对每一个进入驾驶舱的人进行有效的控制。即便恐怖分子控制了客舱，但是飞机依然牢牢掌控在机长手里，至于以人质相要挟，我想就是有舍才有得了。 在有史以来世界上最强大的国家，最大公司的首席执行官们很可能也不知道世界正在发生什么以及将要发生什么。 我：是的，这个世界是丰富多彩且风云变幻的，没有任何人可以对未知的趋势进行预测。当然了上帝除外（如果真的存在的话），不管是任何人对未来所做的预测都是基于已经发生的事情，然而过去的经验并不足以支撑我们对于未来的预测。由此我联想到我大天朝的诸多专家，你们真的好厉害的，对任何事情都敢进行大胆的预测，棒棒哒。 A. 乔伊表面上快乐地结婚了。他杀了他的妻子。 B. 乔伊表面上快乐地结婚了。他为了得到妻子的遗产而杀了他。请问你举得A和B哪种更有可能发生呢？ 乍看上去第二种情形更有可能发生，而这是完全的逻辑错误，因为第一种情形更宽泛，有更多种可能的原因可以导致他去杀死他的妻子。例如他的妻子是个疯子，或者他的妻子与他人通奸了等等。究其原因，这就是叙述谬误及其突出情感事实的特点会扰乱我们对事件概率的预测。 恐怖主义使人死亡，而最大的杀手仍然是环境，环境每年造成近1300万人死亡。但恐怖主义引起人们的愤怒，这使我们高估了恐怖袭击的可能性，当发生恐怖袭击时，人们的这种倾向更为强烈。 我：之前网上流传“中国人早晚死在中国人的手里”，这一句话就揭露了事件的本质，不论是三聚氰胺、地沟油、各种化学腌制品、以及为了挣钱而给所有病人都进行输液治疗的医生。由于没有信仰，而导致了对生命的蔑视，对道德底线的不断突破。人们对于恐怖主义的痛恨远远大于对环境污染所造成人员死亡的痛恨。换句话说，几个人的死亡是悲剧，而100万人的死亡只是统计学意义上的说法。统计学默默的存在于我们的体内。 9·11中大约2500人直接死于本·拉登所致的恐袭，遇难者的家庭得到各种机构和慈善团体的捐赠。但是，根据研究者的结果，在那一年余下的3个月，将近1000人成为恐怖主义沉默的受害者。为什么？害怕坐飞机转而开车的人面临更高的死亡风险。证据表明那段时间的公路死亡率上升，因为公路比天空更致命。这些家庭没有得到捐助，他们甚至不知道他们的亲人也是本·拉登的受害者。 我：很有意思的视角，要多从其它视角去分析同一个问题。 实际上，很多冒险家自以为是命运的宠儿，这只是因为冒险家很多，而我们没有听到那些背运的冒险家的故事。 我：是的，想想那些尸沉海底或者是尸体腐烂于荒原藏身于苍鹰之腹的人吧，没有人会为他们写书宣传，也没有人去讲述他们没有得到幸运之神宠爱的故事，所以世人往往会简单的认为这些成功的冒险家真幸运，冒险家都是命运的宠儿等等。其实质仅仅是因为失败的冒险家更多，而成功的更少，并且只有幸存下来的人才会宣扬他们自己的故事而已。 问“假设硬币是公平的，抛出得到正面与反面的可能性是相同的。我把它抛出99次，每次都得到正面，下一次抛出得到反面的概率多大？”学院派：是1/2，因为每面出现的可能性相等，都是50%。实战派：不会超过1%，这枚硬币肯定做了手脚，这不可能是公平游戏，也就是说，在抛出99次，每次都是正面的情况下，对公平性的假设很可能是错误的。 我：想想其实这个问题非常普遍，我们在现实生活中面临的不确定性与我们在考试和游戏中遇到的简化情况之间几乎没有相同之处。 心理学家詹姆斯·尚蒂研究了哪些学科有真正的专家，哪些学科没有。是专家的专家：牲畜检验员、宇航员、飞机试飞员、土壤检验员、国际象棋大师、物理学家、数学家、会计师、谷物检验员、图像分析员、保险分析师不是专家的专家：证券经纪商、临床心理医生、精神病医生、大学招生官员、法官、顾问、人事官员、情报分析师、经济学家、金融预测者、金融学教授、政治科学家、国际清算银行员工、个人金融咨询师 我：我觉得这个分类还是比较靠谱的，以后大家看到国内的某个专家大放厥词的时候，思考下他的行业领域就知道了。 任何因为预测而对他人造成伤害的人都应该被称为傻瓜或骗子。有些预测者对社会造成的损害比罪犯更大。 我：是的，但是我们却没有法律对他们量刑。犹然记得欧洲某国的地震学家预测地震失败被判刑，我国是不是应该尽快制定类似法律呢？ 发现的经典模式是这样的：你寻找你知道的东西（比如到达印度的新方法），结果发现了一个你不知道的东西（美洲）。如果你认为我们周围的发明来自于一个闭门造车的人，请再想一想：几乎现在的一切都是偶然的产物。 我：历史总是惊人的相似，这就是原因。 在数学上，一旦有人宣布证明了某个神秘定理，我们就会经常看到突然凭空出现许多类似证明，偶尔还会有人指责别人泄密或剽窃。可能并没有剽窃存在：证明方法存在这一信息本身就是证明方法的一部分。 我：当存在证明方法时，本身就说明了这个问题是可证明的，这一信息也是极具价值的。 作者的观点说：预测要求我们知道未来将发现哪些技术。但这一认识几乎会自动地让我们立即开始发展这些技术。因此，我们不知道我们将知道什么。 我：虽然有点拗口，但是说的非常明白。 美国文化鼓励失败，而不像欧洲和亚洲文化把失败视为耻辱和尴尬。美国的专长在于为世界其他地方承担这些小风险，这正是这个国家具有超常创新力的原因。一旦有了想法就去实施，之后再“完善”这种想法或产品。 我：少说多做，其实说起来容易做起来并不容易。就像编程界的一句名言“talk is cheap, show me the code!”。 哲学家及数学家布莱斯·帕斯卡有个很有意思的观点：我不知道上帝是否存在，但我知道，如果他不存在，我做无神论者就得不到好处，但假如他存在，做无神论者就会损失很大，所以我应该相信上帝。 我：按此道理，难道我也要从一个无神论者变为一个信仰上帝的教徒？哈哈，不过此论断在神学上有严重缺陷：只有相当天真的人才会相信上帝不会因为假信仰惩罚我们。言外之意是不是因为你不是一个虔诚的教徒，上帝同样会惩罚你呢？ 我们很容易忘记我们活着本身就是极大的运气，一个可能性微小的事件，一个极大的偶然。 我：对生活中的任何困难挑战都应微笑着勇敢面对，因为你多活一天就赚一天，没什么好害怕的。 人们总是从自己不赞同的人那里学到最多的东西，但很少人意识到。 我：其实敌人就像一面镜子，它可以照出来我们身上难以发觉的弱点，而这些弱点在我们的好朋友身上也极有可能存在，也许这就从另一角度印证了，物以类聚，人以群分吧。]]></content>
      <categories>
        <category>Life-Talk</category>
      </categories>
      <tags>
        <tag>Thinking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Guava使用指南]]></title>
    <url>%2F2015%2F11%2F29%2FGoogle-Guava-use-guide%2F</url>
    <content type="text"><![CDATA[Guava简介Google Guava很优秀，大有取代Apache Commons之势。闲话少叙，直接上Guava的终极目标，用原汁原味的英文来说就是“Our goal is for you to write less code and for the code you do write to be simpler, cleaner, and more readable”，Guava中主要包括Basic Utilities，Collections，Caches，Functional Idioms，Concurrency，Strings，Primitives Support，Ranges，I/O等等，API参考文档点我，Guava在Github上仓库地址点我 Guava使用示例Preconditions可以用在方法的开始或者是构造函数的开始，对于不合法的校验可以快速报错1234567891011121314151617181920212223242526272829303132333435363738package cn.codepub.guava.demo;import static com.google.common.base.Preconditions.checkArgument;import static com.google.common.base.Preconditions.checkNotNull;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/28 20:12 * &lt;/p&gt; * &lt;p&gt; * ClassName:PreconditionsDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class PreconditionsDemo &#123; public static void main(String[] args) &#123; new Car(null);//Exception in thread "main" java.lang.NullPointerException //Exception in thread "main" java.lang.IllegalArgumentException: speed (0.0) must be positive new Car("Audi").drive(0); &#125;&#125;class Car &#123; private String name; public Car(String name) &#123; this.name = checkNotNull(name);//NPE Null Pointer Exception &#125; public void drive(double speed) &#123; checkArgument(speed &gt; 0.0, "speed (%s) must be positive", speed); &#125;&#125; MoreObjects.toStringHelper()让你可以更优雅的覆写Object.toString()方法123456789101112131415161718192021222324252627282930313233343536package cn.codepub.guava.demo;import com.google.common.base.MoreObjects;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/28 20:26 * &lt;/p&gt; * &lt;p&gt; * ClassName:MoreObjectsDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class MoreObjectsDemo &#123; private String name; private String userId; private String petName; private String sex; @Override public String toString() &#123; //prints:MoreObjectsDemo&#123;name=testName, userId=NO1, petName=PIG&#125; return MoreObjects.toStringHelper(this).add("name", "testName").add("userId", "NO1").add("petName", "PIG").omitNullValues().toString(); //prints:MoreObjectsDemo&#123;name=testName, userId=NO1, petName=PIG&#125; //return MoreObjects.toStringHelper(this).add("name", "testName").add("userId", "NO1").add("petName", "PIG").toString(); &#125; public static void main(String[] args) &#123; System.out.println(new MoreObjectsDemo()); &#125;&#125; 用Stopwatch代替System.nanoTime()简单点说就是Stopwatch使用纳秒计时，使用该类度量时间比使用System.nanoTime()好，好处在哪呢？基于性能和测试来说，它可以作为另一种替代时间源；根据System.nanoTime()的说明，返回的值没有绝对意义，只能解释为相对于另一个返回的时间戳。Stopwatch是一个更有效的抽象,因为它只公开这些相对的值,而不是绝对值。可以通过其提供的start和stop方法得到。123456789101112131415161718192021222324252627282930313233343536373839package cn.codepub.guava.demo;import com.google.common.base.Stopwatch;import java.util.concurrent.TimeUnit;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/28 20:53 * &lt;/p&gt; * &lt;p&gt; * ClassName:StopWatchDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class StopWatchDemo &#123; public static void main(String[] args) &#123; //The Construct Stopwatch is default access permissions Stopwatch stopwatch = Stopwatch.createUnstarted(); //start stopwatch.start(); System.out.println("You can do something!"); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; stopwatch.stop(); long nanos = stopwatch.elapsed(TimeUnit.NANOSECONDS); System.out.println(nanos);//1000076976 &#125;&#125; CharMatcher123456789101112131415161718192021222324252627282930package cn.codepub.guava.demo;import com.google.common.base.CharMatcher;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/28 21:31 * &lt;/p&gt; * &lt;p&gt; * ClassName:CharMatcherDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class CharMatcherDemo &#123; private static final CharMatcher ID_MATCHER = CharMatcher.DIGIT.or(CharMatcher.is('-')); public static void main(String[] args) &#123; String userID = "123454-333"; String s = ID_MATCHER.retainFrom(userID); System.out.println(s);//123454-333 s = ID_MATCHER.retainFrom("1 test 11-222"); System.out.println(s);//111-222 &#125;&#125; String Joining12345678910111213141516171819202122232425262728293031package cn.codepub.guava.demo;import com.google.common.base.Joiner;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/28 21:43 * &lt;/p&gt; * &lt;p&gt; * ClassName:StringJoiningDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class StringJoiningDemo &#123; private static final Joiner JOINER = Joiner.on(",").skipNulls(); //set replace string private static final Joiner JOINER_USE_FOR_NULL = Joiner.on(",").useForNull("replace"); public static void main(String[] args) &#123; String join = JOINER.join("Kurt", "Kevin", null, "Chris"); System.out.println(join);//Kurt, Kevin, Chris join = JOINER_USE_FOR_NULL.join("Kurt", "Kevin", null, "Chris"); System.out.println(join); &#125;&#125; String Splitting使用指定的分隔符对字符串进行拆分，默认对空白符不做任何处理，并且不会静默的丢弃末尾分隔符，如果需要处理的话要显式调用trimResults()，omitEmptyStrings() 12345678910111213141516171819202122232425262728293031323334package cn.codepub.guava.demo;import com.google.common.base.Splitter;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/28 21:57 * &lt;/p&gt; * &lt;p&gt; * ClassName:SplitterDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class SplitterDemo &#123; public static void main(String[] args) &#123; Iterable&lt;String&gt; split = Splitter.on(',').trimResults().omitEmptyStrings() .split("foo, ,bar,quux,blue,"); for (String s : split) &#123; System.out.print(s + "---");//foo---bar---quux---blue--- &#125; System.out.println(); String[] split1 = "foo, ,bar,quux,blue,".split(","); for (String s : split1) &#123; System.out.print(s + "---");//foo--- ---bar---quux---blue--- &#125; &#125;&#125; Optional1234567891011121314151617181920212223242526272829303132333435363738package cn.codepub.guava.demo;import java.util.Optional;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/28 22:24 * &lt;/p&gt; * &lt;p&gt; * ClassName:OptionalDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class OptionalDemo &#123; public static void main(String[] args) &#123; //Creating an Optional&lt;T&gt; //Optional.of(notNull); //Optional.empty(); //Optional.ofNullable(maybeNull); String test = "test"; // if test == null will throws NPE Optional&lt;String&gt; s = Optional.of(test); System.out.println(s.get());//test Optional&lt;Object&gt; empty = Optional.empty(); System.out.println(empty);//Optional.empty Optional&lt;Object&gt; o = Optional.ofNullable(null); System.out.println(o);//Optional.empty &#125;&#125; Hashing1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package cn.codepub.guava.demo;import com.google.common.base.Charsets;import com.google.common.hash.HashCode;import com.google.common.hash.Hashing;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/29 16:24 * &lt;/p&gt; * &lt;p&gt; * ClassName:HashingDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class HashingDemo &#123; public static void main(String[] args) &#123; byte sex = 1; Person person = new Person(100, "eric", "wang", 1L, sex); System.out.println(person.hashCode());//1935260882 &#125;&#125;class Person &#123; private int age; private String firstName; private String lastName; private long id; private byte sex; public Person(int age, String firstName, String lastName, long id, byte sex) &#123; this.age = age; this.firstName = firstName; this.lastName = lastName; this.id = id; this.sex = sex; &#125; public int getAge() &#123; return this.age; &#125; public String getFirstName() &#123; return this.firstName; &#125; public String getLastName() &#123; return this.lastName; &#125; public long getId() &#123; return this.id; &#125; public byte getSex() &#123; return this.sex; &#125; @Override public int hashCode() &#123; HashCode hashCode = Hashing.murmur3_128().newHasher().putInt(this.getAge()).putLong(this.getId()) .putString(this.getFirstName(), Charsets.UTF_8).putString(this.getLastName(), Charsets.UTF_8) .putByte(this.getSex()).hash(); return hashCode.hashCode(); &#125;&#125; Caching12345678910111213141516171819202122232425262728293031323334353637package cn.codepub.guava.demo;import com.google.common.cache.*;import com.sun.corba.se.impl.orbutil.graph.Graph;import java.security.Key;import java.util.concurrent.TimeUnit;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/29 16:54 * &lt;/p&gt; * &lt;p&gt; * ClassName:CachingDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class CachingDemo &#123; LoadingCache&lt;Key, Graph&gt; cache = CacheBuilder.newBuilder().maximumSize(10000) .expireAfterWrite(10, TimeUnit.MINUTES).removalListener(new RemovalListener&lt;Object, Object&gt;() &#123; @Override public void onRemoval(RemovalNotification&lt;Object, Object&gt; removalNotification) &#123; //implements your listener &#125; &#125;).build(new CacheLoader&lt;Key, Graph&gt;() &#123; @Override public Graph load(Key key) throws Exception &#123; return null; &#125; &#125;);&#125; 链式调用Guava中提供了大量方法，让我们可以使用链式调用的方式从而减少代码量。简单来说方法链一般适合对一个对象进行连续操作（集中在一句代码）。一定程度上可以减少代码量，缺点是它占用了函数的返回值。如果不需要使用到函数返回值的话，建议大家在封装自己的代码库的时候可以使用这种方式，提供一个简单的demo如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package cn.codepub.guava.demo;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/11/29 16:00 * &lt;/p&gt; * &lt;p&gt; * ClassName:ChainEncapsulationDemo * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class ChainEncapsulationDemo &#123; private String message = ""; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public static void main(String[] args) &#123; ChainEncapsulationDemo chainEncapsulationDemo = new ChainEncapsulationDemo(); chainEncapsulationDemo = chainEncapsulationDemo.method1().method2().method3(); System.out.println(chainEncapsulationDemo.getMessage()); &#125; public ChainEncapsulationDemo method1() &#123; this.setMessage(this.getMessage() + "add method1 ...\n"); return this; &#125; public ChainEncapsulationDemo method2() &#123; this.setMessage(this.getMessage() + "add method2 ...\n"); return this; &#125; public ChainEncapsulationDemo method3() &#123; this.setMessage(this.getMessage() + "add method3 ..."); return this; &#125;&#125; Apache VS. Guava关于使用Apache Commons还是Guava的讨论看这里]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Guava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 14.10安装和卸载搜狗拼音输入法]]></title>
    <url>%2F2015%2F11%2F27%2FUbuntu14-10-install-and-uninstall-sogou-input-method%2F</url>
    <content type="text"><![CDATA[安装搜狗输入法 参考了其他一些资料都说添加fcitx的PPA，但是我不确定有木有用 12345678910111213eric@eric-VirtualBox:/etc/apt$ sudo add-apt-repository ppa:fcitx-team/nightly Experimental releases of Fcitx, use with caution. More info: https://launchpad.net/~fcitx-team/+archive/ubuntu/nightlyPress [ENTER] to continue or ctrl-c to cancel adding itgpg: keyring `/tmp/tmp_2dkioxm/secring.gpg' createdgpg: keyring `/tmp/tmp_2dkioxm/pubring.gpg' createdgpg: requesting key 7E5FA1EE from hkp server keyserver.ubuntu.comgpg: /tmp/tmp_2dkioxm/trustdb.gpg: trustdb createdgpg: key 7E5FA1EE: public key "Launchpad PPA for Fcitx Team PPA" importedgpg: Total number processed: 1gpg: imported: 1 (RSA: 1)OK 然后刷新软件源，安装一些依赖软件 1sudo apt-get update 刷新完成之后，安装im-config1sudo apt-get install im-config 继续安装搜狗输入法依赖的fcitx一系列文件，输入命令：sudo apt-get -f install，遇到有Y/N的地方直接输Y12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697eric@eric-VirtualBox:~/Desktop$ sudo apt-get -f installReading package lists... DoneBuilding dependency treeReading state information... DoneCorrecting dependencies... DoneThe following extra packages will be installed: fcitx fcitx-bin fcitx-config-common fcitx-config-gtk fcitx-data fcitx-frontend-all fcitx-frontend-gtk2 fcitx-frontend-gtk3 fcitx-frontend-qt4 fcitx-libs fcitx-libs-gclient fcitx-libs-qt fcitx-module-dbus fcitx-module-kimpanel fcitx-module-lua fcitx-module-x11 fcitx-modules fcitx-ui-classicSuggested packages: fcitx-tools fcitx-m17n kdebase-bin plasma-widgets-kimpanelRecommended packages: fcitx-frontend-qt5The following NEW packages will be installed: fcitx fcitx-bin fcitx-config-common fcitx-config-gtk fcitx-data fcitx-frontend-all fcitx-frontend-gtk2 fcitx-frontend-gtk3 fcitx-frontend-qt4 fcitx-libs fcitx-libs-gclient fcitx-libs-qt fcitx-module-dbus fcitx-module-kimpanel fcitx-module-lua fcitx-module-x11 fcitx-modules fcitx-ui-classic0 upgraded, 18 newly installed, 0 to remove and 7 not upgraded.1 not fully installed or removed.Need to get 35.6 kB/2,315 kB of archives.After this operation, 8,956 kB of additional disk space will be used.Do you want to continue? [Y/n] yWARNING: The following packages cannot be authenticated! fcitx-config-common fcitx-config-gtkInstall these packages without verification? [y/N] yGet:1 http://mirrors.aliyun.com/ubuntu/ precise/universe fcitx-config-common all 0.4.0-2 [3,548 B]Get:2 http://mirrors.aliyun.com/ubuntu/ precise/universe fcitx-config-gtk amd64 0.4.0-2 [32.1 kB]Fetched 35.6 kB in 0s (247 kB/s)Selecting previously unselected package fcitx-libs:amd64.(Reading database ... 168557 files and directories currently installed.)Preparing to unpack .../fcitx-libs_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-libs:amd64 (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-bin.Preparing to unpack .../fcitx-bin_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-bin (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-data.Preparing to unpack .../fcitx-data_1%3a4.2.8.5-2~utopic1_all.deb ...Unpacking fcitx-data (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-modules.Preparing to unpack .../fcitx-modules_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-modules (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx.Preparing to unpack .../fcitx_1%3a4.2.8.5-2~utopic1_all.deb ...Unpacking fcitx (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-libs-gclient:amd64.Preparing to unpack .../fcitx-libs-gclient_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-libs-gclient:amd64 (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-module-dbus.Preparing to unpack .../fcitx-module-dbus_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-module-dbus (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-frontend-gtk2:amd64.Preparing to unpack .../fcitx-frontend-gtk2_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-frontend-gtk2:amd64 (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-frontend-gtk3:amd64.Preparing to unpack .../fcitx-frontend-gtk3_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-frontend-gtk3:amd64 (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-libs-qt:amd64.Preparing to unpack .../fcitx-libs-qt_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-libs-qt:amd64 (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-frontend-qt4:amd64.Preparing to unpack .../fcitx-frontend-qt4_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-frontend-qt4:amd64 (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-module-kimpanel.Preparing to unpack .../fcitx-module-kimpanel_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-module-kimpanel (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-config-common.Preparing to unpack .../fcitx-config-common_0.4.0-2_all.deb ...Unpacking fcitx-config-common (0.4.0-2) ...Selecting previously unselected package fcitx-config-gtk.Preparing to unpack .../fcitx-config-gtk_0.4.0-2_amd64.deb ...Unpacking fcitx-config-gtk (0.4.0-2) ...Selecting previously unselected package fcitx-frontend-all.Preparing to unpack .../fcitx-frontend-all_1%3a4.2.8.5-2~utopic1_all.deb ...Unpacking fcitx-frontend-all (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-module-lua.Preparing to unpack .../fcitx-module-lua_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-module-lua (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-module-x11.Preparing to unpack .../fcitx-module-x11_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-module-x11 (1:4.2.8.5-2~utopic1) ...Selecting previously unselected package fcitx-ui-classic.Preparing to unpack .../fcitx-ui-classic_1%3a4.2.8.5-2~utopic1_amd64.deb ...Unpacking fcitx-ui-classic (1:4.2.8.5-2~utopic1) ...Processing triggers for man-db (2.7.0.2-2) ...Processing triggers for shared-mime-info (1.2-0ubuntu3) ...Processing triggers for gnome-menus (3.10.1-0ubuntu2) ...Processing triggers for desktop-file-utils (0.22-1ubuntu2) ...Processing triggers for bamfdaemon (0.5.1+14.10.20140925-0ubuntu1) ...Rebuilding /usr/share/applications/bamf-2.index...Processing triggers for mime-support (3.55ubuntu1) ...Processing triggers for hicolor-icon-theme (0.13-1) ...Processing triggers for libgtk2.0-0:amd64 (2.24.25-0ubuntu1) ...Processing triggers for libgtk-3-0:amd64 (3.12.2-0ubuntu15) ... 到搜狗官网下载搜狗拼音输入法，选择你系统对应的软件包，我系统是64位的，所以我选择了amd64的 下载地址点我，另外搜狗也提供了安装说明帮助页面，点我查看帮助。 下载完成之后，可以直接将文件放在桌面上进行安装 先看看下载的deb文件1234567eric@eric-VirtualBox:~/Desktop$ lltotal 18304drwxr-xr-x 2 eric eric 4096 11月 27 22:06 ./drwxr-xr-x 18 eric eric 4096 11月 27 21:34 ../-rw-rw-r-- 1 eric eric 627 11月 27 22:06 cache.txt-rwxr-x--- 1 root root 18730988 11月 27 00:01 sogoupinyin_2.0.0.0068_amd64.deb*eric@eric-VirtualBox:~/Desktop$ 输入sudo dpkg -i sogoupinyin_2.0.0.0068_amd64.deb命令开始安装输入法，可使用tab键自动补全命令，安装完成之后使用如下命令启用sudo im-config -s fcitx -z default，完成之后别忘记重启系统。重启之后点击右上角的那个软件盘即可看到搜狗输入法安装成功，点击即可输入汉字了。 卸载搜狗输入法 首先使用命令查看下安装的搜狗拼音输入法sudo dpkg -l so*，然后先卸载搜狗拼音sudo apt-get purge sogoupinyin 卸载fcitx，sudo apt-get purge fcitx 彻底卸载fcitx及相关配置，sudo apt-get autoremove 最后别忘注销或者重启系统，如果注销按钮不能使用，可以使用命令sudo pkill Xorg，当再次登录系统之后，可以看到搜狗输入法已经完全被卸载干净了。]]></content>
      <categories>
        <category>Operating System</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜狗2016校园招聘之编程题解析-大数据开发]]></title>
    <url>%2F2015%2F10%2F24%2F2016-campus-recruitment-of-sougou-programming-problem-resolution-big-data-development%2F</url>
    <content type="text"><![CDATA[最近邻居 解题思路： 使用JDK中的Point2D类，该类定义了坐标系空间中的一个点 Point2D是一个抽象类，但是在该类内部定义了静态的Double类，并且Double继承自Point2D 可以通过Double的构造方法来实例化空间中的某个点 将所有的输入数据全部实例化并存放在一个Point2D.Double的数组中 对该数组进行暴力破解，计算其中任意两个点之间的距离，时间复杂度为$O(n^2)$，并保留下最小的两个点的编号，且编号小的在前 Java算法实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.awt.geom.Point2D;import java.util.Scanner;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/26 15:26 * &lt;/p&gt; * &lt;p&gt; * ClassName:Main * &lt;/p&gt; * &lt;p&gt; * Description: * 测试数据 * 3 * 1.0 1.0002 * 3.03 3.023 * 0.0 -0.001 * Closest points: 0, 2 * Process finished with exit code 0 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class Main &#123; static int[] getClosest(Point2D.Double[] points) &#123; int[] result = new int[2]; double distance = Double.MAX_VALUE; for (int i = 0; i &lt; points.length; i++) &#123; for (int j = i + 1; j &lt; points.length; j++) &#123; if (i != j) &#123; double distance1 = points[i].distance(points[j]); if (distance1 &lt; distance) &#123; distance = distance1; if (i &lt; j) &#123; result[0] = i; result[1] = j; &#125; else &#123; result[0] = j; result[i] = i; &#125; &#125; &#125; &#125; &#125; return result; &#125; public static void main(String[] args) &#123; Point2D.Double[] points; Scanner input = new Scanner(System.in); &#123; int n = input.nextInt(); input.nextLine(); points = new Point2D.Double[n]; for (int i = 0; i &lt; n; ++i) &#123; double x = input.nextDouble(); double y = input.nextDouble(); input.nextLine(); points[i] = new Point2D.Double(x, y); &#125; &#125; int[] result = getClosest(points); System.out.printf("Closest points: %d, %d\n", result[0], result[1]); &#125;&#125; 混乱还原 解题思路： 利用伪随机特性，只要时间种子一样且上限一样，其实随机数每次都会产生相同的数 既然要求还原，那么我们从后往前执行对应的操作即可 使用一个额外的栈来存储所产生的随机数 在乱序操作中，是将随机数对应的元素与最后一个元素进行交换，那么还原的时候，就要从第一个元素开始与最后产生的那个随机数对应的元素进行交换，依次类推，直到栈空即可 Java算法实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import java.util.Arrays;import java.util.Random;import java.util.Scanner;import java.util.Stack;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/26 15:26 * &lt;/p&gt; * &lt;p&gt; * ClassName:Main * &lt;/p&gt; * &lt;p&gt; * Description:输入数据 * 12312 3 1 2 3 * Success! * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class Main &#123; static Stack&lt;Integer&gt; s = new Stack&lt;Integer&gt;(); static void shuffle(int a[], int seed) &#123; int n = a.length; Random random = new Random(seed); for (; n &gt; 1; n--) &#123; int r = random.nextInt(n); int tmp = a[n - 1]; a[n - 1] = a[r]; a[r] = tmp; &#125; &#125; static void restore(int a[], int seed) &#123; int n = a.length; Random random = new Random(seed); int temp = n; for (; temp &gt; 1; temp--) &#123; int r = random.nextInt(temp); s.add(r); &#125; for (int i = 0; i &lt; n; i++) &#123; if (!s.isEmpty()) &#123; int r = s.pop(); int tmp = a[i + 1]; a[i + 1] = a[r]; a[r] = tmp; &#125; &#125; &#125; public static void main(String[] args) &#123; int seed, n, i; int[] a, b; Scanner input = new Scanner(System.in); &#123; seed = input.nextInt(); n = input.nextInt(); a = new int[n]; for (i = 0; i &lt; n; ++i) a[i] = input.nextInt(); &#125; b = Arrays.copyOf(a, a.length); shuffle(a, seed); restore(a, seed); for (i = 0; i &lt; n; i++) &#123; if (a[i] != b[i]) break; &#125; if (i == n) System.out.printf("Success!\n"); else System.out.printf("Failed!\n"); &#125;&#125; 算法如有疏漏或不妥之处，还望不吝赐教！]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Python的CSV模块读写CSV文件]]></title>
    <url>%2F2015%2F10%2F08%2FUsing-the-Python-CSV-module-to-read-and-write-CSV-file%2F</url>
    <content type="text"><![CDATA[开发环境：Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)] on win32 自序也许你会说，我为什么要学习使用CSV模块呢？没有CSV模块我一样可以解析操作CSV文件，比如下面这种代码：12345with open('stocks.csv') as f:for line in f: row = line.split(',') # process row ... 使用这种方式的一个缺点就是你仍然需要去处理一些棘手的细节问题。比如，如果某些字段值被引号包围，你不得不去除这些引号。另外，如果一个被引号包围的字段碰巧含有一个逗号，那么程序就会因为产生一个错误而停止。 默认情况下，CSV库可识别Microsoft Excel所使用的CSV编码规则。这或许也是最常见的形式，并且也会给你带来最好的兼容性。然而，如果你查看CSV的文档，就会发现有很多种方法将它应用到其他编码格式上（如修改分隔字符等，用Tab分隔）。所以你应该总是优先选择CSV模块分割或解析CSV数据。 CSV格式简介逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号），其文件以纯文本形式存储表格数据（数字和文本）。纯文本意味着该文件是一个字符序列，不含必须像二进制数字那样被解读的数据。CSV文件由任意数目的记录组成，记录间以某种换行符分隔；每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或制表符。通常，所有记录都有完全相同的字段序列。 CSV文件注意点如果你正在读取CSV数据并将它们转换为命名元组，需要注意对列名进行合法性认证。一个CSV格式文件有一个包含非法标识符的列头行，这样最终会导致在创建一个命名元组时产生一个ValueError异常而失败。为了解决这问题，你可能不得不先去修正列标题。123456789import rewith open('stock.csv') as f: f_csv = csv.reader(f) headers = [ re.sub('[^a-zA-Z_]', '_', h) for h in next(f_csv) ] Row = namedtuple('Row', headers) for r in f_csv: row = Row(*r) # Process row ... 还有重要的一点需要强调的是，CSV产生的数据都是字符串类型的，它不会做任何其他类型的转换。如果你需要做这样的类型转换，你必须自己手动去实现。 CSV模块操作示例输入文件：stocks.csv1234567Symbol,Price,Date,Time,Change,Volume"AA",39.48,"6/11/2007","9:36am",-0.18,181800"AIG",71.38,"6/11/2007","9:36am",-0.15,195500"AXP",62.58,"6/11/2007","9:36am",-0.46,935000"BA",98.31,"6/11/2007","9:36am",+0.12,104800"C",53.08,"6/11/2007","9:36am",-0.25,360900"CAT",78.29,"6/11/2007","9:36am",-0.23,225400 输出文件：dest.csv1234Symbol,Price,Date,Time,Change,VolumeAA,39.48,6/11/2007,9:36am,-0.18,181800AIG,71.38,6/11/2007,9:36am,-0.15,195500AXP,62.58,6/11/2007,9:36am,-0.46,935000 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#!/usr/bin/python# coding: UTF-8"""Created on 2015/9/30 14:55@author: 'WX'"""from collections import namedtupleimport csvif __name__ == "__main__": fileName = 'stocks.csv' writeFileName = 'dest.csv' with open(fileName) as f: reader = csv.reader(f) headers = next(reader) print("使用下标进行访问") for row in reader: # 可以使用下标进行访问，row是一个元组 length = len(row) for i in range(length): print(row[i], end='\t') print() print('------------------------------------------------') with open(fileName) as f: # 使用命名元组访问 reader = csv.reader(f) headers = next(reader) Row = namedtuple('Row', headers) print("使用命名元组进行访问") for r in reader: row = Row(*r) # 这时候就可以使用首行的列名来进行访问了 line = '%s\t%s\t%s\t%s\t%s\t%s' % (row.Symbol, row.Price, row.Date, row.Time, row.Change, row.Volume) print(line) print('------------------------------------------------') with open(fileName) as f: # 将内容读取到字典序列中，然后用key去读取 dict_reader = csv.DictReader(f) print("使用字典序列进行访问") for row in dict_reader: line = '%s\t%s\t%s\t%s\t%s\t%s' % ( row['Symbol'], row['Price'], row['Date'], row['Time'], row['Change'], row['Volume']) print(line) print('------------------------------------------------') print("使用元组方式写入，SUCCESS！") with open(writeFileName, mode='w', newline='') as wf: headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume'] rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ] writer = csv.writer(wf) writer.writerow(headers) writer.writerows(rows) print("使用字典方式写入，SUCCESS！") # 在Windows平台需要指定newline=''，否则在两行内容之间会多出一行空行 with open(writeFileName, mode='w', newline='') as wf: headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume'] rows = [&#123;'Symbol': 'AA', 'Price': 39.48, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': -0.18, 'Volume': 181800&#125;, &#123;'Symbol': 'AIG', 'Price': 71.38, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': -0.15, 'Volume': 195500&#125;, &#123;'Symbol': 'AXP', 'Price': 62.58, 'Date': '6/11/2007', 'Time': '9:36am', 'Change': -0.46, 'Volume': 935000&#125;, ] writer = csv.DictWriter(wf, headers) writer.writeheader() writer.writerows(rows) # 更换一种分隔符写入文件 with open(writeFileName, mode='w', newline='') as wf: headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume'] rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ] writer = csv.writer(wf, delimiter=';') writer.writerow(headers) writer.writerows(rows) print("使用分好作为分隔符写入，SUCCESS！") 控制台输出1234567891011121314151617181920212223242526272829C:\Python34\python.exe E:/workspaces/Python3/edu/shu/python/shumo/CsvUtil.py使用下标进行访问AA 39.48 6/11/2007 9:36am -0.18 181800AIG 71.38 6/11/2007 9:36am -0.15 195500AXP 62.58 6/11/2007 9:36am -0.46 935000BA 98.31 6/11/2007 9:36am +0.12 104800C 53.08 6/11/2007 9:36am -0.25 360900CAT 78.29 6/11/2007 9:36am -0.23 225400------------------------------------------------使用命名元组进行访问AA 39.48 6/11/2007 9:36am -0.18 181800AIG 71.38 6/11/2007 9:36am -0.15 195500AXP 62.58 6/11/2007 9:36am -0.46 935000BA 98.31 6/11/2007 9:36am +0.12 104800C 53.08 6/11/2007 9:36am -0.25 360900CAT 78.29 6/11/2007 9:36am -0.23 225400------------------------------------------------使用字典序列进行访问AA 39.48 6/11/2007 9:36am -0.18 181800AIG 71.38 6/11/2007 9:36am -0.15 195500AXP 62.58 6/11/2007 9:36am -0.46 935000BA 98.31 6/11/2007 9:36am +0.12 104800C 53.08 6/11/2007 9:36am -0.25 360900CAT 78.29 6/11/2007 9:36am -0.23 225400------------------------------------------------使用元组方式写入，SUCCESS！使用字典方式写入，SUCCESS！Process finished with exit code 0 分析和统计最后，如果你读取CSV数据的目的是做数据分析和统计的话，你可能需要看一看Pandas包。Pandas包含了一个非常方便的函数叫pandas.read_csv()，它可以加载CSV数据到一个DataFrame对象中去。然后利用这个对象你就可以生成各种形式的统计、过滤数据以及执行其他高级操作了。 参考文献[1] http://baike.baidu.com/subview/468993/5926031.htm[2] http://python3-cookbook.readthedocs.org/zh_CN/latest/c06/p01_read_write_csv_data.html?highlight=csv]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员必备工具箱]]></title>
    <url>%2F2015%2F10%2F07%2Fprogrammer-essential-toolbox%2F</url>
    <content type="text"><![CDATA[平台工具箱 Everything：是速度最快的文件名搜索软件。其速度之快令人震惊，百 GB 硬盘几十万个文件，可以在几秒钟之内完成索引；文件名搜索瞬间呈现结果，简直是秒杀 Windows 搜索功能 Listary：是一款用于 Windows 的文件名定位/搜索辅助软件。它为 Windows 传统低效的文件打开/保存对话框提供了便捷、人性化的文件（夹）定位方式，同时改善了常见文件管理器中文件夹切换的效率 MobaXterm：“十项全能”的远程终端登录软件。破解 Securecrt 怕中毒，Xshell 用着不爽，Putty 太单薄，手头没 Mac 用不了 iterm2。那就试试这个全能开源的终端吧 Total Commander：是一款应用于 Windows 平台的文件管理器，它包含两个并排的窗口，这种设计可以让用户方便地对不同位置的“文件或文件夹”进行操作，例如复制、移动、删除、比较等，相对 Windows 资源管理器而言方便很多，极大地提高了文件操作的效率，被广大软件爱好者亲切地简称为：TC 。它拥有文件快速预览、快速搜索、多标签、文件比较、批量重命名、FTP 客户端等诸多实用的功能，并可通过大量的插件进行个性化配置 ExtremeCopy：是国外开发的一款用于 Windows 的文件复制软件，该软件无论在 Windows XP还是在Windows 7 都能获得比系统自带的复制功能要快20%~120%的性能提升。根据官方的视频显示其不仅比 Windows 7 自带的文件复制功能要快，甚至还比目前国外流行的 TeraCopy 和 SuperCopier 还要快得多 Beyond Compare：是一套由Scooter Software推出的内容比较工具软件。除了可以作文件比较以外，还可以比对文件目录、FTP目录及压缩文件的内容等。因为这些功能，Beyond Compare被应用于版本控制及数据同步的工作上 XMind：是一个由香港 XMind 公司开发的脑力激荡法和心智图的软件工具，其主要用途为帮助用户捕捉想法，组织各类报表。它支持心智图、石川图（又称因果图或鱼骨图）、树形结构图、组织结构图和电子表格。亦可以用来做知识管理，会议记录，任务管理和尽管去做 SecureCRT：是VanDyke Software的一个商业SSH、Telnet客户端和虚拟终端软件 Postman：是一种网页调试与发送网页HTTP请求的工具。我们可以用来很方便的模拟GET/POST/DELETE/HEAD/PUT等请求来调试接口 Navicat Premium：是一套数据库管理工具，结合了其它Navicat成员的功能，支持单一程序同時连接到MySQL、MariaDB、SQL Server、SQLite、Oracle和PostgreSQL数据库。可满足现今数据库管理系统的使用功能，包括存储过程、事件、触发器、函数等，最重要的是Navicat的功能不仅符合专业开发人员的所有需求，对数据库服务器的新手来说学习起来也相当容易 Sublime Text3：文本编辑器，好用、漂亮、强大、极客 为知笔记：知识管理工具，付费产品，入坑需谨慎 StrokesPlus：鼠标手势工具，前进后退翻页放大缩小关闭全部鼠标手势搞定，再也不用一个个点了，极大解放你的鼠标手 Ad Muncher：俗称大奶牛，富有盛名的广告拦截软件。历史悠久，口碑极好。现已成为免费软件，下载地址 ManicTime：分秒不漏的记录你在用电脑的时候到底在干什么，可以自动分类各种操作，形成统计报表。督促自己干正事的利器，下载地址，破解版点这里 TeamViewer：远程控制软件，是一个能在任何防火墙和NAT代理的后台用于远程控制，桌面共享和文件传输的简单且快速的解决方案 FastStone Capture：话不多说，简单好用小巧的截图工具，推荐 邮箱客户端：Yomail，做的很不错，登录邮箱之后会自动进行配置 虚拟机软件：Oracle VM VirtualBox，堪比 VMware 的好用虚拟机软件 视频播放器：PotPlayer，韩国人开发的绿色播放器，强大、小巧、免费，最棒的是其视频加速播放功能，在我与国内的常用视频播放软件的对比实验中，视频加速播放依然保留语音画面流畅度的没有超过它的，在我观看学习视频的过程中，为我节约了大量时间，我通常是2-2.5倍加速播放 Python的IDE：JetBrains PyCharm，专业强大 Java的IDE：IntelliJ IDEA，一句话，用了IDEA之后再也不愿用Eclipse了 PHP的IDE：PhpStorm，JetBrains出品，必属精品 JavaScript的IDE：WebStorm，JetBrains出品，必属精品 JetBrains全栈IDE：AppCode | CLion | DataGrip | GoLand | IntelliJ IDEA | PhpStorm | PyCharm | Rider | RubyMine | WebStorm 浏览器：Chrome，不需要理由，不理解的请绕道吧 飞秋：超级牛逼的两台电脑用网线直连进行文件传输工具，只需要设置IP为同一网段即可，在我从公司离职，将文件资料转移到我的笔记本上的时候超级有用 GifCam：Gif录屏工具，非常棒的屏幕录像工具，可生成Gif格式图片 坚果云：一个稳定、安全、方便的网盘服务可能是很多人的理想需求，在国内，「坚果云」或许是目前满足全部条件的一个选择 Wireshark：是一个免费开源的网络数据包分析软件。网络数据包分析软件的功能是截取网络数据包，并尽可能显示出最为详细的网络数据包数据。在过去，网络数据包分析软件是非常昂贵，或是专门属于营利用的软件，Wireshark的出现改变了这一切 Shadowsocks：是一种基于Socks5代理方式的网络数据加密传输包，并采用Apache许可证、GPL、MIT许可证等多种自由软件许可协议开放源代码。shadowsocks分为服务器端和客户端，在使用之前，需要先将服务器端部署到服务器上面，然后通过客户端连接并创建本地代理 Camtasia Studio：是最专业的屏幕录像和编辑的软件套装。软件提供了强大的屏幕录像（Camtasia Recorder）、视频的剪辑和编辑（Camtasia Studio）、视频菜单制作（Camtasia MenuMaker）、视频剧场（Camtasia Theater）和视频播放功能（Camtasia Player）等。使用本套装软件，用户可以方便地进行屏幕操作的录制和配音、视频的剪辑和过场动画、添加说明字幕和水印、制作视频封面和菜单、视频压缩和播放 屏幕录像专家：是一款专业的屏幕录像制作工具，这款软件界面是中文版本，里面的内容并不怎么复杂，录制视频和简单按设置的快捷键、点击录制键、或者点击三角按钮，就可以录制了 屏幕画笔：一款小巧的绘画软件。有了它，你就可以将屏幕作为画板，在上面自由书写和绘画了。所绘制的图片可以保存为多种格式，可用于教学演示，产品展示 ZoomIt：演示必备辅助软件 ZoomIt 是一款非常实用的投影演示辅助软件。它源自Sysinternals公司，后来此公司被微软收购，因此，有些网友也称ZoomIt为微软放大镜。ZoomIt体积小巧（只有一个exe文件，0.2MB）、完全免费、易于使用。通过快捷键可以很方便地调用ZoomIt三项功能：屏幕放大、屏幕标注、定时提醒 Chrome Plugins AdBlock：最受欢迎的Chrome扩展，拥有超过4000万用户！屏蔽整个互联网上的广告 ADfree.Player.Online：一款屏蔽国内主要视频网站部分视频广告的扩展 Adkill and Media Download：去视频广告、视频音频下载、正常显示反盗链图片三合一 Alexa Traffic Rank：Alexa排名是指网站的世界排名，非常有权威。直接主流网站或博客绝对是有Alexa排名的 Better History：更好地查看您的历史记录。为查看您的历史记录带来最好的搜索体验，最清晰的界面和最有帮助的筛选 Currently：是一款简约又优美的 Chrome 新标签页功能扩展，它用当前时间和天气更换新的标签屏幕。可以显示时间以及天气，右下角还可以切换回传统标签页 Enable Copy：在不允许复制的页面上成功复制 Firebug Lite for Google Chrome™：前端调试必备，不多说 Imagus：是一款图片浏览增强辅助扩展，是在线图片浏览编辑chrome插件。用户在浏览器安装了imagus插件后，浏览大量图片网站的时候，鼠标悬停在缩略图上就出现大图，并且可以通过快捷键更改图片大小、旋转、保存到列表、发送到搜索引擎等等操作 JSONView：验证和查看JSON格式数据 Markdown Pretty：一个让chrome能解析markdown文件(.md, .markdown)的插件，并提供更加舒适中文阅读环境 Momentum：是一款自动更换壁纸，自带时钟，任务日历和工作清单的插件。官方的解释就是：替换你 Chrome 浏览器默认的“标签页”。里面的图片全部来自500PX里面的高清图，无广告，无弹窗，非常适合笔记本使用，让装逼再上新台阶。让我来感受下出自细节，触及心灵的美 Octotree：是不是受够了Github上面一层一层点击目录？那么用我吧，帮你在左侧自动生成目录树 One Tab：强烈推荐，使用场景是这样的，我们使用Chrome经常会一次打开好多tab，很多是会用到的，又不舍得关，内存又耗着，这个时候点击下OneTab，直接把所有tab回收，然后每天的历史都给你记录着，接着你可以一键还原某一天的tab，真乃为Chrome而生 Page load time：显示页面加载时间 Proxy SwitchyOmega：轻松快捷地管理和切换多个代理设置 Scroll To Top：不是所有的网站都良心的在底部放置了返回顶部的按钮，所以使用了这个工具，它会在网页右侧生成一个返回顶部和底部的按钮，当然也可以自定义位置 Scroll To Top Button：不是所有的网站都良心的在底部放置了返回顶部的按钮，所以使用了这个工具，它会在网页右侧生成一个返回顶部和底部的按钮，当然也可以自定义位置 SimilarWeb：网站流量来源和排名，通过SimilarWeb扩展程序查看真实的深入网站参与、流量来源和网站排名信息 SimpleUndoClose：这个简单的弹出可让您轻而易举的撤销关闭的标签！特别是我不小心关闭了页面，这个工具太有用了 vTabs：标签开太多之后，就用这个来快速的导航，可以设置在浏览器的左侧显示精简的标签页 Xpath Finder：帮助你在chrome devtools里查找Xpath匹配元素 下一代Tunnello VPN：快速，简单安全。解除任何网站，影片或应用程式封锁的最佳解决方案。绕过审查并更改您的位置 复制链接：是一款可以选择网页上的链接文本，直接复制其中包含的链接的chrome插件。在chrome中安装了复制链接插件以后，用户在选中一段网页上的链接文本以后，在右键菜单中就会出现复制链接的菜单，选择该菜单，用户就可以直接复制该链接包含的URL地址 网页截图:注释&amp;录屏：录屏，捕获整个页面或任何部分，矩形，圆形，箭头，线条和文字，模糊敏感信息，一键上传分享注释。支持PNG和链接 超级拖曳：附带6个最常用鼠标手势。 设计师助手：为设计师, 前端开发工程师提供帮助的工具箱, 目前包括取色工具和标尺 任何以Chrome插件形式存在的翻墙工具：多数的这类插件都是骗人的，这些插件会在前期提供一段时间的翻墙服务，一旦你成为年费会员，这些插件多数都开始停止服务，再也无法登录，过段时间又会换个名字换个插件出现，例如：红杏翻墙，时空隧道等 在用机器 PC1：MacBook Pro MJLQ2CH/A 15.6 PC2：微星（MSI）GE60 2QL-1047XCN 15.6 + 12G内存 + 128G固态硬盘 + i7四核八线程 手机：iPhone 7 耳机：森海塞尔 阅读器：Kindle Paperwhite 3 黑色 机械键盘：樱桃（Cherry）MX-BOARD 2.0 C 黑色红轴高帽键 鼠标1：微软无线便携蓝影4000鼠标，完全无需鼠标垫 鼠标2：罗技（Logitech）M545无线鼠标；全新升级，赛车造型，实用侧键，精准度高！ 鼠标3：罗技（Logitech）M570 火星轨迹球鼠标 Mac的一些优势 Mac的基本功能完善且贴心。区域截屏很好用，Spotlight搜索、启动程序非常方便，无需像Win额外安装软件 Mac下没有很多Win下多余的东西。几乎不用担心病毒，不用整理硬盘，不用分区，几乎没有死机，不用关机，基本没用过进程管理器。所有和使用无关的东西，都被藏起来，用户很容易学习和使用 Mac设置简单。拿网络设置说，Mac下很快就可以设置好（包括复杂的801.x安全认证），而且可以方便的切换多套配置（家里、办公室），在Win下却非常困难，需要专业人员完成 软件。Mac下的软件都很Mac，操作风格统一，简单好用，使用非常顺畅 系统是一个整体。所有的软件的工具栏都是统一的，UI、操作相似；所有软件下都可以检查字母拼写；所有浏览器的密码保存都是统一存在keychain中 Mac很少有更新，不像Win更新了也还是原样 无数的细节。比如复制是Command+c，用大拇指和其他手指配合 对于开发人员，结合了Windows、Linux的优点 右下角的弹窗战争，在Mac上，完全没遇到过右下角弹窗的事情。就连Mac通知中心的事情也是我可以方便的自己控制的 关于为啥Mac好，推荐看为什么Mac适合编程？ 其它 一个靠谱的项目经理 一个大胸的妹子 一个牛逼的人体工学座椅 一个超大的显示屏 一个性能吊炸天MacBook Pro 最后附上Mac OS X常用快捷键一览]]></content>
      <categories>
        <category>Skill</category>
      </categories>
      <tags>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度2016校园招聘之编程题解析-软件研发]]></title>
    <url>%2F2015%2F09%2F29%2F2016-campus-recruitment-of-baidu-programming-problem-resolution-software-development%2F</url>
    <content type="text"><![CDATA[比大小 解题思路：解此题需要使用到康托展开，康托展开的公式如下 $$X=a_n*(n-1)!+a_{n-1}*(n-2)!+\cdot\cdot\cdot+a_i*(i-1)!+\cdot\cdot\cdot+a_2*(2-1)!+a_1*(1-1)!$$ 公式看不懂没关系，下面以一个例子来讲解公式的使用！ 例如：有一个数组S=[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;]，它的其中之一个排列是S1=[&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;a&quot;]，现在欲把S1映射成X，需要怎么做呢？按如下步骤走起 $$X=a_4*3!+a_3*2!+a_2*1!+a_1*0!$$ 首先计算n，n等于数组S的长度，n=4 再来计算a4=”b”这个元素在数组[&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;a&quot;]中是第几大的元素。”a”是第0大的元素，”b”是第1大的元素，”c”是第2大的元素，”d”是第3大的元素，所以a4=1 同样a3=”c”这个元素在数组[&quot;c&quot;,&quot;d&quot;,&quot;a&quot;]中是第几大的元素。”a”是第0大的元素，”c”是第1大的元素，”d”是第2大的元素，所以a3=1 a2=”d”这个元素在数组[&quot;d&quot;,&quot;a&quot;]中是第几大的元素。”a”是第0大的元素，”d”是第1大的元素，所以a2=1 a1=”a”这个元素在数组[&quot;a&quot;]中是第几大的元素。”a”是第0大的元素，所以a1=0 所以X(S1)=1\*3!+1\*2!+1\*1!+0\*0!=9 注意所有的计算都是按照从0开始的，如果[“a”,”b”,”c”,”d”]算为第1个的话，那么将X(S1)+1即为最后的结果 Java算法实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import java.util.Iterator;import java.util.Scanner;import java.util.Set;import java.util.TreeSet;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/29 16:16 * &lt;/p&gt; * &lt;p&gt; * ClassName:Main * &lt;/p&gt; * &lt;p&gt; * Description:本题需要用到康托展开，其公式为 X=an*(n-1)!+an-1*(n-2)!+...+ai*(i-1)!+...+a2*1!+a1*0! * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class Main &#123; // 3 // abcdefghijkl // hgebkflacdji // gfkedhjblcia static int charLength = 12;//定义字符序列的长度 public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); while (scanner.hasNextInt()) &#123; int n = scanner.nextInt(); String lines[] = new String[n]; int res[] = new int[n];//存储结果的数组 for (int i = 0; i &lt; n; i++) &#123; lines[i] = scanner.next(); res[i] = calculate(lines[i]); &#125; for (int s : res) &#123; System.out.println(s); &#125; &#125; &#125; //计算某个字符序列的位次 private static int calculate(String line) &#123; Set&lt;Character&gt; s = new TreeSet&lt;Character&gt;(); for (char c : line.toCharArray()) &#123; s.add(c); &#125; //存储每一个字符在该序列中是第几大的元素，然后将其值存储到counts数组中 int counts[] = new int[s.size()]; char[] chars = line.toCharArray(); for (int i = 0; i &lt; chars.length; i++) &#123; Iterator&lt;Character&gt; iterator = s.iterator(); int temp = 0; Character next; while (iterator.hasNext()) &#123; next = iterator.next(); if (next == chars[i]) &#123; counts[i] = temp; s.remove(next); break; &#125; else &#123; temp++; &#125; &#125; &#125; int sum = 1; for (int i = 0; i &lt; counts.length; i++) &#123; sum = sum + counts[i] * factorial(charLength - i - 1); &#125; return sum; &#125; //计算阶乘的函数 private static int factorial(int n) &#123; if (n &gt; 1) &#123; return n * factorial(n - 1); &#125; else &#123; return 1; &#125; &#125;&#125; 拓展一下-求其逆过程 解题思路：使用康托逆展开，辗转相除得到的值为这个字符是第几大，这样取出对应位置的字符，然后利用后面的字符覆盖该字符即可，防止取到重复的字符。取模得到余数之后，重复上述过程。例如：已知S=[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;]，那么当输入10的时候，或者说X(S1)=9的时候能否推出S1=[&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;a&quot;]呢？ 由 $$X(S1)=a_4*3!+a_3*2!+a_2*1!+a_1*0!=9$$ 所以问题变成由9能否唯一地映射出一组a4、a3、a2、a1？首先如果不考虑ai的范围，那么有如下： $$1*3!+1*2!+1*1!+0*0!=9$$ $$0*3!+4*2!+1*1!+0*0!=9$$ $$0*3!+3*2!+3*1!+0*0!=9$$ $$0*3!+2*2!+5*1!+0*0!=9$$ ……，但是每一个ai其实是有取值范围的，首先要知道ai表示的含义，其代表在当前剩余的序列中ai是处于第几大的位置，那么满足0&lt;=ai&lt;=i，同时a1必然为0，因为最后始终剩余一个元素。所以上式中只有第一个满足条件，那么a4=1，a3=1，a2=1，a1=1。推导出S1=[&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;a&quot;]。 Java算法实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.util.Scanner;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/29 16:16 * &lt;/p&gt; * &lt;p&gt; * ClassName:Main * &lt;/p&gt; * &lt;p&gt; * Description:本题需要用到康托展开，其公式为 X=an*(n-1)!+an-1*(n-2)!+...+ai*(i-1)!+...+a2*1!+a1*0! * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class MainExpand &#123; static int charLength = 12;//定义字符序列的长度 public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); while (scanner.hasNextInt()) &#123; int n = scanner.nextInt(); int lines[] = new int[n]; String res[] = new String[n];//存储结果的数组 for (int i = 0; i &lt; n; i++) &#123; lines[i] = scanner.nextInt(); res[i] = calculate(lines[i] - 1); &#125; for (String s : res) &#123; System.out.println(s); &#125; &#125; &#125; //计算某个字符序列的位次 private static String calculate(int line) &#123; char alpha[] = &#123;'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l'&#125;; StringBuilder sb = new StringBuilder(); for (int i = charLength - 1; i &gt;= 0; i--) &#123; int temp = line / factorial(i); line = line % factorial(i); sb.append(String.valueOf(alpha[temp])); for (int j = temp; j &lt; alpha.length - 1; j++) &#123; alpha[j] = alpha[j + 1]; &#125; &#125; return sb.toString(); &#125; //计算阶乘的函数 private static int factorial(int n) &#123; if (n &gt; 1) &#123; return n * factorial(n - 1); &#125; else &#123; return 1; &#125; &#125;&#125; 判断字符串是否出现解题思路： 将字符串a存储在一个map集合中，以每个字符的ASCII码作为key，以其出现的次数作为value，记为aMap 遍历字符串b，对于b中的每一个字符，如果aMap的key中含有该字符的ASCII码，如果该key对应的value&gt;1，那么将value值减1 否则value=1的话，那么将该键值对从aMap中移除 在判断aMap的key是否包含b中的某个字符的时候，只要有一次不包含，那么就说明没有都出现 否则的话，表示b中的字符在a中都出现过 Java算法实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.HashMap;import java.util.Map;import java.util.Scanner;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/28 20:05 * &lt;/p&gt; * &lt;p&gt; * ClassName:test * &lt;/p&gt; * &lt;p&gt; * Description:TODO * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class Main &#123; public static void main(String[] args) &#123; //以某个字符的ASCII码作为key，以其出现的次数作为value Map&lt;Integer, Integer&gt; aMap = new HashMap&lt;Integer, Integer&gt;(); Scanner input = new Scanner(System.in); while (input.hasNextLine()) &#123; String a = input.nextLine(); String b = input.nextLine(); char[] chars = a.toCharArray(); for (char c : chars) &#123; if (aMap.keySet().contains((int) c)) &#123; int temp = aMap.get((int) c); aMap.put((int) c, (temp + 1)); &#125; else &#123; aMap.put((int) c, 1); &#125; &#125; char[] chars1 = b.toCharArray(); boolean flag = true; for (char c : chars1) &#123; if (aMap.keySet().contains((int) c)) &#123; int temp = aMap.get((int) c); if (temp == 1) &#123; //说明只有一个 aMap.remove((int) c); &#125; else &#123; //说明多过于一个 aMap.put((int) c, (temp - 1)); &#125; &#125; else &#123; flag = false; break; &#125; &#125; if (flag) &#123; System.out.println(1); &#125; else &#123; System.out.println(0); &#125; aMap.clear(); &#125; &#125;&#125; 组合概率解题思路：需要递推公式，然后用动态规划求解。 Java算法实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.text.DecimalFormat;import java.util.Scanner;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/28 20:30 * &lt;/p&gt; * &lt;p&gt; * ClassName:Test3 * &lt;/p&gt; * &lt;p&gt; * Description:动态规划求解 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class Main3 &#123; static DecimalFormat dec = new DecimalFormat("0.0000"); static double v[][];//表示取i个数时和为j的概率 public static void main(String[] args) &#123; Scanner input = new Scanner(System.in); while (input.hasNextInt()) &#123; int n = input.nextInt(); int a = input.nextInt(); int b = input.nextInt(); int x = input.nextInt(); v = new double[n + 1][x + 1]; double sum = b - a + 1; for (int i = a; i &lt;= b; i++) &#123; v[1][i] = 1.0 / sum;//取1个数和为i的概率 &#125; for (int i = 1; i &lt;= n; i++) &#123;//对n个数进行迭代 for (int j = a; j &lt;= b; j++) &#123;// for (int k = 1; k &lt;= x; k++) &#123; if (k &gt;= j) &#123;// print(v);// System.out.println(); v[i][k] = v[i][k] + v[i - 1][k - j] / sum; &#125; &#125; &#125; &#125; //输出取n个数和为x的概率 System.out.println(dec.format(v[n][x])); &#125; &#125; private static void print(double[][] v) &#123; for (int i = 0; i &lt; v.length; i++) &#123; for (int j = 0; j &lt; v[i].length; j++) &#123; System.out.print(v[i][j] + "\t"); &#125; System.out.println(); &#125; &#125;&#125; 参考文献[1] http://www.jeepshoe.org/416642954.htm[2] http://www.cnblogs.com/sunus/p/4536082.html 算法如有疏漏或不妥之处，还望不吝赐教！]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一种面向突发事件的文本语料自动标注方法]]></title>
    <url>%2F2015%2F09%2F26%2FAn-Automatic-Annotation-Method-for-Emergency-Text-Corpus%2F</url>
    <content type="text"><![CDATA[自序人生天地间，若白驹之过隙，忽然而已。无奈诸多俗务缠身，难得清闲。一直都是忙忙碌碌，然而碌碌无为，只顾埋头前行，偶有机会驻足回首，实乃吾之大幸。 因为我的父母是地地道道、兀兀穷年的农民，我这一路摸爬滚打全靠自己不断摸索总结。在人生的道路上，我一直很感谢能给我人生上面指导的人，而技术上面的指导却不是我迫切需要的，技术可以通过Google获得，然而人生的经验却只有过来人才能予以传授。不过我还是信奉我那句话，有变化就有机遇，一定要勇于拥抱变化。 我一直自认为是一个简简单单、没有心机、毫无城府、专心搬砖的人，所以坐下来细细思索，发现没有其它东西以飨读者，唯有一点点技术心得值得予以分享。下面就介绍一下在研究生期间所做的一点点工作。 CEC简介CEC语料库Chinese Emergency Corpus简称CEC，中文名称为中文突发事件语料库，是根据上海大学刘宗田教授所提出的《面向事件的本体研究》由人工标注所构建完成，目前整个语料库在重新进行了编码转换、XML格式化、错误修复之后托管在GitHub上了，地址点我。 事件的定义是什么事件指在某个特定的时间和环境下发生的、由若干角色参与、表现出若干动作特征的一件事情。形式上，事件可表示为e，定义为一个六元组：e ::= (A, O, T, P, S, L)其中，事件六元组中的元素称为事件要素，分别表示动作（Action）、对象（Object）、时间（Time）、地点（Place）、状态（Status）、语言表现（Language Expressions）。 A(动作)：A表示事件所包含的动作或动作序列的集合，在文本中，动作通常是作为识别一个事件的触发词 O(对象)：O表示一个事件中的对象集合，包括事件中所有的参与者和涉及到的对象。对象可分别是动作的施动者(主体)和受动者(客体) T(时间)：T表示事件发生的时刻或时间段，时间分为绝对时间和相对时间，两类时间都可以通过计算转换成形如[t1,t2]的序偶表示，以此描述事件的开始、发展和结束时间，当开始时间和结束时间一样时，表示事件发生在瞬间 P(地点)：表示事件发生的地点；例如：在小池塘里游泳, 场所：小池塘, 场所特征：水中 S(状态)：表示事件发生过程中对象的状态集合，由事件发生的前置条件、后置结果集合组成。前置条件指为进行该事件, 各要素应当或可能满足的约束条件, 它们可以是事件发生的触发条件;中间断言指事件发生过程的中间状态各要素满足的条件;事件发生后,事件各要素将引起变化或者各要素状态的变迁, 这些变化和变迁后的结果, 将成为事件的后置条件。 L(语言表现)：事件的语言表现规律, 包括核心词集合、核心词表现、核心词搭配等。核心词是事件在句子中常用的标志性词汇。核心词表现则为在句子中各要素的表示与核心词之间的位置关系。核心词搭配是指核心词与其他词汇的固有的搭配。可以为事件附上不同语言种类的表现, 例如中文、英文、法文等等。 在事件的六个要素中，前五个要素是事件的内在要素。 CEC如何构建事件本体是以“事件”为认知单元，研究事件的组成以及事件之间的关系，并对事件进行归纳和概括，形成事件类，进而构建事件本体模型。研究本体，必然要先构建语料库，所以在互联网上选取了突发事件语料来进行语料的事件标注，突发事件的分类体系，包括三个层次：一级4个大类（自然灾害类N、事故灾难类A、公共卫生事件P、社会安全事件S），二级33个子类，三级94个小类。我们标注的CEC语料库主要包括五类：地震、火灾、交通事故、恐怖袭击、食物中毒。合计332篇。 CEC标注标签规范 CEC标注样例123456789101112131415161718192021222324&lt;Body&gt;&lt;Title&gt;成都网友称震感强烈 女同事当即哭泣&lt;/Title&gt;&lt;ReportTime type="absTime"&gt;2008年05月12日16:15&lt;/ReportTime&gt;&lt;Content&gt; &lt;Paragraph&gt; &lt;Sentence&gt; &lt;Event eid="e1"&gt; &lt;Time type="relTime" tid="t1"&gt;5月12日14时28分&lt;/Time&gt;， &lt;Location lid="l1"&gt;四川&lt;/Location&gt;发生7.8级 &lt;Denoter type="emergency" did="d1"&gt;地震&lt;/Denoter&gt;。 &lt;/Event&gt; &lt;/Sentence&gt; &lt;Sentence&gt; &lt;Event eid="e2"&gt; &lt;Time type="relTime" tid="t2"&gt;15时50分&lt;/Time&gt;，新民网 &lt;Participant sid="s1"&gt;记者&lt;/Participant&gt;网上 &lt;Denoter type="action" did="d2"&gt;连线&lt;/Denoter&gt;成都网友 &lt;Participant oid="o2"&gt;姚先生&lt;/Participant&gt;。 &lt;/Event&gt; &lt;/Sentence&gt; &lt;/Paragraph&gt;&lt;/Content&gt;&lt;eRelation relType="Causal" cause_eid="e1" effect_eid="e5" /&gt;&lt;/Body&gt; LTP（Language Technology Platform）平台云简介语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心历时十年研制的一整套开放中文自然语言处理系统。LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富、高效、高精度的中文自然语言处理模块（包括词法、句法、语义等5项中文处理核心技术，在多次国内外技术评测中获得优异成绩，特别是获得CoNLL 2009国际句法和语义分析联合评测的第一名），应用程序接口，可视化工具，以及能够以网络服务使用的语言技术云。学术版LTP已共享给500多家研究机构免费使用，百度、腾讯、华为、金山等企业付费使用LTP商业版本。2010年，LTP荣获行业最高奖–“钱伟长中文信息处理科学技术一等奖”。 语言技术平台的总体结构如下所示 若想进一步了解，请点我进入LTP官网进行查看。 我的工作-用机器进行标注在我们构建CEC的过程中全部由人工进行标注，先用网络爬虫从网络上爬取突发事件类的新闻报道，之后人工对事件的要素进行识别，然后添加对应的标签以及属性等。此过程费时费力，而且由于不同的人有不同的判断，易造成标注过程中的标准不一致，进而为整个语料库带来一些歧义性的标注内容，所以应该由机器进行初步的标注，人工在机器标注的基础之上进行深入细致的分析，修正等。 效果图 如何实现挖掘事件要素规则借助于人工构建的CEC语料库，使用LTP对CEC语料库中的原文本（未标注的文本）进行词性标注、命名实体识别、语义角色标注等一系列基础性工作，之后与CEC语料库进行对比，抽取出事件的每个要素所对应的词性、语义角色等，这样可以获取到大量的有序的词性集合以及语义角色集合，既然词性和语义角色标注都是有序的，那么很自然的想到利用序列模式挖掘算法对词性集合、语义角色集合进行频繁项集挖掘，我采用的是韩家炜于2004年提出的PrefixSpan序列模式挖掘算法，有兴趣的可以去读读他的论文，地址点我。 构建触发词表根据触发词类型的不同，分别抽取出每一种类型的触发词构建成触发词表。在CEC语料库中触发词共有七种，分别是：突发事件(emergency) 、移动事件(movement)、声明类事件(statement)、原子动作事件(action) 、操作事件(operation) 、状态改变事件(stateChange)、感知事件(perception)。 使用同义词词林（扩展版）扩充触发词表首先要知道为什么需要扩充触发词表？既然触发词表由CEC中抽取所得，限于CEC规模有限，触发词表的数量亦必然有限，那么需要使用同义词词林对其进行扩充，在扩充的过程中，并不是将与某个触发词是同义词的所有义项全部扩充进触发词表，而是有一定的阈值，对于满足一定阈值的同义词项才认为均是合法的，予以扩充。 用户自定义事件要素词典同样的，我们不禁要问一下，为什么需要用户自定义要素词典呢？同样是限于CEC规模，挖掘的要素识别规则必然有限，亦不能涵盖大量未知的情况，所以需要人工定义事件要素词典，并且该词典可以采用倒序排列，按照文本串的长度由长到短排序，这样机器识别要素的时候，可以防止过分切分事件要素，类似于切词里面的正向最大切分。 本算法有一定的领域局限性，目前仅限于突发事件类的新闻报道，且准确率有待进一步提高，此为后话。有关实现的具体细节，请参考我的论文：《一种面向突发事件的文本语料自动标注方法》]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蘑菇街2016校园招聘之编程题解析-技术类]]></title>
    <url>%2F2015%2F09%2F23%2F2016-campus-recruitment-of-mushroom-street-programming-problem-resolution-technology%2F</url>
    <content type="text"><![CDATA[回文串 解题思路：既然通过添加一个字母可以变为回文串，那么通过删除与添加的字母相对位置的字符，应该亦为回文串。 例如： ‘abcb’在末尾添加’a’ –&gt; ‘abcba’为回文串‘abcb’删除与想要添加的字符’a’对应位置的字符 –&gt; ‘bcb’亦为回文串 ‘aabbaab’在头部添加’b’ –&gt; ‘baabbaab’为回文串‘aabbaab’删除与想要添加的字符’b’对应位置的字符 –&gt; ‘aabbaa’亦为回文串 Java算法实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.util.Scanner;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/23 13:35 * &lt;/p&gt; * &lt;p&gt; * ClassName:Main * &lt;/p&gt; * &lt;p/&gt; * Description:给定一个串，通过添加一个字母将其变成“回文串” * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class Main &#123; final String Y = "YES"; final String N = "NO"; public String isPalindrome(String input) &#123; if (input == null || "".equals(input)) &#123; return Y; &#125; int length = input.length(); //题目说明不超过10个字符，那么超过的话，直接返回NO if (length &gt; 10) &#123; return N; &#125; StringBuilder sb = new StringBuilder(input); for (int i = 0; i &lt; length; i++) &#123; sb.deleteCharAt(i); String temp = sb.toString(); if (sb.reverse().toString().equals(temp)) &#123; return Y; &#125; else &#123; sb = new StringBuilder(input); continue; &#125; &#125; return N; &#125; public static void main(String[] args) &#123; Scanner cin = new Scanner(System.in); String input; while (cin.hasNext()) &#123; input = cin.next(); System.out.println(new Main().isPalindrome(input)); &#125; &#125;&#125; 聊天 解题思路： 小蘑的时间假设为[a，b]，小菇的时间假设是[c+t，d+t]，小菇起床的时间是t∈[l，r] 那么当&quot;a &lt; b &lt; (c+t) &lt; (d+t)&quot;或者&quot;(c+t) &lt; (d+t) &lt; a &lt; b&quot;的情况时，小蘑和小菇无法聊天，由题目条件已知&quot;a &lt; b&quot;和&quot;c &lt; d&quot;，那么推出&quot;(c+t) &lt; (d+t)&quot; 所以仅仅当&quot;b &lt; (c+t)&quot;或者&quot;(d+t) &lt; a&quot;时无法聊天，其余情况都是可以聊天的 Java算法实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import java.util.Scanner;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/23 13:35 * &lt;/p&gt; * &lt;p&gt; * ClassName:Main2 * &lt;/p&gt; * &lt;p/&gt; * Description:求小菇合适的起床时间，有一个非常奇葩的问题，在牛客网上测试，如果main中写 * &lt;pre&gt; * Scanner cin = new Scanner(System.in); * while (cin.hasNextInt()) &#123;&#125;//正确 * &lt;/pre&gt; * 写 * &lt;pre&gt; * while(true)&#123;&#125;//报错 * &lt;/pre&gt; * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class Main2 &#123; private static void isLegal(int num, boolean flag) &#123;// flag为true标识判断[1,50]，flag为false标识判断[0,1000] if (flag) &#123; if (!(1 &lt;= num &amp;&amp; num &lt;= 50)) &#123;// System.out.println("数据非法"); System.exit(0); &#125; &#125; else &#123; if (!(0 &lt;= num &amp;&amp; num &lt;= 1000)) &#123;// System.out.println("数据非法"); System.exit(0); &#125; &#125; &#125; public static void main(String[] args) &#123; Scanner cin = new Scanner(System.in); while (cin.hasNextInt()) &#123; int p = 0, q = 0, l = 0, r = 0; p = cin.nextInt(); isLegal(p, true); q = cin.nextInt(); isLegal(q, true); l = cin.nextInt(); isLegal(l, false); r = cin.nextInt(); isLegal(r, false); int[] time_A_B = new int[p * 2];//标识小蘑的时间 int[] time_C_D = new int[q * 2];//标识小菇的时间 for (int i = 0; i &lt; time_A_B.length; i++) &#123;// 接收p行的数据，每一行数据是一个时间对 int temp = cin.nextInt(); isLegal(temp, false); time_A_B[i] = temp; &#125; for (int i = 0; i &lt; time_C_D.length; i++) &#123; int temp = cin.nextInt(); isLegal(temp, false); time_C_D[i] = temp; &#125; int count = 0;//标识小菇能有多少个合适的起床时间 begin: for (int t = l; t &lt;= r; t++) &#123; for (int i = 0; i &lt; time_A_B.length; i += 2) &#123; for (int j = 0; j &lt; time_C_D.length; j += 2) &#123; if (!(time_C_D[j] + t &gt; time_A_B[i + 1] || time_C_D[j + 1] + t &lt; time_A_B[i])) &#123; count++; continue begin; &#125; &#125; &#125; &#125; System.out.println(count); &#125; &#125;&#125; 搬圆桌 解题思路： length = sqrt((x1-x2)^2+(y1-y2)^2)先计算两个圆心点之间的距离 参考上图，以(2 0 0 0 4)作为输入数据进行说明，当两个圆心点之间的距离lenght&lt;(2*r+1)的时候，我们是不能够沿着两个圆心之间的连线进行移动的，而由两点之间直线最短，可知，沿两圆心的连线进行移动是最短的距离，换句话说，旋转一次移动的最长距离就是2*r，而在旋转之前需要先移动一步，所以阈值设为2*r+1 那么当lenght&lt;(2*r+1)的时候，我们该如何进行旋转呢？正确的做法是以(x1,y1)为圆心r为半径作圆，与以(x,y)为圆心的圆的交叉点就是支点，固定此点旋转即可 根据以上的分析，再以(2 0 0 0 5)作为输入数据进行说明，当length&gt;=(2*r+1)，那么length中有几个(2*r+1)，我们就需要走几步，如果相除不为整数，那么加1即可 Java算法实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.Scanner;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/9/23 16:54 * &lt;/p&gt; * &lt;p&gt; * ClassName:Main3 * &lt;/p&gt; * &lt;p/&gt; * Description:求圆桌移动到目标位置的步数 * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */public class Main3 &#123; public static void main(String[] args) &#123; Scanner cin = new Scanner(System.in); while (cin.hasNextInt()) &#123; int r = cin.nextInt(); if (r &lt; 1 || r &gt; 100000) &#123; System.exit(0); &#125; int x = cin.nextInt(); int y = cin.nextInt(); int x1 = cin.nextInt(); int y1 = cin.nextInt(); if (x &lt; -100000 || x &gt; 100000) &#123; System.exit(0); &#125; if (y &lt; -100000 || y &gt; 100000) &#123; System.exit(0); &#125; if (x1 &lt; -100000 || x1 &gt; 100000) &#123; System.exit(0); &#125; if (y1 &lt; -100000 || y1 &gt; 100000) &#123; System.exit(0); &#125; double length = Math.sqrt(Math.pow(x - x1, 2) + Math.pow(y - y1, 2)); int count; //向上取整之后强转为int型即可 count = (int) Math.ceil(length / (2 * r + 1)); System.out.println(count); &#125; &#125;&#125; 本文代码均在牛客网在线OJ测试通过。算法如有疏漏或不妥之处，还望不吝赐教！]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Selenium模拟用户登录支付宝账户查询交易详情]]></title>
    <url>%2F2015%2F08%2F29%2FSelenium-simulated-user-login-alipay-account-query-transaction-details%2F</url>
    <content type="text"><![CDATA[Selenium简介Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE、Mozilla Firefox、Chrome等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成.Net、Java、Perl等不同语言的测试脚本。Selenium是ThoughtWorks专门为Web应用程序编写的一个验收测试工具。 Selenium的优势据Selenium主页所说，与其他测试工具相比，使用Selenium的最大好处是：Selenium测试直接在浏览器中运行，就像真实用户所做的一样。Selenium测试可以在Windows、Linux和Macintosh上的Internet Explorer、Mozilla和Firefox中运行。其他测试工具都不能覆盖如此多的平台。使用Selenium和在浏览器中运行测试还有很多其他好处。下面是主要的两大好处： 通过编写模仿用户操作的Selenium测试脚本，可以从终端用户的角度来测试应用程序 通过在不同浏览器中运行测试，更容易发现浏览器的不兼容性 Selenium的核心，也称browser bot，是用JavaScript编写的。这使得测试脚本可以在受支持的浏览器中运行。browser bot负责执行从测试脚本接收到的命令，测试脚本要么是用HTML的表布局编写的，要么是使用一种受支持的编程语言编写的。 Selenium-WebDriverSelenium-WebDriver支持如下浏览器，在所有支持这些浏览器的操作系统中能都运行良好。 Google Chrome 12.0.712.0+ Internet Explorer 6, 7, 8, 9 - 32 and 64-bit where applicable Firefox 3.0, 3.5, 3.6, 4.0, 5.0, 6, 7 Opera 11.5+ HtmlUnit 2.9 Android – 2.3+ for phones and tablets (devices &amp; emulators) iOS 3+ for phones (devices &amp; emulators) and 3.2+ for tablets (devices &amp; emulators) Selenium开发环境如欲使用Java语言，那么JDK的环境是必备的。使用Selenium来驱动浏览器，那么必须要有浏览器驱动包，同样还需要Selenium的Jar包，点击下载即可。 Selenium启动浏览器启动Firefox1234//后一个参数是Firefox的安装路径System.setProperty("webdriver.firefox.bin", "C:/Program Files (x86)/Mozilla Firefox/firefox.exe");WebDriver driver = new FirefoxDriver();driver.get("www.baidu.com"); 启动Chrome1234//后一个参数是Chrome浏览器的驱动，需要下载System.setProperty("webdriver.chrome.driver", "D:/chromedriver.exe");WebDriver driver = new ChromeDriver();driver.get("www.baidu.com"); 启动IE作为开发人员，建议您应该远离IE，虽然我不太愿意提供IE的启动方式给您！123System.setProperty("webdriver.ie.driver", "D:/IEDriverServer.exe");WebDriver driver = new InternetExplorerDriver();driver.get("www.baidu.com"); 常见问题的处理方法 How to select any element from the web element with “display: none” attribute using Selenium ?特别是在处理因为&lt;select&gt;标签中带有style=&#39;display:none&#39;而无法设置option的情况时很有效！ 方法： Use execute_script() to set the display property of that element and then use the Selenium Select for selecting a required value. 12JavascriptExecutor js = (JavascriptExecutor) driver;//将driver对象强转成JavascriptExecutorjs.executeScript("document.getElementById('J-select-range').style.display='inline';");//修改display的值，也可以修改为display='list-item' Element is not currently visible and may not be manipulated 一般这种错误是因为父元素有style=&#39;display:none&#39;属性或者是页面还没有加载完全，最简单的就是使用Thread.sleep(1000);休息一秒之后再操作。当然也可以使用until方法直到其出现再对其进行操作。 1234WebDriverWait wait = new WebDriverWait(driver, 300);WebElement selectElement = wait.until(ExpectedConditions.visibilityOfElementLocated(By.id("formLevel:levels_input")));Select select = new Select(selectElement);select.selectByVisibleText("SECURITY"); 模拟用户登录支付宝查询交易信息的程序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import org.openqa.selenium.By;import org.openqa.selenium.JavascriptExecutor;import org.openqa.selenium.WebDriver;import org.openqa.selenium.WebElement;import org.openqa.selenium.chrome.ChromeDriver;import org.openqa.selenium.support.ui.Select;import java.util.List;/** * 注意此流程适应于旧版支付宝，新版支付宝页面未测试 * * @author Wang Xu 2015/8/29 * @version V1.0.0 * @since V1.0.0 */public class SeleniumAlipay &#123; private static WebDriver driver; // 定义自己的休眠方法，精简代码量 private static void sleep(long time) &#123; try &#123; Thread.sleep(time); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); System.out.println(e.getMessage()); &#125; &#125; // 登录操作，负责将界面跳转到交易记录界面 private static void login() &#123; // 启动火狐浏览器// System.setProperty("webdriver.firefox.bin", "C:/Program Files (x86)/Mozilla Firefox/firefox.exe");// WebDriver driver = new FirefoxDriver();// 启动Chrome浏览器 System.setProperty("webdriver.chrome.driver", "D:/chromedriver.exe"); driver = new ChromeDriver();// 获取登录页面 driver.get("https://auth.alipay.com/login/index.htm?goto=https%3A%2F%2Fwww.alipay.com%2F");// 获取用户名输入框 driver.findElement(By.id("J-input-user")).clear(); driver.findElement(By.id("J-input-user")).sendKeys("支付宝账户");// 休息500ms，否则，速度太快，会将密码内容填充到用户名输入框中 sleep(500);// 获取密码输入框 driver.findElement(By.id("password_rsainput")).clear(); driver.findElement(By.id("password_rsainput")).sendKeys("密码");// 休息8秒等待用户输入验证码 sleep(8000);// 当前URL 0 ：https://authsu18.alipay.com/login/certCheck.htm?goto=https%3A%2F%2Fwww.alipay.com%2F System.out.println("当前URL 0 ：" + driver.getCurrentUrl()); driver.get("https://www.alipay.com/");// 点击个人用户登录 driver.findElement(By.className("personal-login")).click();// 当前URL 1 ：https://www.alipay.com/ System.out.println("当前URL 1 ：" + driver.getCurrentUrl()); sleep(2000); WebElement myAlipay = driver.findElement(By.className("am-button-innerNav,button-myalipay")); System.out.println("myAlipay isSelected ：" + myAlipay.isSelected());//false System.out.println("myAlipay isEnabled ：" + myAlipay.isEnabled());//true System.out.println("myAlipay isDisplayed ：" + myAlipay.isDisplayed());//true// 点击进入我的支付宝按钮 driver.findElement(By.className("am-button-innerNav,button-myalipay")).click();// 当前URL 2 ：https://my.alipay.com/portal/i.htm System.out.println("当前URL 2 ：" + driver.getCurrentUrl());// boolean selected1 = driver.findElement(By.className("fn-ml10")).isSelected();// System.out.println("收支明细是否选中：" + selected1);// org.openqa.selenium.NoSuchElementException: no such keyword: Element was not in a form, so could not submit.// driver.findElement(By.className("fn-ml10")).submit();//跳转收支明细// Exception in thread "main" org.openqa.selenium.WebDriverException: unknown error: Element is not clickable at point// driver.findElement(By.xpath("//*[@id=\"J-trend-consume\"]/div/div[1]/div/a[1]")).click();// driver.findElement(By.className("fn-ml10")).click();// 无反应，看样子一直不会让你点的了// WebDriverWait webDriverWait = new WebDriverWait(driver, 3);// webDriverWait.until(ExpectedConditions.elementToBeClickable(By.xpath("//*[@id=\"J-trend-consume\"]/div/div[1]/div/a[1]")));// 跳转到收支明细界面// driver.get("https://xlab.alipay.com/consume/record/items.htm"); //跳转到交易记录界面 driver.get("https://consumeprod.alipay.com/record/index.htm"); String currentUrl = driver.getCurrentUrl();// 当前URL 3 ：https://consumeprod.alipay.com/record/advanced.htm System.out.println("当前URL 3 ：" + currentUrl); sleep(1000); &#125; public static void main(String[] args) &#123; login(); String oppositeUser = getOppositeUser("20150826110500100010740029003925"); System.out.println("交易方对方信息：" + oppositeUser); oppositeUser = getOppositeUser("20150720110500100010740025980311"); System.out.println("交易方对方信息：" + oppositeUser); oppositeUser = getOppositeUser("2015081521001004740064396260"); System.out.println("交易方对方信息：" + oppositeUser); &#125; // 获取交易对方信息 private static String getOppositeUser(String transactionNo) &#123; //获取关键字对应的下拉框 WebElement keywordInput = driver.findElement(By.id("J-keyword")); keywordInput.clear(); keywordInput.sendKeys(transactionNo); WebElement keywordSelect = driver.findElement(By.id("keyword")); List&lt;WebElement&gt; options = keywordSelect.findElements(By.tagName("option")); //until方法表示直到可点再点// WebElement selectElement = wait.until(ExpectedConditions// .visibilityOfElementLocated(By.id("keyword")));// 需要执行JavaScript语句，所以强转driver JavascriptExecutor js = (JavascriptExecutor) driver;// 也可以这么用setAttribute("style",""); js.executeScript("document.getElementById('keyword').style.display='list-item';"); js.executeScript("document.getElementById('keyword').removeAttribute('smartracker');"); js.executeScript("document.getElementById('keyword').options[1].selected = true;"); js.executeScript("document.getElementById('J-select-range').style.display='list-item';");// 设置交易时间选项 Select selectTime = new Select(driver.findElement(By.id("J-select-range"))); selectTime.selectByIndex(3);//选中的是最近三个月 System.out.println("selectTime.isMultiple() : " + selectTime.isMultiple());// 设置关键字选项 Select selectKeyword = new Select(driver.findElement(By.id("keyword"))); // selectKeyword.selectByValue("bizInNo");//此处的value填写&lt;option&gt;标签中的value值 selectKeyword.selectByIndex(1);//选中的是交易号 System.out.println("selectKeyword.isMultiple() : " + selectKeyword.isMultiple()); WebElement queryButton = driver.findElement(By.id("J-set-query-form"));//拿到搜索按钮// 点击搜索按钮 queryButton.submit(); WebElement tr = driver.findElement(By.id("J-item-1"));//先获取tr WebElement td = tr.findElement(By.xpath("//*[@id=\"J-item-1\"]/td[5]/p[1]")); return td.getText(); &#125;&#125; 效果图 参考文献[1] http://baike.baidu.com/subview/478050/6464537.htm[2] http://blog.csdn.net/wanglha/article/details/39755613[3] http://webdriver.blog.51cto.com/10517592/1673920[4] http://blog.csdn.net/pf20050904/article/details/20052485[5] http://www.open-open.com/doc/view/9f52277e5d9c4dd3b851ae54011e733a]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于同义词词林扩展版的词语相似度计算]]></title>
    <url>%2F2015%2F08%2F04%2FBased-on-the-extended-version-of-synonyms-Cilin-word-similarity-computing%2F</url>
    <content type="text"><![CDATA[词语相似度计算词义相似度计算在很多领域中都有广泛的应用，例如信息检索、信息抽取、文本分类、词义排歧、基于实例的机器翻译等等。国内目前主要是使用知网和同义词词林来进行词语的相似度计算。 本文主要是根据《基于同义词词林的词语相似度计算方法–田久乐》论文中所提出的分层算法实现相似度计算，程序采用Java语言编写。 同义词词林扩展版《同义词词林》是梅家驹等人于1983年编纂而成，这本词典中不仅包括了一个词语的同义词，也包含了一定数量的同类词，即广义的相关词。《同义词词林扩展版》是由哈尔滨工业大学信息检索实验室所重新修订所得。该版收录词语近7万条，全部按意义进行编排，是一部同义类词典。 同义词词林按照树状的层次结构把所有收录的词条组织到一起，把词汇分成大、中、小三类，大类有12个，中类有97个，小类有1400个。每个小类里都有很多的词，这些词又根据词义的远近和相关性分成了若干个词群（段落）。每个段落中的词语又进一步分成了若干个行，同一行的词语要么词义相同，要么词义有很强的相关性。 《同义词词林》提供了5层编码，第1级用大写英文字母表示；第2级用小写英文字母表示；第3级用二位十进制整数表示；第4级用大写英文字母表示；第5级用二位十进制整数表示。例如：Cb30A01= 这里 这边 此地 此间 此处 此Cb30A02# 该地 该镇 该乡 该站 该区 该市 该村Cb30A03@ 这方 分层及编码表如下所示 由于第5级有的行是同义词，有的行是相关词，有的行只有一个词，分类结果需要特别说明，可以分出具体的3种情况。使用特殊符号对这3种情况进行区别对待，所以第8位的标记有3种，分别是“=”代表“相等”、“同义”；“#”代表“不等”、“同类”，属于相关词语；“@”代表“自我封闭”、“独立”，它在词典中既没有同义词，也没有相关词。 在对文本内容进行相似度计算中，采用该论文中给出的计算公式，两个义项的相似度用Sim表示若两个义项不在同一棵树上，Sim(A,B)=f若两个义项在同一棵树上：若在第2层分支，系数为a Sim(A,B)=1*a*cos*(n*π/180)((n-k+1)/n)若在第3层分支，系数为b Sim(A,B)=1*1*b*cos*(n*π/180)((n-k+1)/n)若在第4层分支，系数为c Sim(A,B)=1*1*1*c×cos*(n*π/180)((n-k+1)/n)若在第5层分支，系数为d Sim(A,B)=1*1*1*1*d*cos*(n*π/180)((n-k+1)/n) 当编码相同，而只有末尾是“#”时，那么认为其相似度为e。例如Ad02B04# 非洲人 亚洲人 则其相似度为e。 其中n是分支层的节点分支总数，k是两个分支间的距离。如：人 Aa01A01= 和 少儿 Ab04B01=以A开头的子分支只有14个，分别是Aa...，Ab...，Ac... ——— An...，而不是以A开头的所有结点的个数，所以n=14；在第2层分支上，人的编码是a，少儿的编码是b，a和b之间差1，所以k=1。 该文献中给出的参数值为a=0.65，b=0.8，c=0.9，d=0.96，e=0.5，f=0.1。 Java代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371package cn.codepub.algorithms.similarity.cilin;import com.google.common.base.Preconditions;import lombok.extern.log4j.Log4j2;import org.apache.commons.io.IOUtils;import org.apache.commons.lang3.StringUtils;import org.junit.Test;import java.io.IOException;import java.io.InputStream;import java.util.*;import static java.lang.Math.PI;import static java.lang.Math.cos;/** * &lt;p&gt; * Created with IntelliJ IDEA. 2015/8/2 21:54 * &lt;/p&gt; * &lt;p&gt; * ClassName:WordSimilarity 同义词词林扩展版计算词语相似度 * &lt;/p&gt; * &lt;p&gt; * Description:&lt;br/&gt; * "=" 代表 相等 同义 &lt;br/&gt; * "#" 代表 不等 同类 属于相关词语 &lt;br/&gt; * "@" 代表 自我封闭 独立 它在词典中既没有同义词, 也没有相关词 &lt;br/&gt; * &lt;/P&gt; * * @author Wang Xu * @version V1.0.0 * @since V1.0.0 */@Log4j2//注意使用Log4j2的注解，那么在pom中必须引入2.x版本的log4j，如果使用Log4j注解，pom中引入1.x版本的log4j//相应的配置文件也要一致，2.x版本配置文件为log4j2.xml，1.x版本配置文件为log4j.xmlpublic class WordSimilarity &#123; /** * when we use Lombok's Annotation, such as @Log4j * * @Log4j &lt;br/&gt; * public class LogExample &#123; * &#125; * &lt;p&gt; * will generate: * public class LogExample &#123; * private static final org.apache.logging.log4j.Logger log = org.apache.logging.log4j.Logger.getLogger(LogExample.class); * &#125; * &lt;/p&gt; */ //定义一些常数先 private static final double a = 0.65; private static final double b = 0.8; private static final double c = 0.9; private static final double d = 0.96; private static final double e = 0.5; private static final double f = 0.1; private static final double degrees = 180; //存放的是以词为key，以该词的编码为values的List集合，其中一个词可能会有多个编码 private static Map&lt;String, ArrayList&lt;String&gt;&gt; wordsEncode = new HashMap&lt;String, ArrayList&lt;String&gt;&gt;(); //存放的是以编码为key，以该编码多对应的词为values的List集合，其中一个编码可能会有多个词 private static Map&lt;String, ArrayList&lt;String&gt;&gt; encodeWords = new HashMap&lt;String, ArrayList&lt;String&gt;&gt;(); /** * 读取同义词词林并将其注入wordsEncode和encodeWords */ private static void readCiLin() &#123; InputStream input = WordSimilarity.class.getClass().getResourceAsStream("/cilin.txt"); List&lt;String&gt; contents = null; try &#123; contents = IOUtils.readLines(input); for (String content : contents) &#123; content = Preconditions.checkNotNull(content); String[] strsArr = content.split(" "); String[] strs = Preconditions.checkNotNull(strsArr); String encode = null; int length = strs.length; if (length &gt; 1) &#123; encode = strs[0];//获取编码 &#125; ArrayList&lt;String&gt; encodeWords_values = new ArrayList&lt;String&gt;(); for (int i = 1; i &lt; length; i++) &#123; encodeWords_values.add(strs[i]); &#125; encodeWords.put(encode, encodeWords_values);//以编码为key，其后所有值为value for (int i = 1; i &lt; length; i++) &#123; String key = strs[i]; if (wordsEncode.containsKey(strs[i])) &#123; ArrayList&lt;String&gt; values = wordsEncode.get(key); values.add(encode); //重新放置回去 wordsEncode.put(key, values);//以某个value为key，其可能的所有编码为value &#125; else &#123; ArrayList&lt;String&gt; temp = new ArrayList&lt;String&gt;(); temp.add(encode); wordsEncode.put(key, temp); &#125; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); System.err.println("load dictionary failed！"); log.error(e.getMessage()); &#125; &#125; /** * 对外暴露的接口，返回两个词的相似度的计算结果 * * @param word1 * @param word2 * @return 相似度值 */ public static double getSimilarity(String word1, String word2) &#123; //在计算时候再加载，实现懒加载 readCiLin(); //如果比较词没有出现在同义词词林中，则相似度为0 if (!wordsEncode.containsKey(word1) || !wordsEncode.containsKey(word2)) &#123; return 0; &#125; //获取第一个词的编码 ArrayList&lt;String&gt; encode1 = getEncode(word1); //获取第二个词的编码 ArrayList&lt;String&gt; encode2 = getEncode(word2); double maxValue = 0;//最终的计算结果值，取所有相似度里面结果最大的那个 for (String e1 : encode1) &#123; for (String e2 : encode2) &#123; log.info(e1); log.info(e2); String commonStr = getCommonStr(e1, e2); int length = StringUtils.length(commonStr); double k = getK(e1, e2); double n = getN(commonStr); log.info("k--&gt;" + k); log.info("n--&gt;" + n); log.info("encode length--&gt;" + length); double res = 0; //如果有一个以“@”那么表示自我封闭，肯定不在一棵树上，直接返回f if (e1.endsWith("@") || e2.endsWith("@") || 0 == length) &#123; if (f &gt; maxValue) &#123; maxValue = f; &#125; continue; &#125; if (1 == length) &#123; //说明在第二层上计算 res = a * cos(n * PI / degrees) * ((n - k + 1) / n); &#125; else if (2 == length) &#123; //说明在第三层上计算 res = b * cos(n * PI / degrees) * ((n - k + 1) / n); &#125; else if (4 == length) &#123; //说明在第四层上计算 res = c * cos(n * PI / degrees) * ((n - k + 1) / n); &#125; else if (5 == length) &#123; //说明在第五层上计算 res = d * cos(n * PI / degrees) * ((n - k + 1) / n); &#125; else &#123; //注意不存在前面七个字符相同，而结尾不同的情况，所以这个分支一定是8个字符都相同，那么只需比较结尾即可 if (e1.endsWith("=") &amp;&amp; e2.endsWith("=")) &#123; //说明两个完全相同 res = 1; &#125; else if (e1.endsWith("#") &amp;&amp; e2.endsWith("#")) &#123; //只有结尾不同，说明结尾是“#” res = e; &#125; &#125; log.info("res: " + res); if (res &gt; maxValue) &#123; maxValue = res; &#125; &#125; &#125; return maxValue; &#125; /** * 判断一个词在同义词词林中是否是自我封闭的，是否是独立的 * * @param source * @return */ private boolean isIndependent(String source) &#123; Iterator&lt;String&gt; iter = wordsEncode.keySet().iterator(); while (iter.hasNext()) &#123; String key = iter.next(); if (StringUtils.equalsIgnoreCase(key, source)) &#123; ArrayList&lt;String&gt; values = wordsEncode.get(key); for (String value : values) &#123; if (value.endsWith("@")) &#123; return true; &#125; &#125; &#125; &#125; return false; &#125; /** * 根据word的内容，返回其对应的编码 * * @param word * @return */ protected static ArrayList&lt;String&gt; getEncode(String word) &#123; return wordsEncode.get(word); &#125; /** * 计算N的值，N表示所在分支层分支数，如：人 Aa01A01= 和 少儿 Ab04B01=，以A开头的子分支只有14个 * 这一点在论文中说的非常不清晰，所以以国人的文章进行编码真是痛苦 * * @param encodeHead 输入两个字符串的公共开头 * @return 经过计算之后得到N的值 */ protected static int getN(String encodeHead) &#123; int length = StringUtils.length(encodeHead); switch (length) &#123; case 1: return getCount(encodeHead, 2); case 2: return getCount(encodeHead, 4); case 4: return getCount(encodeHead, 5); case 5: return getCount(encodeHead, 7); default: return 0; &#125; &#125; protected static int getCount(String encodeHead, int end) &#123; Set&lt;String&gt; res = new HashSet&lt;String&gt;(); Iterator&lt;String&gt; iter = encodeWords.keySet().iterator(); while (iter.hasNext()) &#123; String curr = iter.next(); if (curr.startsWith(encodeHead)) &#123; String temp = curr.substring(0, end); if (res.contains(temp)) &#123; continue; &#125; else &#123; res.add(temp); &#125; &#125; &#125; return res.size(); &#125; /** * @param encode1 第一个编码 * @param encode2 第二个编码 * @return 这两个编码对应的分支间的距离，用k表示 */ protected static int getK(String encode1, String encode2) &#123; String temp1 = encode1.substring(0, 1); String temp2 = encode2.substring(0, 1); if (StringUtils.equalsIgnoreCase(temp1, temp2)) &#123; temp1 = encode1.substring(1, 2); temp2 = encode2.substring(1, 2); &#125; else &#123; return Math.abs(temp1.charAt(0) - temp2.charAt(0)); &#125; if (StringUtils.equalsIgnoreCase(temp1, temp2)) &#123; temp1 = encode1.substring(2, 4); temp2 = encode2.substring(2, 4); &#125; else &#123; return Math.abs(temp1.charAt(0) - temp2.charAt(0)); &#125; if (StringUtils.equalsIgnoreCase(temp1, temp2)) &#123; temp1 = encode1.substring(4, 5); temp2 = encode2.substring(4, 5); &#125; else &#123; return Math.abs(Integer.valueOf(temp1) - Integer.valueOf(temp2)); &#125; if (StringUtils.equalsIgnoreCase(temp1, temp2)) &#123; temp1 = encode1.substring(5, 7); temp2 = encode2.substring(5, 7); &#125; else &#123; return Math.abs(temp1.charAt(0) - temp2.charAt(0)); &#125; return Math.abs(Integer.valueOf(temp1) - Integer.valueOf(temp2)); &#125; /** * 获取编码的公共部分字符串 * * @param encode1 * @param encode2 * @return */ protected static String getCommonStr(String encode1, String encode2) &#123; int length = StringUtils.length(encode1); StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; length; i++) &#123; if (encode1.charAt(i) == encode2.charAt(i)) &#123; sb.append(encode1.charAt(i)); &#125; else &#123; break; &#125; &#125; int sbLen = StringUtils.length(sb); //注意第三层和第五层均有两个字符，所以长度不可能出现3和6的情况 if (sbLen == 3 || sbLen == 6) &#123; sb.deleteCharAt(sbLen - 1); &#125; return String.valueOf(sb); &#125; @Test public void testGetN() &#123; readCiLin(); int a = getN("A"); System.out.println(a); &#125; @Test public void testGetK() &#123; int k = getK("Aa01A01=", "Aa01A01="); System.out.println(k); &#125; @Test public void testGetCommonStr() &#123; String commonStr = getCommonStr("Aa01A01=", "Aa01A03="); System.out.println(commonStr); &#125; @Test public void testGetSimilarity() &#123; readCiLin(); double similarity = getSimilarity("人民", "国民"); System.out.println("人民--" + "国民:" + similarity); similarity = getSimilarity("人民", "群众"); System.out.println("人民--" + "群众:" + similarity); similarity = getSimilarity("人民", "党群"); System.out.println("人民--" + "党群:" + similarity); similarity = getSimilarity("人民", "良民"); System.out.println("人民--" + "良民:" + similarity); similarity = getSimilarity("人民", "同志"); System.out.println("人民--" + "同志:" + similarity); similarity = getSimilarity("人民", "成年人"); System.out.println("人民--" + "成年人:" + similarity); similarity = getSimilarity("人民", "市民"); System.out.println("人民--" + "市民:" + similarity); similarity = getSimilarity("人民", "亲属"); System.out.println("人民--" + "亲属:" + similarity); similarity = getSimilarity("人民", "志愿者"); System.out.println("人民--" + "志愿者:" + similarity); similarity = getSimilarity("人民", "先锋"); System.out.println("人民--" + "先锋:" + similarity); &#125; @Test public void testGetSimilarity2() &#123; readCiLin(); double similarity = getSimilarity("非洲人", "亚洲人"); System.out.println(similarity); double similarity1 = getSimilarity("骄傲", "仔细"); System.out.println(similarity1); &#125;&#125; 说明，词语相似度是个数值，一般取值范围在[0，1]之间，在原论文中，使用cos函数计算主要是将值归一化到[0，1]之间，可以将cos函数看作是一个调节因子。 testGetSimilarity的测试结果如下所示： 12345678910人民--国民:1.0人民--群众:0.9576614882494312人民--党群:0.8978076452338418人民--良民:0.7182461161870735人民--同志:0.6630145969121822人民--成年人:0.6306922220793977人民--市民:0.5405933332109123人民--亲属:0.36039555547394153人民--志愿者:0.22524722217121346人民--先锋:0.18019777773697077 本文使用的是同义词词林的扩展版，而原论文使用的是同义词词林，由于两者存在微小差距，所以本文计算结果与论文中的计算结果存在稍许误差，如果算法没错，这是可以理解的！ 以上仅为个人理解，如若发现错误，欢迎大家积极留言指正！ 代码已经推送到GitHub上了，地址点我。注意在文章末尾所注的参考资料中的链接里面的计算方法在求n的时候存在错误，希望莫要受其误导！ 参考文献[1] http://www.cnblogs.com/einyboy/archive/2012/09/09/2677265.html]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lombok开发指南]]></title>
    <url>%2F2015%2F07%2F30%2FLombok-development-guidelines%2F</url>
    <content type="text"><![CDATA[Lombok简介Lombok是一款好用顺手的工具，就像Google Guava一样，在此予以强烈推荐，每一个Java工程师都应该使用它。Lombok是一种Java™实用工具，可用来帮助开发人员消除Java的冗长代码，尤其是对于简单的Java对象（POJO）。它通过注释实现这一目的。通过在开发环境中实现Lombok，开发人员可以节省构建诸如hashCode()和equals()这样的方法以及以往用来分类各种accessor和mutator的大量时间。 Lombok官网地址：https://projectlombok.org/ 里面还提供了一个简短的学习视频。 安装LombokEclipse安装Lombok双击Jar安装首先下载Jar包，下载地址：http://projectlombok.org/download.html注意如果eclipse没有安装到默认目录，那么需要点击Specify选择eclipse.exe所在的路径，然后Install即可完成安装。 在新建项目之后，使用Lombok如果程序还报错，那么点击eclipse菜单的Project选项的clean，清理一下即可。 Eclipse手动安装Lombok步骤 将lombok.jar复制到myeclipse.ini/eclipse.ini所在的文件夹目录下 打开eclipse.ini/myeclipse.ini，在最后面插入以下两行并保存：-Xbootclasspath/a:lombok.jar-javaagent:lombok.jar 重启eclipse/myeclipse 最后需要注意的是，在使用lombok注解的时候记得要导入lombok.jar包到工程，如果使用的是Maven Project，要在pom.xml中添加依赖，并设置Maven为自动导入，参见IntelliJ部分。12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.8&lt;/version&gt;&lt;/dependency&gt; IntelliJ安装Lombok通过IntelliJ的插件中心安装注意一点，在IntelliJ中如果创建的是Maven项目，那么在pom.xml文件中添加依赖后，需要设置Maven为自动导入。 IntelliJ手动安装Lombok如果不想通过IntelliJ的插件中心安装的话，也可以手动安装，详细步骤参见Github上的说明：https://github.com/mplushnikov/lombok-intellij-plugin 简单点说手动安装步骤如下：Download the latest release and install it manually using Preferences &gt; Plugins &gt; Install plugin from disk… Lombok用法Lombok注解说明 val：用在局部变量前面，相当于将变量声明为final @NonNull：给方法参数增加这个注解会自动在方法内对该参数进行是否为空的校验，如果为空，则抛出NPE（NullPointerException） @Cleanup：自动管理资源，用在局部变量之前，在当前变量范围内即将执行完毕退出之前会自动清理资源，自动生成try-finally这样的代码来关闭流 @Getter/@Setter：用在属性上，再也不用自己手写setter和getter方法了，还可以指定访问范围 @ToString：用在类上，可以自动覆写toString方法，当然还可以加其他参数，例如@ToString(exclude=”id”)排除id属性，或者@ToString(callSuper=true, includeFieldNames=true)调用父类的toString方法，包含所有属性 @EqualsAndHashCode：用在类上，自动生成equals方法和hashCode方法 @NoArgsConstructor, @RequiredArgsConstructor and @AllArgsConstructor：用在类上，自动生成无参构造和使用所有参数的构造函数以及把所有@NonNull属性作为参数的构造函数，如果指定staticName = “of”参数，同时还会生成一个返回类对象的静态工厂方法，比使用构造函数方便很多 @Data：注解在类上，相当于同时使用了@ToString、@EqualsAndHashCode、@Getter、@Setter和@RequiredArgsConstrutor这些注解，对于POJO类十分有用 @Value：用在类上，是@Data的不可变形式，相当于为属性添加final声明，只提供getter方法，而不提供setter方法 @Builder：用在类、构造器、方法上，为你提供复杂的builder APIs，让你可以像如下方式一样调用Person.builder().name(&quot;Adam Savage&quot;).city(&quot;San Francisco&quot;).job(&quot;Mythbusters&quot;).job(&quot;Unchained Reaction&quot;).build();更多说明参考Builder @SneakyThrows：自动抛受检异常，而无需显式在方法上使用throws语句 @Synchronized：用在方法上，将方法声明为同步的，并自动加锁，而锁对象是一个私有的属性$lock或$LOCK，而java中的synchronized关键字锁对象是this，锁在this或者自己的类对象上存在副作用，就是你不能阻止非受控代码去锁this或者类对象，这可能会导致竞争条件或者其它线程错误 @Getter(lazy=true)：可以替代经典的Double Check Lock样板代码 @Log：根据不同的注解生成不同类型的log对象，但是实例名称都是log，有六种可选实现类 @CommonsLog Creates log = org.apache.commons.logging.LogFactory.getLog(LogExample.class); @Log Creates log = java.util.logging.Logger.getLogger(LogExample.class.getName()); @Log4j Creates log = org.apache.log4j.Logger.getLogger(LogExample.class); @Log4j2 Creates log = org.apache.logging.log4j.LogManager.getLogger(LogExample.class); @Slf4j Creates log = org.slf4j.LoggerFactory.getLogger(LogExample.class); @XSlf4j Creates log = org.slf4j.ext.XLoggerFactory.getXLogger(LogExample.class); Lombok使用示例 val示例 123456789public static void main(String[] args) &#123; val sets = new HashSet&lt;String&gt;(); val lists = new ArrayList&lt;String&gt;(); val maps = new HashMap&lt;String, String&gt;(); //=&gt;相当于如下 final Set&lt;String&gt; sets2 = new HashSet&lt;&gt;(); final List&lt;String&gt; lists2 = new ArrayList&lt;&gt;(); final Map&lt;String, String&gt; maps2 = new HashMap&lt;&gt;();&#125; @NonNull示例 1234567891011public void notNullExample(@NonNull String string) &#123; string.length();&#125;//=&gt;相当于public void notNullExample(String string) &#123; if (string != null) &#123; string.length(); &#125; else &#123; throw new NullPointerException("null"); &#125;&#125; @Cleanup示例 12345678910111213141516171819202122public static void main(String[] args) &#123; try &#123; @Cleanup InputStream inputStream = new FileInputStream(args[0]); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; //=&gt;相当于 InputStream inputStream = null; try &#123; inputStream = new FileInputStream(args[0]); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (inputStream != null) &#123; try &#123; inputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; @Getter/@Setter示例 1234@Setter(AccessLevel.PUBLIC)@Getter(AccessLevel.PROTECTED)private int id;private String shap; @ToString示例 1234567891011@ToString(exclude = "id", callSuper = true, includeFieldNames = true)public class LombokDemo &#123; private int id; private String name; private int age; public static void main(String[] args) &#123; //输出LombokDemo(super=LombokDemo@48524010, name=null, age=0) System.out.println(new LombokDemo()); &#125;&#125; @EqualsAndHashCode示例 12345@EqualsAndHashCode(exclude = &#123;"id", "shape"&#125;, callSuper = false)public class LombokDemo &#123; private int id; private String shap;&#125; @NoArgsConstructor, @RequiredArgsConstructor and @AllArgsConstructor示例 12345678910111213141516171819@NoArgsConstructor@RequiredArgsConstructor(staticName = "of")@AllArgsConstructorpublic class LombokDemo &#123; @NonNull private int id; @NonNull private String shap; private int age; public static void main(String[] args) &#123; new LombokDemo(1, "circle"); //使用静态工厂方法 LombokDemo.of(2, "circle"); //无参构造 new LombokDemo(); //包含所有参数 new LombokDemo(1, "circle", 2); &#125;&#125; @Data示例 1234567891011121314151617import lombok.Data;@Datapublic class Menu &#123; private String shopId; private String skuMenuId; private String skuName; private String normalizeSkuName; private String dishMenuId; private String dishName; private String dishNum; //默认阈值 private float thresHold = 0; //新阈值 private float newThresHold = 0; //总得分 private float totalScore = 0;&#125; 在IntelliJ中按下Ctrl+F12就可以看到Lombok已经为我们自动生成了一系列的方法。 @Value示例 1234567891011121314@Valuepublic class LombokDemo &#123; @NonNull private int id; @NonNull private String shap; private int age; //相当于 private final int id; public int getId() &#123; return this.id; &#125; ...&#125; @Builder示例 12345678910@Builderpublic class BuilderExample &#123; private String name; private int age; @Singular private Set&lt;String&gt; occupations; public static void main(String[] args) &#123; BuilderExample test = BuilderExample.builder().age(11).name("test").build(); &#125;&#125; @SneakyThrows示例 12345678910111213141516171819202122import lombok.SneakyThrows;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.InputStream;import java.io.UnsupportedEncodingException;public class Test &#123; @SneakyThrows() public void read() &#123; InputStream inputStream = new FileInputStream(""); &#125; @SneakyThrows public void write() &#123; throw new UnsupportedEncodingException(); &#125; //相当于 public void read() throws FileNotFoundException &#123; InputStream inputStream = new FileInputStream(""); &#125; public void write() throws UnsupportedEncodingException &#123; throw new UnsupportedEncodingException(); &#125;&#125; @Synchronized示例 12345678910111213public class SynchronizedDemo &#123; @Synchronized public static void hello() &#123; System.out.println("world"); &#125; //相当于 private static final Object $LOCK = new Object[0]; public static void hello() &#123; synchronized ($LOCK) &#123; System.out.println("world"); &#125; &#125;&#125; @Getter(lazy = true) 1234567891011public class GetterLazyExample &#123; @Getter(lazy = true) private final double[] cached = expensive(); private double[] expensive() &#123; double[] result = new double[1000000]; for (int i = 0; i &lt; result.length; i++) &#123; result[i] = Math.asin(i); &#125; return result; &#125;&#125; 相当于如下所示 12345678910111213141516171819202122232425import java.util.concurrent.atomic.AtomicReference;public class GetterLazyExample &#123; private final AtomicReference&lt;java.lang.Object&gt; cached = new AtomicReference&lt;&gt;(); public double[] getCached() &#123; java.lang.Object value = this.cached.get(); if (value == null) &#123; synchronized (this.cached) &#123; value = this.cached.get(); if (value == null) &#123; final double[] actualValue = expensive(); value = actualValue == null ? this.cached : actualValue; this.cached.set(value); &#125; &#125; &#125; return (double[]) (value == this.cached ? null : value); &#125; private double[] expensive() &#123; double[] result = new double[1000000]; for (int i = 0; i &lt; result.length; i++) &#123; result[i] = Math.asin(i); &#125; return result; &#125;&#125;]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Lombok</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3入门手册之四]]></title>
    <url>%2F2015%2F07%2F13%2FPython3-beginner-s-handbook-four%2F</url>
    <content type="text"><![CDATA[Version：Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)] on win32 函数可接受任意数量参数的函数为了能让一个函数接受任意数量的位置参数，可以使用一个*参数123456def avg(first, *rest): return (first + sum(rest)) / (1 + len(rest))# Sample useavg(1, 2) # 1.5avg(1, 2, 3, 4) # 2.5 在这个例子中，rest是由所有其他位置参数组成的元组。然后我们在代码中把它当成了一个序列来进行后续的计算。 使用一个以**开头的参数来表示接受任意数量的关键字参数。123456789101112131415161718import htmldef make_element(name, value, **attrs): keyvals = [' %s="%s"' % item for item in attrs.items()] attr_str = ''.join(keyvals) element = '&lt;&#123;name&#125;&#123;attrs&#125;&gt;&#123;value&#125;&lt;/&#123;name&#125;&gt;'.format( name=name, attrs=attr_str, value=html.escape(value)) return element# Example# Creates '&lt;item size="large" quantity="6"&gt;Albatross&lt;/item&gt;'res = make_element('item', 'Albatross', size='large', quantity=6)print(res)# Creates '&lt;p&gt;&lt;spam&gt;&lt;/p&gt;'res = make_element('p', '&lt;spam&gt;')print(res) 在这里，attrs是一个包含所有被传入进来的关键字参数的字典。 如果你还希望某个函数能同时接受任意数量的位置参数和关键字参数，可以同时使用*和**。1234def anyargs(*args, **kwargs): print(args) # A tuple Prints ('a', 'b', 'c', 'd', 'e') print(kwargs) # A dict Prints&#123;'key1': '1', 'key2': '2'&#125;anyargs('a', 'b', 'c', 'd', 'e', key1='1', key2='2') 使用这个函数时，所有位置参数会被放到args元组中，所有关键字参数会被放到字典kwargs中。一个*参数只能出现在函数定义中最后一个位置参数后面，而 **参数只能出现在最后一个参数。 有一点要注意的是，在*参数后面仍然可以定义其他参数。 只接受关键字参数的函数利用这种技术，我们还能在接受任意多个位置参数的函数中指定关键字参数。12345678def mininum(*values, clip=None): m = min(values) if clip is not None: m = clip if clip &gt; m else m return mminimum(1, 5, 2, -5, 10) # Returns -5minimum(1, 5, 2, -5, 10, clip=0) # Returns 0&lt;/pre&gt; 返回多个值的函数为了能返回多个值，函数直接return一个元组就行了。12345&gt;&gt;&gt; def myfun():... return 1, 2, 3...&gt;&gt;&gt; a, b, c = myfun()` 尽管myfun()看上去返回了多个值，实际上是先创建了一个元组然后返回的。这个语法看上去比较奇怪，实际上我们使用的是逗号来生成一个元组，而不是用括号。123456&gt;&gt;&gt; a = (1, 2) # With parentheses&gt;&gt;&gt; a(1, 2)&gt;&gt;&gt; b = 1, 2 # Without parentheses&gt;&gt;&gt; b(1, 2) 定义有默认参数的函数定义一个有可选参数的函数是非常简单的，直接在函数定义中给参数指定一个默认值，并放到参数列表最后就行了。12345def spam(a, b=42): print(a, b)spam(1) # Ok. a=1, b=42spam(1, 2) # Ok. a=1, b=2 如果默认参数是一个可修改的容器比如一个列表、集合或者字典，可以使用None作为默认值12345# Using a list as a default valuedef spam(a, b=None): if b is None: b = [] ... 有一点是值得注意的，默认参数的值仅仅在函数定义的时候赋值一次12345678910&gt;&gt;&gt; x = 42&gt;&gt;&gt; def spam(a, b=x):... print(a, b)...&gt;&gt;&gt; spam(1)1 42&gt;&gt;&gt; x = 23 # Has no effect&gt;&gt;&gt; spam(1)1 42&gt;&gt;&gt; 注意到当我们改变x的值的时候对默认参数值并没有影响，这是因为在函数定义的时候就已经确定了它的默认值了。 其次，默认参数的值应该是不可变的对象，比如None、True、False、数字或字符串。 特别的，千万不要像下面这样写代码：12def spam(a, b=[]): # NO! ... 如果你这么做了，当默认值在其他地方被修改后你将会遇到各种麻烦。这些修改会影响到下次调用这个函数时的默认值。12345678910111213&gt;&gt;&gt; def spam(a, b=[]):... print(b)... return b...&gt;&gt;&gt; x = spam(1)&gt;&gt;&gt; x[]&gt;&gt;&gt; x.append(99)&gt;&gt;&gt; x.append('Yow!')&gt;&gt;&gt; x[99, 'Yow!']&gt;&gt;&gt; spam(1) # Modified list gets returned![99, 'Yow!'] 这种结果应该不是你想要的。为了避免这种情况的发生，最好是将默认值设为None， 然后在函数里面检查它，前面的例子就是这样做的。 在测试None值时使用is操作符是很重要的，也是这种方案的关键点。 1234def spam(a, b=None): if not b: # NO! Use 'b is None' instead b = [] ... 定义匿名或内联函数当一些函数很简单，仅仅只是计算一个表达式的值的时候，就可以使用lambda表达式来代替了。12345&gt;&gt;&gt; add = lambda x, y: x + y&gt;&gt;&gt; add(2,3)5&gt;&gt;&gt; add('hello', 'world')'helloworld' lambda表达式典型的使用场景是排序或数据reduce等：1234&gt;&gt;&gt; names = ['David Beazley', 'Brian Jones',... 'Raymond Hettinger', 'Ned Batchelder']&gt;&gt;&gt; sorted(names, key=lambda name: name.split()[-1].lower())['Ned Batchelder', 'David Beazley', 'Raymond Hettinger', 'Brian Jones'] 有一点要注意的是，lambda表达式中的变量x是自由变量，在运行时绑定值，而不是定义时就绑定，这跟函数的默认值参数定义是不同的。而如果你想让某个匿名函数在定义时就捕获到值，可以将那个参数值定义成默认参数即可，类似如下： 12345678&gt;&gt;&gt; x = 10&gt;&gt;&gt; a = lambda y, x=x: x + y&gt;&gt;&gt; x = 20&gt;&gt;&gt; b = lambda y, x=x: x + y&gt;&gt;&gt; a(10)20&gt;&gt;&gt; b(10)30 如果你不设定为默认参数的话，运行如下12345678&gt;&gt;&gt; x = 10&gt;&gt;&gt; a = lambda y: x + y&gt;&gt;&gt; x = 20&gt;&gt;&gt; b = lambda y: x + y&gt;&gt;&gt; a(10)30&gt;&gt;&gt; b(10)30 Notes在这里列出来的问题是新手很容易犯的错误，有些新手可能会不恰当的lambda表达式。 比如，通过在一个循环或列表推导中创建一个lambda表达式列表，并期望函数能在定义时就记住每次的迭代值。 12345678910&gt;&gt;&gt; funcs = [lambda x: x+n for n in range(5)]&gt;&gt;&gt; for f in funcs:... print(f(0))...44444&gt;&gt;&gt; 但是实际效果是运行是n的值为迭代的最后一个值。现在我们用另一种方式修改一下：12345678910&gt;&gt;&gt; funcs = [lambda x, n=n: x+n for n in range(5)]&gt;&gt;&gt; for f in funcs:... print(f(0))...01234&gt;&gt;&gt; 通过使用函数默认值参数形式，lambda函数在定义时就能绑定到值。 减少可调用对象的参数个数如果需要减少某个函数的参数个数，你可以使用functools.partial()。partial()允许你给一个或多个参数设置固定的值，减少接下来被调用时的参数个数。12345678from functools import partialdef spam(a, b, c, d): print(a, b, c, d)spam(1, 2, 3, 4) # Prints 1 2 3 4s1 = partial(spam, 1) # a = 1s1(5, 6, 7)s2 = partial(spam, 3, 4, d=12) # a=3,b=4,d=12s2(1) 可以看出partial()固定某些参数并返回一个新的callable对象。这个新的callable接受未赋值的参数，然后跟之前已经赋值过的参数合并起来，最后将所有参数传递给原始函数。 假设你有一个点的列表来表示(x,y)坐标元组。你可以使用下面的函数来计算两点之间的距离：123456points = [ (1, 2), (3, 4), (5, 6), (7, 8) ]import mathdef distance(p1, p2): x1, y1 = p1 x2, y2 = p2 return math.hypot(x2 - x1, y2 - y1) 现在假设你想以某个点为基点，根据点和基点之间的距离来排序所有的这些点。列表的sort()方法接受一个关键字参数来自定义排序逻辑，但是它只能接受一个单个参数的函数(distance()很明显是不符合条件的)。现在我们可以通过使用partial()来解决这个问题： 1234&gt;&gt;&gt; pt = (4, 3)&gt;&gt;&gt; points.sort(key=partial(distance,pt))&gt;&gt;&gt; points[(3, 4), (1, 2), (5, 6), (7, 8)] 更进一步，partial() 通常被用来微调其他库函数所使用的回调函数的参数。 例如，下面是一段代码，使用 multiprocessing 来异步计算一个结果值， 然后这个值被传递给一个接受一个result值和一个可选logging参数的回调函数： 1234567891011121314151617181920def output_result(result, log=None): if log is not None: log.debug('Got: %r', result)# A sample functiondef add(x, y): return x + yif __name__ == '__main__': import logging from multiprocessing import Pool from functools import partial logging.basicConfig(level=logging.DEBUG) log = logging.getLogger('test') p = Pool() p.apply_async(add, (3, 4), callback=partial(output_result, log=log)) p.close() p.join() 类与对象改变对象的字符串显示要改变一个实例的字符串表示，可重新定义它的__str__()和__repr__()方法。12345678910class Pair: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return 'Pair(&#123;0.x!r&#125;, &#123;0.y!r&#125;)'.format(self) def __str__(self): return '(&#123;0.x!s&#125;, &#123;0.y!s&#125;)'.format(self) __repr__()方法返回一个实例的代码表示形式，通常用来重新构造这个实例。内置的repr()函数返回这个字符串，跟我们使用交互式解释器显示的值是一样的。__str__()方法将实例转换为一个字符串，使用str()或print()函数会输出这个字符串。12345&gt;&gt;&gt; p = Pair(3, 4)&gt;&gt;&gt; pPair(3, 4) # __repr__() output&gt;&gt;&gt; print(p)(3, 4) # __str__() output !r格式化代码指明输出使用__repr__()来代替默认的__str__()。你还可以做如下的测试：12345&gt;&gt;&gt; p = Pair(3, 4)&gt;&gt;&gt; print('p is &#123;0!r&#125;'.format(p))p is Pair(3, 4)&gt;&gt;&gt; print('p is &#123;0&#125;'.format(p))p is (3, 4) 定义__repr__()和__str__()通常是很好的习惯，因为它能简化调试和实例输出。例如，如果仅仅只是打印输出或日志输出某个实例，那么程序员会看到实例更加详细与有用的信息。__repr__()生成的文本字符串标准做法是需要让eval(repr(x)) == x为真。如果实在不能这样子做，应该创建一个有用的文本表示，并使用&lt;和&gt;括起来。1234&gt;&gt;&gt; f = open('file.dat')&gt;&gt;&gt; f&lt;_io.TextIOWrapper name='file.dat' mode='r' encoding='UTF-8'&gt;&gt;&gt;&gt; 如果__str__()没有被定义，那么就会使用__repr__()来代替输出。上面的format()方法的使用看上去很有趣，格式化代码{0.x}对应的是第1个参数的x属性。因此，在下面的函数中，0实际上指的就是self本身：12def __repr__(self): return 'Pair(&#123;0.x!r&#125;, &#123;0.y!r&#125;)'.format(self) 作为这种实现的一个替代，你也可以使用%操作符，就像下面这样12def __repr__(self): return 'Pair(%r, %r)' % (self.x, self.y) 让对象支持上下文管理协议为了让一个对象兼容with语句，你需要实现__enter()__和__exit__()方法。例如，考虑如下的一个类，它能为我们创建一个网络连接： 12345678910111213141516171819from socket import socket, AF_INET, SOCK_STREAMclass LazyConnection: def __init__(self, address, family=AF_INET, type=SOCK_STREAM): self.address = address self.family = family self.type = type self.sock = None def __enter__(self): if self.sock is not None: raise RuntimeError('Already connected') self.sock = socket(self.family, self.type) self.sock.connect(self.address) return self.sock def __exit__(self, exc_ty, exc_val, tb): self.sock.close() self.sock = None 这个类的关键特点在于它表示了一个网络连接，但是初始化的时候并不会做任何事情(比如它并没有建立一个连接)。连接的建立和关闭是使用with语句自动完成的1234567891011from functools import partialconn = LazyConnection(('www.python.org', 80))# Connection closedwith conn as s: # conn.__enter__() executes: connection open s.send(b'GET /index.html HTTP/1.0\r\n') s.send(b'Host: www.python.org\r\n') s.send(b'\r\n') resp = b''.join(iter(partial(s.recv, 8192), b'')) # conn.__exit__() executes: connection closed 编写上下文管理器的主要原理是你的代码会放到with语句块中执行。当出现with语句的时候，对象的__enter__()方法被触发，它返回的值(如果有的话)会被赋值给as声明的变量。然后，with语句块里面的代码开始执行。最后，__exit__()方法被触发进行清理工作。 不管with代码块中发生什么，上面的控制流都会执行完，就算代码块中发生了异常也是一样的。事实上，__exit__()方法的第三个参数包含了异常类型、异常值和追溯信息(如果有的话)。__exit__()方法能自己决定怎样利用这个异常信息，或者忽略它并返回一个None值。如果__exit__()返回True，那么异常会被清空，就好像什么都没发生一样，with语句后面的程序继续在正常执行。 如果你想要获得嵌套with语句的效果，代码需要如下修改123456789101112131415161718192021222324252627from socket import socket, AF_INET, SOCK_STREAMclass LazyConnection: def __init__(self, address, family=AF_INET, type=SOCK_STREAM): self.address = address self.family = family self.type = type self.connections = [] def __enter__(self): sock = socket(self.family, self.type) sock.connect(self.address) self.connections.append(sock) return sock def __exit__(self, exc_ty, exc_val, tb): self.connections.pop().close()# Example usefrom functools import partialconn = LazyConnection(('www.python.org', 80))with conn as s1: pass with conn as s2: pass # s1 and s2 are independent sockets 在第二个版本中，LazyConnection类可以被看做是某个连接工厂。在内部，一个列表被用来构造一个栈。每次__enter__()方法执行的时候，它复制创建一个新的连接并将其加入到栈里面。__exit__()方法简单的从栈中弹出最后一个连接并关闭它。这里稍微有点难理解，不过它能允许嵌套使用with语句创建多个连接。 在需要管理一些资源比如文件、网络连接和锁的编程环境中，使用上下文管理器是很普遍的。这些资源的一个主要特征是它们必须被手动的关闭或释放来确保程序的正确运行。例如，如果你请求了一个锁，那么你必须确保之后释放了它，否则就可能产生死锁。通过实现__enter__()和__exit__()方法并使用with语句可以很容易的避免这些问题，因为__exit__()方法可以让你无需担心这些了。 创建大量对象时节省内存方法对于主要是用来当成简单的数据结构的类而言，你可以通过给类添加__slots__属性来极大的减少实例所占的内存。123456class Date: __slots__ = ['year', 'month', 'day'] def __init__(self, year, month, day): self.year = year self.month = month self.day = day 当你定义__slots__后，Python就会为实例使用一种更加紧凑的内部表示。实例通过一个很小的固定大小的数组来构建，而不是为每个实例定义一个字典，这跟元组或列表很类似。在__slots__中列出的属性名在内部被映射到这个数组的指定小标上。使用slots一个不好的地方就是我们不能再给实例添加新的属性了，只能使用在__slots__中定义的那些属性名。 在类中封装属性名Python程序员不去依赖语言特性去封装数据，而是通过遵循一定的属性和方法命名规约来达到这个效果。第一个约定是任何以单下划线_开头的名字都应该是内部实现。 12345678910111213class A: def __init__(self): self._internal = 0 # An internal attribute self.public = 1 # A public attribute def public_method(self): ''' A public method ''' pass def _internal_method(self): pass Python并不会真的阻止别人访问内部名称。但是如果你这么做肯定是不好的，可能会导致脆弱的代码。同时还要注意到，使用下划线开头的约定同样适用于模块名和模块级别函数。例如，如果你看到某个模块名以单下划线开头(比如_socket)，那它就是内部实现。类似的，模块级别函数比如sys._getframe()在使用的时候就得加倍小心了。 使用双下划线开始会导致访问名称变成其他形式。比如，在前面的类B中，私有属性会被分别重命名为_B__private和_B__private_method。这时候你可能会问这样重命名的目的是什么，答案就是继承——这种属性通过继承是无法被覆盖的。比如：12345678class C(B): def __init__(self): super().__init__() self.__private = 1 # Does not override B.__private # Does not override B.__private_method() def __private_method(self): pass 这里，私有名称__private和__private_method被重命名为_C__private和_C__private_method，这个跟父类B中的名称是完全不同的。 通常的，如果定义的变量与保留关键字冲突，那么可以单划线作为后缀。一般的以单划线开头定义变量即可，如果你清楚你的代码会涉及到子类，那么推荐是用双下划线方案。 创建可管理的属性自定义某个属性的一种简单方法是将它定义为一个property。例如，下面的代码定义了一个property，增加对一个属性简单的类型检查：1234567891011121314151617181920class Person: def __init__(self, first_name): self.first_name = first_name # Getter function @property def first_name(self): return self._first_name # Setter function @first_name.setter def first_name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._first_name = value # Deleter function (optional) @first_name.deleter def first_name(self): raise AttributeError("Can't delete attribute") 上述代码中有三个相关联的方法，这三个方法的名字都必须一样。第一个方法是一个getter函数，它使得first_name成为一个属性。其他两个方法给first_name属性添加了setter和deleter函数。需要强调的是只有在first_name属性被创建后，后面的两个装饰器@first_name.setter和@first_name.deleter才能被定义。 property的一个关键特征是它看上去跟普通的attribute没什么两样，但是访问它的时候会自动触发getter、setter和deleter方法。例如： 12345678910111213&gt;&gt;&gt; a = Person('Guido')&gt;&gt;&gt; a.first_name # Calls the getter'Guido'&gt;&gt;&gt; a.first_name = 42 # Calls the setterTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "prop.py", line 14, in first_name raise TypeError('Expected a string')TypeError: Expected a string&gt;&gt;&gt; del a.first_nameTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: can't delete attribute 在实现一个property时候，底层数据(如果有的话)仍然需要存储在某个地方。因此，在get和set方法中，你会看到对_first_name属性的操作，这也是实际数据保存的地方。另外，你可能还会问为什么__init__()方法中设置了self.first_name而不是self._first_name。在这个例子中，我们创建一个property的目的就是在设置attribute的时候进行检查。因此，你可能想在初始化的时候也进行这种类型检查。通过设置self.first_name，自动调用setter方法，这个方法里面会进行参数的检查，否则就是直接访问self._first_name了。 还能在已存在的get和set方法基础上定义property。1234567891011121314151617181920class Person: def __init__(self, first_name): self.set_first_name(first_name) # Getter function def get_first_name(self): return self._first_name # Setter function def set_first_name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._first_name = value # Deleter function (optional) def del_first_name(self): raise AttributeError("Can't delete attribute") # Make a property from existing get/set methods name = property(get_first_name, set_first_name, del_first_name) 调用父类方法12345678class A: def spam(self): print('A.spam')class B(A): def spam(self): print('B.spam') super().spam() # Call parent spam() super()函数的一个常见用法是在__init__()方法中确保父类被正确的初始化了： 12345678class A: def __init__(self): self.x = 0class B(A): def __init__(self): super().__init__() self.y = 1 定义接口或者抽象基类使用abs模块可以很轻松的定义抽象基类:12345678910from abc import ABCMeta, abstractmethodclass IStream(metaclass=ABCMeta): @abstractmethod def read(self, maxbytes=-1): pass @abstractmethod def write(self, data): pass 抽象类的一个特点是它不能直接被实例化，比如你想像下面这样做是不行的:12a = IStream() # TypeError: Can't instantiate abstract class # IStream with abstract methods read, write 抽象类的目的就是让别的类继承它并实现特定的抽象方法：123456class SocketStream(IStream): def read(self, maxbytes=-1): pass def write(self, data): pass 抽象基类的一个主要用途是在代码中检查某些类是否为特定类型，实现了特定接口：1234def serialize(obj, stream): if not isinstance(stream, IStream): raise TypeError('Expected an IStream') pass 除了继承这种方式外，还可以通过注册方式来让某个类实现抽象基类：12345678import io# Register the built-in I/O classes as supporting our interfaceIStream.register(io.IOBase)# Open a normal file and type checkf = open('foo.txt')isinstance(f, IStream) # Returns True @abstractmethod还能注解静态方法、类方法和properties。你只需保证这个注解紧靠在函数定义前即可： 1234567891011121314151617181920class A(metaclass=ABCMeta): @property @abstractmethod def name(self): pass @name.setter @abstractmethod def name(self, value): pass @classmethod @abstractmethod def method1(cls): pass @staticmethod @abstractmethod def method2(): pass 标准库中有很多用到抽象基类的地方。collections模块定义了很多跟容器和迭代器(序列、映射、集合等)有关的抽象基类。numbers库定义了跟数字对象(整数、浮点数、有理数等)有关的基类。io库定义了很多跟I/O操作相关的基类。 你可以使用预定义的抽象类来执行更通用的类型检查，例如：12345678910111213141516import collections# Check if x is a sequenceif isinstance(x, collections.Sequence):...# Check if x is iterableif isinstance(x, collections.Iterable):...# Check if x has a sizeif isinstance(x, collections.Sized):...# Check if x is a mappingif isinstance(x, collections.Mapping): 尽管ABCs可以让我们很方便的做类型检查，但是我们在代码中最好不要过多的使用它。因为Python的本质是一门动态编程语言，其目的就是给你更多灵活性，强制类型检查或让你代码变得更复杂，这样做无异于舍本求末。 实现自定义容器collections定义了很多抽象基类，当你想自定义容器类的时候它们会非常有用。比如你想让你的类支持迭代，那就让你的类继承collections.Iterable即可：123import collectionsclass A(collections.Iterable): pass 不过你需要实现collections.Iterable所有的抽象方法，否则会报错:1234&gt;&gt;&gt; a = A()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: Can't instantiate abstract class A with abstract methods __iter__ collections中很多抽象类会为一些常见容器操作提供默认的实现，这样一来你只需要实现那些你最感兴趣的方法即可。假设你的类继承自collections.MutableSequence，如下：123456789101112131415161718192021222324class Items(collections.MutableSequence): def __init__(self, initial=None): self._items = list(initial) if initial is not None else [] # Required sequence methods def __getitem__(self, index): print('Getting:', index) return self._items[index] def __setitem__(self, index, value): print('Setting:', index, value) self._items[index] = value def __delitem__(self, index): print('Deleting:', index) del self._items[index] def insert(self, index, value): print('Inserting:', index, value) self._items.insert(index, value) def __len__(self): print('Len') return len(self._items) 在类中定义多个构造器为了实现多个构造器，你需要使用到类方法。例如：123456789101112131415import timeclass Date: """方法一：使用类方法""" # Primary constructor def __init__(self, year, month, day): self.year = year self.month = month self.day = day # Alternate constructor @classmethod def today(cls): t = time.localtime() return cls(t.tm_year, t.tm_mon, t.tm_mday) 直接调用类方法即可，下面是使用示例：12a = Date(2012, 12, 21) # Primaryb = Date.today() # Alternate 类方法的一个主要用途就是定义多个构造器。它接受一个class作为第一个参数(cls)。你应该注意到了这个类被用来创建并返回最终的实例。在继承时也能工作的很好：12345class NewDate(Date): passc = Date.today() # Creates an instance of Date (cls=Date)d = NewDate.today() # Creates an instance of NewDate (cls=NewDate) 创建不调用init方法的实例可以通过__new__()方法创建一个未初始化的实例。例如考虑如下这个类： 12345class Date: def __init__(self, year, month, day): self.year = year self.month = month self.day = day 下面演示如何不调用__init__()方法来创建这个Date实例：12345678&gt;&gt;&gt; d = Date.__new__(Date)&gt;&gt;&gt; d&lt;__main__.Date object at 0x1006716d0&gt;&gt;&gt;&gt; d.yearTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'Date' object has no attribute 'year'&gt;&gt;&gt; 结果可以看到，这个Date实例的属性year还不存在，所以你需要手动初始化：123456789&gt;&gt;&gt; data = &#123;'year':2012, 'month':8, 'day':29&#125;&gt;&gt;&gt; for key, value in data.items():... setattr(d, key, value)...&gt;&gt;&gt; d.year2012&gt;&gt;&gt; d.month8&gt;&gt;&gt; 实现状态对象或者状态机你想实现一个状态机或者是在不同状态下执行操作的对象，但是又不想在代码中出现太多的条件判断语句。那么如下是最好的实现方式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class Connection1: """新方案——对每个状态定义一个类""" def __init__(self): self.new_state(ClosedConnectionState) def new_state(self, newstate): self._state = newstate # Delegate to the state class def read(self): return self._state.read(self) def write(self, data): return self._state.write(self, data) def open(self): return self._state.open(self) def close(self): return self._state.close(self)# Connection state base classclass ConnectionState: @staticmethod def read(conn): raise NotImplementedError() @staticmethod def write(conn, data): raise NotImplementedError() @staticmethod def open(conn): raise NotImplementedError() @staticmethod def close(conn): raise NotImplementedError()# Implementation of different statesclass ClosedConnectionState(ConnectionState): @staticmethod def read(conn): raise RuntimeError('Not open') @staticmethod def write(conn, data): raise RuntimeError('Not open') @staticmethod def open(conn): conn.new_state(OpenConnectionState) @staticmethod def close(conn): raise RuntimeError('Already closed')class OpenConnectionState(ConnectionState): @staticmethod def read(conn): print('reading') @staticmethod def write(conn, data): print('writing') @staticmethod def open(conn): raise RuntimeError('Already open') @staticmethod def close(conn): conn.new_state(ClosedConnectionState)c = Connection1()print(c._state)c.open()print(c._state)c.read()print(c._state)c.write('Hello')print(c._state)c.close()print(c._state)#### 通过字符串调用对象方法 让类支持比较操作Python类对每个比较操作都需要实现一个特殊方法来支持。例如为了支持&gt;=操作符，你需要定义一个__ge__()方法。尽管定义一个方法没什么问题，但如果要你实现所有可能的比较方法那就有点烦人了。 装饰器functools.total_ordering就是用来简化这个处理的。使用它来装饰一个来，你只需定义一个__eq__()方法，外加其他方法(__lt__, __le__, __gt__, or __ge__)中的一个即可。然后装饰器会自动为你填充其它比较方法。123456789101112131415161718192021222324252627282930313233from functools import total_orderingclass Room: def __init__(self, name, length, width): self.name = name self.length = length self.width = width self.square_feet = self.length * self.width@total_orderingclass House: def __init__(self, name, style): self.name = name self.style = style self.rooms = list() @property def living_space_footage(self): return sum(r.square_feet for r in self.rooms) def add_room(self, room): self.rooms.append(room) def __str__(self): return '&#123;&#125;: &#123;&#125; square foot &#123;&#125;'.format(self.name, self.living_space_footage, self.style) def __eq__(self, other): return self.living_space_footage == other.living_space_footage def __lt__(self, other): return self.living_space_footage &lt; other.living_space_footage 这里我们只是给House类定义了两个方法：__eq__()和__lt__()，它就能支持所有的比较操作。123456789101112131415161718192021# Build a few houses, and add rooms to themh1 = House('h1', 'Cape')h1.add_room(Room('Master Bedroom', 14, 21))h1.add_room(Room('Living Room', 18, 20))h1.add_room(Room('Kitchen', 12, 16))h1.add_room(Room('Office', 12, 12))h2 = House('h2', 'Ranch')h2.add_room(Room('Master Bedroom', 14, 21))h2.add_room(Room('Living Room', 18, 20))h2.add_room(Room('Kitchen', 12, 16))h3 = House('h3', 'Split')h3.add_room(Room('Master Bedroom', 14, 21))h3.add_room(Room('Living Room', 18, 20))h3.add_room(Room('Office', 12, 16))h3.add_room(Room('Kitchen', 15, 17))houses = [h1, h2, h3]print('Is h1 bigger than h2?', h1 &gt; h2) # prints Trueprint('Is h2 smaller than h3?', h2 &lt; h3) # prints Trueprint('Is h2 greater than or equal to h1?', h2 &gt;= h1) # Prints Falseprint('Which one is biggest?', max(houses)) # Prints 'h3: 1101-square-foot Split'print('Which is smallest?', min(houses)) # Prints 'h2: 846-square-foot Ranch' 创建缓存实例在创建一个类的对象时，如果之前使用同样参数创建过这个对象，你想返回它的缓存引用。为了达到这样的效果，你需要使用一个和类本身分开的工厂函数，例如：123456789101112131415# The class in questionclass Spam: def __init__(self, name): self.name = name# Caching supportimport weakref_spam_cache = weakref.WeakValueDictionary()def get_spam(name): if name not in _spam_cache: s = Spam(name) _spam_cache[name] = s else: s = _spam_cache[name] return s 对于大部分程序而已，这里代码已经够用了。不过还是有一些更高级的实现值得了解下。首先是这里使用到了一个全局变量，并且工厂函数跟类放在一块。我们可以通过将缓存代码放到一个单独的缓存管理器中：123456789101112131415161718192021222324import weakrefclass CachedSpamManager: def __init__(self): self._cache = weakref.WeakValueDictionary() def get_spam(self, name): if name not in self._cache: s = Spam(name) self._cache[name] = s else: s = self._cache[name] return s def clear(self): self._cache.clear()class Spam: manager = CachedSpamManager() def __init__(self, name): self.name = name def get_spam(name): return Spam.manager.get_spam(name) 但是这样暴露了类的实例化给用户，如果想阻止用户实例化该类的话，可以通过如下方法解决1234567891011121314151617181920212223242526# ------------------------最后的修正方案------------------------class CachedSpamManager2: def __init__(self): self._cache = weakref.WeakValueDictionary() def get_spam(self, name): if name not in self._cache: temp = Spam3._new(name) # Modified creation self._cache[name] = temp else: temp = self._cache[name] return temp def clear(self): self._cache.clear()class Spam3: def __init__(self, *args, **kwargs): raise RuntimeError("Can't instantiate directly") # Alternate constructor @classmethod def _new(cls, name): self = cls.__new__(cls) self.name = name return self 参考文献[1] A Byte of Python3[2] python3-cookbook]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现字符串相似度-编辑距离]]></title>
    <url>%2F2015%2F07%2F07%2FPython-implementation-string-similarity-edit-distance%2F</url>
    <content type="text"><![CDATA[编辑距离关于维基上的定义参见这里。编辑距离，又称Levenshtein距离，是指两个字串之间，由一个转成另一个所需的最少编辑操作次数。通常许可的编辑操作包括: 将一个字符替换成另一个字符 插入一个字符 删除一个字符 例如将kitten一字转成sitting： sitten （k→s） sittin （e→i） sitting （→g） 俄罗斯科学家Vladimir Levenshtein在1965年提出了这个概念。 算法动态规划经常被用来作为这个问题的解决手段之一。伪代码如下1234567891011121314151617181920整数 Levenshtein距离(字符 str1[1..lenStr1], 字符 str2[1..lenStr2]) 声明 int d[0..lenStr1, 0..lenStr2] 声明 int i, j, cost for i = 由 0 至 lenStr1 d[i, 0] := i for j = 由 0 至 lenStr2 d[0, j] := j for i = 由 1 至 lenStr1 for j = 由 1 至 lenStr2 若 str1[i] = str2[j] 则 cost := 0 否则 cost := 1 d[i, j] := 最小值( d[i-1, j ] + 1, // 刪除 d[i , j-1] + 1, // 插入 d[i-1, j-1] + cost // 替换 ) 返回 d[lenStr1, lenStr2] Python实现12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-"""Created on 2015/7/7 10:08使用动态规划算法实现编辑距离的计算@author: Wang Xu"""import numpy as npclass LevenshteinDistance: def leDistance(self, input_x, input_y): xlen = len(input_x) + 1 # 此处需要多开辟一个元素存储最后一轮的计算结果 ylen = len(input_y) + 1 dp = np.zeros(shape=(xlen, ylen), dtype=int) for i in range(0, xlen): dp[i][0] = i for j in range(0, ylen): dp[0][j] = j for i in range(1, xlen): for j in range(1, ylen): if input_x[i - 1] == input_y[j - 1]: dp[i][j] = dp[i - 1][j - 1] else: dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) return dp[xlen - 1][ylen - 1]if __name__ == '__main__': ld = LevenshteinDistance() print(ld.leDistance('瓦罐蹄膀饭', '瓦罐焖蹄饭')) # Prints 2 print(ld.leDistance('', 'a')) # Prints 1 print(ld.leDistance('b', '')) # Prints 1 print(ld.leDistance('', '')) # Prints 0 print(ld.leDistance('杭椒小炒肉面', '外婆小肉面')) # Prints 3 print(ld.leDistance('外婆小肉面', '杭椒小炒肉面')) # Prints 3]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3入门手册之三]]></title>
    <url>%2F2015%2F07%2F06%2FPython3-beginner-s-handbook-three%2F</url>
    <content type="text"><![CDATA[Version：Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)] on win32 文件与I/O读写文本数据使用带有rt模式的open()函数读取文本文件1234567with open('test.csv', mode='rt', encoding='utf-8') as f: data = f.read() print(data)with open('test.csv', mode='rt', encoding='utf-8') as f: for line in f: print(line, end='') 类似的，为了写入一个文本文件，使用带有wt模式的open()函数，如果之前文件内容存在则清除并覆盖掉1234567with open('dest.csv', mode='wt', encoding='utf-8') as f: f.write('text1') # 注意该方式写入文件默认没有换行符，需手动添加 f.write('text2')with open('dest.csv', mode='wt', encoding='utf-8') as f: print('text1', file=f) # 该方式写入文件默认换行，无需手动添加 print('text2', file=f) 如果是在已存在文件中添加内容，使用模式为at的open()函数。with控制块结束时，文件会自动关闭。你也可以不使用with语句，但是这时候你就必须记得手动关闭文件。 使用print输出元组12row = ('a', 'b', 'c')print(*row, sep=',', end='。') # Prints a,b,c。 读写字节数据使用模式为rb或wb的open()函数来读取或写入二进制数据12345with open('dest2.csv', mode='wb') as f: f.write(b'HelloWorld!')with open('dest2.csv', mode='rb') as f: print(f.read()) 在读取二进制数据的时候，字节字符串和文本字符串的语义差异可能会导致一个潜在的陷阱。特别需要注意的是，索引和迭代动作返回的是字节的值而不是字节字符串。1234567891011121314151617181920212223242526&gt;&gt;&gt; # Text string&gt;&gt;&gt; t = 'Hello World'&gt;&gt;&gt; t[0]'H'&gt;&gt;&gt; for c in t:... print(c)...Hello...&gt;&gt;&gt; # Byte string&gt;&gt;&gt; b = b'Hello World'&gt;&gt;&gt; b[0]72&gt;&gt;&gt; for c in b:... print(c)...72101108108111... 向不存在的文件中写入数据如果只允许向不存在的文件写入数据，可以在open()函数中使用x模式来代替w模式来解决这个问题。1234567with open('dest2.csv', mode='xt') as f: f.write('test')Traceback (most recent call last): File "E:/workspaces/Python3/edu/shu/python/study/FileAndIO.py", line 33, in &lt;module&gt; with open('dest2.csv', mode='xt') as f:FileExistsError: [Errno 17] File exists: 'dest2.csv' 由提示信息可知，当该文件在系统中已经存在的时候，是不允许继续向其中写入内容的。如果文件是二进制的，使用xb来代替xt。 字符串的I/O操作使用io.StringIO()和io.BytesIO()类来创建类文件对象操作字符串数据1234567891011121314import ios = io.StringIO()s.write('Hello World!')print('\nthis is a test', file=s)# 向控制台输出内容print(s.getvalue())s = io.StringIO('Hello World!\n')# 先读取4个字符print(s.read(4))# 读取剩下的字符print(s.read()) io.StringIO只能用于文本。如果你要操作二进制数据，要使用io.BytesIO类来代替1234&gt;&gt;&gt; s = io.BytesIO()&gt;&gt;&gt; s.write(b'binary data')&gt;&gt;&gt; s.getvalue()b'binary data' 读写压缩文件gzip和bz2模块可以很容易的处理这些文件。两个模块都为open()函数提供了另外的实现来解决这个问题123456789# gzip compressionimport gzipwith gzip.open('somefile.gz', 'rt') as f: text = f.read()# bz2 compressionimport bz2with bz2.open('somefile.bz2', 'rt') as f: text = f.read() 类似的，为了写入压缩数据，可以这样做123456789# gzip compressionimport gzipwith gzip.open('somefile.gz', 'wt') as f: f.write(text)# bz2 compressionimport bz2with bz2.open('somefile.bz2', 'wt') as f: f.write(text) 大部分情况下读写压缩数据都是很简单的。但是要注意的是选择一个正确的文件模式是非常重要的。如果你不指定模式，那么默认的就是二进制模式，如果这时候程序想要接受的是文本数据，那么就会出错。当写入压缩数据时，可以使用compresslevel这个可选的关键字参数来指定一个压缩级别。12with gzip.open('somefile.gz', 'wt', compresslevel=5) as f: f.write(text) 默认的等级是9，也是最高的压缩等级。等级越低性能越好，但是数据压缩程度也越低。 最后一点，gzip.open()和bz2.open()还有一个很少被知道的特性，它们可以作用在一个已存在并以二进制模式打开的文件上。比如，下面代码是可行的：1234import gzipf = open('somefile.gz', 'rb')with gzip.open(f, 'rt') as g: text = g.read() 这样就允许gzip和bz2模块可以工作在许多类文件对象上，比如套接字，管道和内存中文件等。 读取二进制数据到可变缓冲区中1234567import os.pathdef read_into_buffer(filename): buf = bytearray(os.path.getsize(filename)) with open(filename, 'rb') as f: f.readinto(buf) return buf 1234567891011121314&gt;&gt;&gt; # Write a sample file&gt;&gt;&gt; with open('sample.bin', 'wb') as f:... f.write(b'Hello World')...&gt;&gt;&gt; buf = read_into_buffer('sample.bin')&gt;&gt;&gt; bufbytearray(b'Hello World')&gt;&gt;&gt; buf[0:5] = b'Hallo'&gt;&gt;&gt; bufbytearray(b'Hallo World')&gt;&gt;&gt; with open('newsample.bin', 'wb') as f:... f.write(buf)...11 文件对象的readinto()方法能被用来为预先分配内存的数组填充数据，甚至包括由array模块或numpy库创建的数组。和普通read()方法不同的是，readinto()填充已存在的缓冲区而不是为新对象重新分配内存再返回它们。因此，你可以使用它来避免大量的内存分配操作。比如，如果你读取一个由相同大小的记录组成的二进制文件时，你可以像下面这样写：12345678910record_size = 32 # Size of each record (adjust value)buf = bytearray(record_size)with open('somefile', 'rb') as f: while True: n = f.readinto(buf) if n &lt; record_size: break # Use the contents of buf ... 文件路径名的操作1234567891011121314151617181920212223&gt;&gt;&gt; import os&gt;&gt;&gt; path = '/Users/beazley/Data/data.csv'&gt;&gt;&gt; # Get the last component of the path&gt;&gt;&gt; os.path.basename(path)'data.csv'&gt;&gt;&gt; # Get the directory name&gt;&gt;&gt; os.path.dirname(path)'/Users/beazley/Data'&gt;&gt;&gt; # Join path components together&gt;&gt;&gt; os.path.join('tmp', 'data', os.path.basename(path))'tmp/data/data.csv'&gt;&gt;&gt; # Expand the user's home directory&gt;&gt;&gt; path = '~/Data/data.csv'&gt;&gt;&gt; os.path.expanduser(path)'/Users/beazley/Data/data.csv'&gt;&gt;&gt; # Split the file extension&gt;&gt;&gt; os.path.splitext(path)('~/Data/data', '.csv') 对于任何的文件名的操作，你都应该使用os.path模块，而不是使用标准字符串操作来构造自己的代码。特别是为了可移植性考虑的时候更应如此，因为os.path模块知道Unix和Windows系统之间的差异并且能够可靠地处理类似Data/data.csv和Data\data.csv这样的文件名。其次，你真的不应该浪费时间去重复造轮子。通常最好是直接使用已经为你准备好的功能。 使用如下方式测试文件的类型，如果测试的文件不存在的时候，结果都会返回False123456789101112131415&gt;&gt;&gt; # Is a regular file&gt;&gt;&gt; os.path.isfile('/etc/passwd')True&gt;&gt;&gt; # Is a directory&gt;&gt;&gt; os.path.isdir('/etc/passwd')False&gt;&gt;&gt; # Is a symbolic link&gt;&gt;&gt; os.path.islink('/usr/local/bin/python3')True&gt;&gt;&gt; # Get the file linked to&gt;&gt;&gt; os.path.realpath('/usr/local/bin/python3')'/usr/local/bin/python3.3' 如果你还想获取元数据(比如文件大小或者是修改日期)，也可以使用os.path模块来解决1234567&gt;&gt;&gt; os.path.getsize('/etc/passwd')3669&gt;&gt;&gt; os.path.getmtime('/etc/passwd')1272478234.0&gt;&gt;&gt; import time&gt;&gt;&gt; time.ctime(os.path.getmtime('/etc/passwd'))'Wed Apr 28 13:10:34 2010' 使用os.listdir()函数来获取某个目录中的文件列表12import osnames = os.listdir('somedir') 获取目录中的列表是很容易的，但是其返回结果只是目录中实体名列表而已。如果你还想获取其他的元信息，比如文件大小，修改时间等等，你或许还需要使用到os.path模块中的函数或者os.stat()函数来收集数据。123456789101112131415161718# Example of getting a directory listingimport osimport os.pathimport globpyfiles = glob.glob('*.py')# Get file sizes and modification datesname_sz_date = [(name, os.path.getsize(name), os.path.getmtime(name)) for name in pyfiles]for name, size, mtime in name_sz_date: print(name, size, mtime)# Alternative: Get file metadatafile_metadata = [(name, os.stat(name)) for name in pyfiles]for name, meta in file_metadata: print(name, meta.st_size, meta.st_mtime) 增加或改变已打开文件的编码如果你想给一个以二进制模式打开的文件添加Unicode编码/解码方式，可以使用io.TextIOWrapper()对象包装它。123456import urllib.requestimport iou = urllib.request.urlopen('http://www.python.org')f = io.TextIOWrapper(u, encoding='utf-8')text = f.read() 如果你想修改一个已经打开的文本模式的文件的编码方式，可以先使用detach()方法移除掉已存在的文本编码层，并使用新的编码方式代替。123456&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.stdout.encoding'UTF-8'&gt;&gt;&gt; sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='latin-1')&gt;&gt;&gt; sys.stdout.encoding'latin-1 在文本模式打开的文件中写入原始的字节数据，将字节数据直接写入文件的缓冲区即可。123456789&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.stdout.write(b'Hello\n')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: must be str, not bytes&gt;&gt;&gt; sys.stdout.buffer.write(b'Hello\n')Hello5&gt;&gt;&gt; 类似的，能够通过读取文本文件的buffer属性来读取二进制数据。 创建临时文件和文件夹tempfile模块中有很多的函数可以完成这任务。为了创建一个匿名的临时文件，可以使用tempfile.TemporaryFile：123456789101112from tempfile import TemporaryFilewith TemporaryFile('w+t') as f: # Read/write to the file f.write('Hello World\n') f.write('Testing\n') # Seek back to beginning and read the data f.seek(0) data = f.read()# Temporary file is destroyed TemporaryFile()的第一个参数是文件模式，通常来讲文本模式使用w+t，二进制模式使用w+b。这个模式同时支持读和写操作，在这里是很有用的，因为当你关闭文件去改变模式的时候，文件实际上已经不存在了。TemporaryFile()另外还支持跟内置的open()函数一样的参数。 在大多数Unix系统上，通过TemporaryFile()创建的文件都是匿名的，甚至连目录都没有。如果你想打破这个限制，可以使用NamedTemporaryFile()来代替。比如：12345from tempfile import NamedTemporaryFilewith NamedTemporaryFile('w+t') as f: print('filename is:', f.name) ...# File automatically destroyed 这里，被打开文件的f.name属性包含了该临时文件的文件名。当你需要将文件名传递给其他代码来打开这个文件的时候，这个就很有用了。和TemporaryFile()一样，结果文件关闭时会被自动删除掉。如果你不想这么做，可以传递一个关键字参数delte=False即可。比如：123with NamedTemporaryFile('w+t', delete=False) as f: print('filename is:', f.name) ... 为了创建一个临时目录，可以使用tempfile.TemporaryDirectory()1234567from tempfile import TemporaryDirectorywith TemporaryDirectory() as dirname: print('dirname is:', dirname) # Use the directory ...# Directory and all contents destroyed TemporaryFile()、NamedTemporaryFile()和TemporaryDirectory()函数应该是处理临时文件目录的最简单的方式了，因为它们会自动处理所有的创建和清理步骤。在一个更低的级别，你可以使用mkstemp()和mkdtemp()来创建临时文件和目录。 与串行端口的数据通信尽管你可以通过使用Python内置的I/O模块来完成这个任务，但对于串行通信最好的选择是使用pySerial包。这个包的使用非常简单，先安装pySerial，使用类似下面这样的代码就能很容易的打开一个串行端口：123456import serialser = serial.Serial('/dev/tty.usbmodem641', # Device name varies baudrate=9600, bytesize=8, parity='N', stopbits=1) 设备名对于不同的设备和操作系统是不一样的。比如，在Windows系统上，你可以使用0，1等表示的一个设备来打开通信端口COM0和COM1。一旦端口打开，那就可以使用read()，readline()和write()函数读写数据了。12ser.write(b'G1 X50 Y50\r\n')resp = ser.readline() 序列化Python对象对于序列化最普遍的做法就是使用pickle模块。为了将一个对象保存到一个文件中，可以这样做：12345import pickledata = ... # Some Python objectf = open('somefile', 'wb')pickle.dump(data, f) 为了将一个对象转储为一个字符串，可以使用pickle.dumps()：s = pickle.dumps(data)为了从字节流中恢复一个对象，使用picle.load()或pickle.loads()函数。比如：123456# Restore from a filef = open('somefile', 'rb')data = pickle.load(f)# Restore from a stringdata = pickle.loads(s) 数据编码和处理读写CSV数据1234567891011import csvfrom collections import namedtuple# 将CSV中的数据读入元组中，并允许使用列名如row.Symbol和row.Change（Symbol和Change是CSV文件中的列名）with open('stock.csv') as f: f_csv = csv.reader(f) headings = next(f_csv) Row = namedtuple('Row', headings) for r in f_csv: row = Row(*r) # Process row ... 另外一个选择就是将数据读取到一个字典序列中去。123456import csvwith open('stocks.csv') as f: f_csv = csv.DictReader(f) for row in f_csv: # process row ... 在这个版本中，你可以使用列名去访问每一行的数据了。比如，row[&#39;Symbol&#39;]或者row[&#39;Change&#39;]。 为了写入CSV数据，你仍然可以使用CSV模块，不过这时候先创建一个writer对象。12345678910headers = ['Symbol','Price','Date','Time','Change','Volume']rows = [('AA', 39.48, '6/11/2007', '9:36am', -0.18, 181800), ('AIG', 71.38, '6/11/2007', '9:36am', -0.15, 195500), ('AXP', 62.58, '6/11/2007', '9:36am', -0.46, 935000), ]with open('stocks.csv','w') as f: f_csv = csv.writer(f) f_csv.writerow(headers) f_csv.writerows(rows) 如果你有一个字典序列的数据，可以像这样做12345678910111213headers = ['Symbol', 'Price', 'Date', 'Time', 'Change', 'Volume']rows = [&#123;'Symbol':'AA', 'Price':39.48, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.18, 'Volume':181800&#125;, &#123;'Symbol':'AIG', 'Price': 71.38, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.15, 'Volume': 195500&#125;, &#123;'Symbol':'AXP', 'Price': 62.58, 'Date':'6/11/2007', 'Time':'9:36am', 'Change':-0.46, 'Volume': 935000&#125;, ]with open('stocks.csv','w') as f: f_csv = csv.DictWriter(f, headers) f_csv.writeheader() f_csv.writerows(rows) 读写JSON数据json模块提供了一种很简单的方式来编码和解码JSON数据。其中两个主要的函数是json.dumps()和json.loads()，要比其他序列化函数库如pickle的接口少得多。1234567import jsondata = &#123; 'name' : 'ACME', 'shares' : 100, 'price' : 542.23&#125;json_str = json.dumps(data) 将JSON编码的字符串转回一个Python数据结构data = json.loads(json_str)如果你要处理的是文件而不是字符串，你可以使用json.dump()和json.load()来编码和解码JSON数据。1234567# Writing JSON datawith open('data.json', 'w') as f: json.dump(data, f)# Reading data backwith open('data.json', 'r') as f: data = json.load(f) 解析简单的XML数据可以使用xml.etree.ElementTree模块从简单的XML文档中提取数据。1234567891011121314151617from urllib.request import urlopenfrom xml.etree.ElementTree import parse# Download the RSS feed and parse itu = urlopen('http://planet.python.org/rss20.xml')doc = parse(u)# Extract and output tags of interestfor item in doc.iterfind('channel/item'): title = item.findtext('title') date = item.findtext('pubDate') link = item.findtext('link') print(title) print(date) print(link) print() xml.etree.ElementTree.parse()函数解析整个XML文档并将其转换成一个文档对象。然后，你就能使用find()、iterfind()和findtext()等方法来搜索特定的XML元素了。这些函数的参数就是某个指定的标签名，例如channel/item或title。 ElementTree模块中的每个元素有一些重要的属性和方法，在解析的时候非常有用。tag属性包含了标签的名字，text属性包含了内部的文本，而get()方法能获取属性值。 有一点要强调的是xml.etree.ElementTree并不是XML解析的唯一方法。对于更高级的应用程序，你需要考虑使用lxml。它使用了和ElementTree同样的编程接口，因此上面的例子同样也适用于lxml。你只需要将刚开始的import语句换成from lxml.etree import parse就行了。lxml完全遵循XML标准，并且速度也非常快，同时还支持验证，XSLT和XPath等特性。 增量式解析大型XML文件任何时候只要你遇到增量式的数据处理时，第一时间就应该想到迭代器和生成器。 下面是一个很简单的函数，只使用很少的内存就能增量式的处理一个大型XML文件。1234567891011121314151617181920212223from xml.etree.ElementTree import iterparsedef parse_and_remove(filename, path): path_parts = path.split('/') doc = iterparse(filename, ('start', 'end')) # Skip the root element next(doc) tag_stack = [] elem_stack = [] for event, elem in doc: if event == 'start': tag_stack.append(elem.tag) elem_stack.append(/) elif event == 'end': if tag_stack == path_parts: yield elem elem_stack[-2].remove(elem) try: tag_stack.pop() elem_stack.pop() except IndexError: pass iterparse()方法允许对XML文档进行增量操作。使用时，你需要提供文件名和一个包含下面一种或多种类型的事件列表：start,end,start-ns和end-ns。由iterparse()创建的迭代器会产生形如(event, elem)的元组，其中event是上述事件列表中的某一个，而elem是相应的XML元素。 start事件在某个元素第一次被创建并且还没有被插入其他数据(如子元素)时被创建。而end事件在某个元素已经完成时被创建。尽管没有在例子中演示，start-ns和end-ns事件被用来处理XML文档命名空间的声明。 将字典转换为XML123456789101112from xml.etree.ElementTree import Elementdef dict_to_xml(tag, d):'''Turn a simple dict of key/value pairs into XML'''elem = Element(tag)for key, val in d.items(): child = Element(key) child.text = str(val) elem.append(child)return elem 对于I/O操作，使用xml.etree.ElementTree中的tostring()函数很容易就能将它转换成一个字节字符串。1234&gt;&gt;&gt; from xml.etree.ElementTree import tostring&gt;&gt;&gt; tostring(e)b'&lt;stock&gt;&lt;price&gt;490.1&lt;/price&gt;&lt;shares&gt;100&lt;/shares&gt;&lt;name&gt;GOOG&lt;/name&gt;&lt;/stock&gt;'&gt;&gt;&gt; xml.sax.saxutils中的escape()和unescape()函数提供对HTML实体的转换方法12345&gt;&gt;&gt; from xml.sax.saxutils import escape, unescape&gt;&gt;&gt; escape('&lt;spam&gt;')'&lt;spam&gt;'&gt;&gt;&gt; unescape(_)'&lt;spam&gt;' 解析和修改XML123456789101112131415161718&gt;&gt;&gt; from xml.etree.ElementTree import parse, Element&gt;&gt;&gt; doc = parse('pred.xml')&gt;&gt;&gt; root = doc.getroot()&gt;&gt;&gt; root&lt;Element 'stop' at 0x100770cb0&gt;&gt;&gt;&gt; # Remove a few elements&gt;&gt;&gt; root.remove(root.find('sri'))&gt;&gt;&gt; root.remove(root.find('cr'))&gt;&gt;&gt; # Insert a new element after &lt;nm&gt;...&lt;/nm&gt;&gt;&gt;&gt; root.getchildren().index(root.find('nm'))1&gt;&gt;&gt; e = Element('spam')&gt;&gt;&gt; e.text = 'This is a test'&gt;&gt;&gt; root.insert(2, e)&gt;&gt;&gt; # Write back to a file&gt;&gt;&gt; doc.write('newpred.xml', xml_declaration=True) 编码和解码十六进制数12345678910&gt;&gt;&gt; # Initial byte string&gt;&gt;&gt; s = b'hello'&gt;&gt;&gt; # Encode as hex&gt;&gt;&gt; import binascii&gt;&gt;&gt; h = binascii.b2a_hex(s)&gt;&gt;&gt; hb'68656c6c6f'&gt;&gt;&gt; # Decode back to bytes&gt;&gt;&gt; binascii.a2b_hex(h)b'hello' 类似的功能同样可以在base64模块中找到123456&gt;&gt;&gt; import base64&gt;&gt;&gt; h = base64.b16encode(s)&gt;&gt;&gt; hb'68656C6C6F'&gt;&gt;&gt; base64.b16decode(h)b'hello' 大部分情况下，通过使用上述的函数来转换十六进制是很简单的。上面两种技术的主要不同在于大小写的处理。函数base64.b16decode()和base64.b16encode()只能操作大写形式的十六进制字母，而binascii模块中的函数大小写都能处理。还有一点需要注意的是编码函数所产生的输出总是一个字节字符串。如果想强制以Unicode形式输出，你需要增加一个额外的界面步骤。例如：12345&gt;&gt;&gt; h = base64.b16encode(s)&gt;&gt;&gt; print(h)b'68656C6C6F'&gt;&gt;&gt; print(h.decode('ascii'))68656C6C6F 在解码十六进制数时，函数b16decode()和a2b_hex()可以接受字节或unicode字符串。但是，unicode字符串必须仅仅只包含ASCII编码的十六进制数。 编码解码Base64数据base64模块中有两个函数b64encode()andb64decode()可以帮你解决这个问题。123456789101112&gt;&gt;&gt; # Some byte data&gt;&gt;&gt; s = b'hello'&gt;&gt;&gt; import base64&gt;&gt;&gt; # Encode as Base64&gt;&gt;&gt; a = base64.b64encode(s)&gt;&gt;&gt; ab'aGVsbG8='&gt;&gt;&gt; # Decode from Base64&gt;&gt;&gt; base64.b64decode(a)b'hello' 读写二进制数组数据可以使用struct模块处理二进制数据。下面是一段示例代码将一个Python元组列表写入一个二进制文件，并使用struct将每个元组编码为一个结构体。12345678910111213141516from struct import Structdef write_records(records, format, f): ''' Write a sequence of tuples to a binary file of structures. ''' record_struct = Struct(format) for r in records: f.write(record_struct.pack(*r))# Exampleif __name__ == '__main__': records = [ (1, 2.3, 4.5), (6, 7.8, 9.0), (12, 13.4, 56.7) ] with open('data.b', 'wb') as f: write_records(records, '&lt;idd', f) 如果你打算以块的形式增量读取文件，你可以这样做：12345678910111213from struct import Structdef read_records(format, f): record_struct = Struct(format) chunks = iter(lambda: f.read(record_struct.size), b'') return (record_struct.unpack(chunk) for chunk in chunks)# Exampleif __name__ == '__main__': with open('data.b','rb') as f: for rec in read_records('&lt;idd', f): # Process rec ... 如果你想将整个文件一次性读取到一个字节字符串中，然后在分片解析。那么你可以这样做：1234567891011121314from struct import Structdef unpack_records(format, data): record_struct = Struct(format) return (record_struct.unpack_from(data, offset) for offset in range(0, len(data), record_struct.size))# Exampleif __name__ == '__main__': with open('data.b', 'rb') as f: data = f.read() for rec in unpack_records('&lt;idd', data): # Process rec ... 参考文献[1] A Byte of Python3[2] python3-cookbook]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3入门手册之二]]></title>
    <url>%2F2015%2F07%2F04%2FPython3-beginner-s-handbook-two%2F</url>
    <content type="text"><![CDATA[Version：Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)] on win32 数字日期和时间数字的四舍五入1234567891011121314151617181920212223#!/usr/bin/python# coding: UTF-8"""Created on 2015/7/3 23:01@author: 'WX'"""print(round(1.23, 1)) # Prints 1.2print(round(1.27, 1)) # Prints 1.3print(round(-1.27, 1)) # Prints -1.3print(round(1.25361, 3)) # Prints 1.254# 当一个值刚好在两个边界的中间的时候，round 函数返回离它最近的偶数print(round(1.25, 1)) # Prints 1.2print(round(1.35, 1)) # Prints 1.4# 传给 round() 函数的 ndigits 参数可以是负数，这种情况下， 舍入运算会作用在十位、百位、千位等上面a = 1627731print(round(a, -1))print(round(a, -2))print(round(a, -3))# 格式化浮点数，保留指定的小数点后保留位数print(format(1.23456, '0.2f')) 执行精确的浮点数运算如果需要精确的浮点数计算，那么推荐使用decimal模块的Decimal123456789101112from decimal import Decimala = Decimal('1.3')b = Decimal('1.7')print(a / b)a = Decimal('4.2')b = Decimal('2.1')c = a.__add__(b)d = a + bprint(c) # Prints 6.3print(d) # Prints 6.3 无穷大与NaN12345678910a = float('inf')b = float('-inf')c = float('nan')print(a) # Prints infprint(b) # Prints -infprint(c) # Prints nanimport math# 可以使用如下方式测试这些值的存在，测试一个NaN值的唯一安全的方法就是使用math.isnan()print(math.isinf(a)) # Prints Trueprint(math.isnan(c)) # Prints True 分数运算1234567891011121314# fractions模块可以被用来执行包含分数的数学运算from fractions import Fractiona = Fraction(5, 4)b = Fraction(7, 16)print(a + b) # Prints 27/16c = a * bprint(c) # Prints 35/64print(c.numerator) # 获取分子print(c.denominator) # 获取分母print(float(c)) # 强转成float类型d = 3.75e = Fraction(*d.as_integer_ratio()) # 强转float类型到分数类型print(e) 大型数组运算涉及到数组的重量级运算操作，可以使用NumPy库。NumPy的一个主要特征是它会给Python提供一个数组对象，相比标准的Python列表而言更适合用来做数学运算1234567891011&gt;&gt;&gt; # Python lists&gt;&gt;&gt; x = [1, 2, 3, 4]&gt;&gt;&gt; y = [5, 6, 7, 8]&gt;&gt;&gt; x * 2[1, 2, 3, 4, 1, 2, 3, 4]&gt;&gt;&gt; x + 10Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: can only concatenate list (not "int") to list&gt;&gt;&gt; x + y[1, 2, 3, 4, 5, 6, 7, 8] 123456789101112&gt;&gt;&gt; # Numpy arrays&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; ax = np.array([1, 2, 3, 4])&gt;&gt;&gt; ay = np.array([5, 6, 7, 8])&gt;&gt;&gt; ax * 2array([2, 4, 6, 8])&gt;&gt;&gt; ax + 10array([11, 12, 13, 14])&gt;&gt;&gt; ax + ayarray([ 6, 8, 10, 12])&gt;&gt;&gt; ax * ayarray([ 5, 12, 21, 32]) NumPy还为数组操作提供了大量的通用函数，这些函数可以作为math模块中类似函数的替代1234&gt;&gt;&gt; np.sqrt(ax)array([ 1. , 1.41421356, 1.73205081, 2. ])&gt;&gt;&gt; np.cos(ax)array([ 0.54030231, -0.41614684, -0.9899925 , -0.65364362]) 使用NumPy构造一个二维数组的例子123456789&gt;&gt;&gt; grid = np.zeros(shape=(10000,10000), dtype=float)&gt;&gt;&gt; grid array([[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]]) 对于NumPy构造的数组，如何选择行和列呢？123456789101112131415161718192021222324252627282930313233343536373839&gt;&gt;&gt; a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])&gt;&gt;&gt; aarray([[ 1, 2, 3, 4],[ 5, 6, 7, 8],[ 9, 10, 11, 12]])&gt;&gt;&gt; # Select row 1&gt;&gt;&gt; a[1]array([5, 6, 7, 8])&gt;&gt;&gt; # Select column 1&gt;&gt;&gt; a[:,1]array([ 2, 6, 10])&gt;&gt;&gt; # Select a subregion and change it&gt;&gt;&gt; a[1:3, 1:3]array([[ 6, 7], [10, 11]])&gt;&gt;&gt; a[1:3, 1:3] += 10&gt;&gt;&gt; aarray([[ 1, 2, 3, 4], [ 5, 16, 17, 8], [ 9, 20, 21, 12]])&gt;&gt;&gt; # Broadcast a row vector across an operation on all rows&gt;&gt;&gt; a + [100, 101, 102, 103]array([[101, 103, 105, 107], [105, 117, 119, 111], [109, 121, 123, 115]])&gt;&gt;&gt; aarray([[ 1, 2, 3, 4], [ 5, 16, 17, 8], [ 9, 20, 21, 12]])&gt;&gt;&gt; # Conditional assignment on an array&gt;&gt;&gt; np.where(a &lt; 10, a, 10)array([[ 1, 2, 3, 4], [ 5, 10, 10, 8], [ 9, 10, 10, 10]]) 通常我们导入NumPy模块的时候会使用语句import numpy as np。这样的话你就不用再你的程序里面一遍遍的敲入numpy，只需要输入np就行了，节省了不少时间。 基本的日期与时间转换为了执行不同时间单位的转换和计算，请使用datetime模块123456789101112&gt;&gt;&gt; from datetime import timedelta&gt;&gt;&gt; a = timedelta(days=2, hours=6)&gt;&gt;&gt; b = timedelta(hours=4.5)&gt;&gt;&gt; c = a + b&gt;&gt;&gt; c.days2&gt;&gt;&gt; c.seconds37800&gt;&gt;&gt; c.seconds / 360010.5&gt;&gt;&gt; c.total_seconds() / 360058.5 如果你想表示指定的日期和时间，先创建一个datetime实例然后使用标准的数学运算来操作它们1234567891011121314&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; a = datetime(2012, 9, 23)&gt;&gt;&gt; print(a + timedelta(days=10))2012-10-03 00:00:00&gt;&gt;&gt;&gt;&gt;&gt; b = datetime(2012, 12, 21)&gt;&gt;&gt; d = b - a&gt;&gt;&gt; d.days89&gt;&gt;&gt; now = datetime.today()&gt;&gt;&gt; print(now)2012-12-21 14:54:43.094063&gt;&gt;&gt; print(now + timedelta(minutes=10))2012-12-21 15:04:43.094063 对大多数基本的日期和时间处理问题，datetime模块以及足够了。 如果你需要执行更加复杂的日期操作，比如处理时区，模糊时间范围，节假日计算等等， 可以考虑使用dateutil模块123456789101112131415161718192021222324&gt;&gt;&gt; a = datetime(2012, 9, 23)&gt;&gt;&gt; a + timedelta(months=1)Traceback (most recent call last):File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'months' is an invalid keyword argument for this function&gt;&gt;&gt;&gt;&gt;&gt; from dateutil.relativedelta import relativedelta&gt;&gt;&gt; a + relativedelta(months=+1)datetime.datetime(2012, 10, 23, 0, 0)&gt;&gt;&gt; a + relativedelta(months=+4)datetime.datetime(2013, 1, 23, 0, 0)&gt;&gt;&gt;&gt;&gt;&gt; # Time between two dates&gt;&gt;&gt; b = datetime(2012, 12, 21)&gt;&gt;&gt; d = b - a&gt;&gt;&gt; ddatetime.timedelta(89)&gt;&gt;&gt; d = relativedelta(b, a)&gt;&gt;&gt; drelativedelta(months=+2, days=+28)&gt;&gt;&gt; d.months2&gt;&gt;&gt; d.days28 字符串转换为日期使用Python的标准模块datetime可以很容易的解决这个问题123456789from datetime import datetimetext = '2012-09-20'y = datetime.strptime(text, '%Y-%m-%d')print(y) # Prints 2012-09-20 00:00:00z = datetime.now()print(z) # Prints 2015-07-04 15:49:17.419612diff = z - yprint(diff) # Prints 1017 days, 15:49:17.419612 datetime.strptime()方法支持很多的格式化代码，比如%Y代表4位数年份，%m代表两位数月份。还有一点值得注意的是这些格式化占位符也可以反过来使用，将日期输出为指定的格式字符串形式12345&gt;&gt;&gt; zdatetime.datetime(2012, 9, 23, 21, 37, 4, 177393)&gt;&gt;&gt; nice_z = datetime.strftime(z, '%A %B %d, %Y')&gt;&gt;&gt; nice_z'Sunday September 23, 2012' 结合时区的日期操作对几乎所有涉及到时区的问题，你都应该使用pytz模块。这个包提供了Olson时区数据库，它是时区信息的事实上的标准，在很多语言和操作系统里面都可以找到。pytz模块一个主要用途是将datetime库创建的简单日期对象本地化。12345678910from datetime import datetimefrom pytz import timezoned = datetime(2015, 1, 1, 9, 30, 0)print(d) # Prints 2015-01-01 09:30:00# central = timezone('Asia/Shanghai')# Localize the date for Chicagocentral = timezone('US/Central')loc_d = central.localize(d)print(loc_d) # Prints 2015-01-01 09:30:00-06:00 一旦日期被本地化了， 它就可以转换为其他时区的时间了。12bang_d = loc_d.astimezone(timezone('Asia/Shanghai'))print(bang_d) # Prints 2015-01-01 23:30:00+08:00 当涉及到时区操作的时候，有个问题就是我们如何得到时区的名称。比如，在这个例子中，我们如何知道“Asia/Shanghai”就是中国对应的时区名呢？为了查找，可以使用ISO3166国家代码作为关键字去查阅字典pytz.country_timezones12# Urumqi 乌鲁木齐（新疆首府）print(pytz.country_timezones['CN']) # Prints ['Asia/Shanghai', 'Asia/Urumqi'] 迭代器与生成器代理迭代Python的迭代器协议需要__iter__()方法返回一个实现了__next__()方法的迭代器对象。如果你只是迭代遍历其他容器的内容，你无须担心底层是怎样实现的。你所要做的只是传递迭代请求既可。这里的iter()函数的使用简化了代码，iter(s)只是简单的通过调用s.__iter__()方法来返回对应的迭代器对象，就跟len(s)会调用s.__len__()原理是一样的。12345678910111213141516171819202122232425262728#!/usr/bin/python# coding: UTF-8"""Created on 2015/7/4 18:33@author: 'WX'"""class Node: def __init__(self, value): self._value = value self._children = [] def __repr__(self): return 'Node(&#123;!r&#125;)'.format(self._value) def add_child(self, node): self._children.append(node) def __iter__(self): return iter(self._children)#Exampleif __name__=='__main__': root = Node(0) child1 = Node(1) child2 = Node(2) root.add_child(child1) root.add_child(child2) for ch in root: print(ch) 反向迭代反向迭代仅仅当对象的大小可预先确定或者对象实现了__reversed__()的特殊方法时才能生效。如果两者都不符合，那你必须先将对象转换为一个列表才行12345678a = [1, 2, 3, 4]for x in reversed(a): print(x, end=' ')print()# Print a file backwardsf = open('test.csv', mode='r', encoding='utf-8')for line in reversed(list(f)): print(line, end='') 要注意的是如果可迭代对象元素很多的话，将其预先转换为一个列表要消耗大量的内存。这时候需要在定义的类上实现__reversed__()方法来实现反向迭代12345678910111213141516171819202122class Countdown: def __init__(self, start): self.start = start # Forward iterator def __iter__(self): n = self.start while n &gt; 0: yield n n -= 1 # Reverse iterator def __reversed__(self): n = 1 while n &lt;= self.start: yield n n += 1for rr in reversed(Countdown(30)): print(rr)for rr in Countdown(30): print(rr) 定义一个反向迭代器可以使得代码非常的高效，因为它不再需要将数据填充到一个列表中然后再去反向迭代这个列表。 迭代器切片函数itertools.islice()正好适用于在迭代器和生成器上做切片操作1234567891011121314151617181920212223242526&gt;&gt;&gt; def count(n):... while True:... yield n... n += 1...&gt;&gt;&gt; c = count(0)&gt;&gt;&gt; c[10:20]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'generator' object is not subscriptable&gt;&gt;&gt; # Now using islice()&gt;&gt;&gt; import itertools&gt;&gt;&gt; for x in itertools.islice(c, 10, 20):... print(x)...10111213141516171819 函数islice()返回一个可以生成指定元素的迭代器，它通过遍历并丢弃直到切片开始索引位置的所有元素。然后才开始一个个的返回元素，并直到切片结束索引位置。这里要着重强调的一点是islice()会消耗掉传入的迭代器中的数据。必须考虑到迭代器是不可逆的这个事实。所以如果你需要之后再次访问这个迭代器的话，那你就得先将它里面的数据放入一个列表中。 跳过可迭代对象的开始部分为了演示，假定你在读取一个开始部分是几行注释的源文件123456789101112131415&gt;&gt;&gt; with open('/etc/passwd') as f:... for line in f:... print(line, end='')...### User Database## Note that this file is consulted directly only when the system is running# in single-user mode. At other times, this information is provided by# Open Directory....##nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/falseroot:*:0:0:System Administrator:/var/root:/bin/sh... 如果你想跳过开始部分的注释行的话，可以这样做12345678&gt;&gt;&gt; from itertools import dropwhile&gt;&gt;&gt; with open('/etc/passwd') as f:... for line in dropwhile(lambda line: line.startswith('#'), f):... print(line, end='')...nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/falseroot:*:0:0:System Administrator:/var/root:/bin/sh... 排列组合的迭代itertools模块提供了三个函数来解决这类问题。其中一个是itertools.permutations()，它接受一个集合并产生一个元组序列，每个元组由集合中所有元素的一个可能排列组成。也就是说通过打乱集合中元素排列顺序生成一个元组1234567891011&gt;&gt;&gt; items = ['a', 'b', 'c']&gt;&gt;&gt; from itertools import permutations&gt;&gt;&gt; for p in permutations(items):... print(p)...('a', 'b', 'c')('a', 'c', 'b')('b', 'a', 'c')('b', 'c', 'a')('c', 'a', 'b')('c', 'b', 'a') 如果你想得到指定长度的所有排列，你可以传递一个可选的长度参数123456789&gt;&gt;&gt; for p in permutations(items, 2):... print(p)...('a', 'b')('a', 'c')('b', 'a')('b', 'c')('c', 'a')('c', 'b') 使用itertools.combinations()可得到输入集合中元素的所有的组合12345678910111213141516171819&gt;&gt;&gt; from itertools import combinations&gt;&gt;&gt; for c in combinations(items, 3):... print(c)...('a', 'b', 'c')&gt;&gt;&gt; for c in combinations(items, 2):... print(c)...('a', 'b')('a', 'c')('b', 'c')&gt;&gt;&gt; for c in combinations(items, 1):... print(c)...('a',)('b',)('c',) 对于combinations()来讲，元素的顺序已经不重要了。也就是说，组合(&#39;a&#39;, &#39;b&#39;)跟(&#39;b&#39;, &#39;a&#39;)其实是一样的(最终只会输出其中一个)。 在计算组合的时候，一旦元素被选取就会从候选中剔除掉(比如如果元素’a’已经被选取了，那么接下来就不会再考虑它了)。而函数itertools.combinations_with_replacement()允许同一个元素被选择多次12345678910111213&gt;&gt;&gt; for c in combinations_with_replacement(items, 3):... print(c)...('a', 'a', 'a')('a', 'a', 'b')('a', 'a', 'c')('a', 'b', 'b')('a', 'b', 'c')('a', 'c', 'c')('b', 'b', 'b')('b', 'b', 'c')('b', 'c', 'c')('c', 'c', 'c') 序列上索引值迭代1234567&gt;&gt;&gt; my_list = ['a', 'b', 'c']&gt;&gt;&gt; for idx, val in enumerate(my_list):... print(idx, val)...0 a1 b2 c 为了按传统行号输出(行号从1开始)，你可以传递一个开始参数：1234567&gt;&gt;&gt; my_list = ['a', 'b', 'c']&gt;&gt;&gt; for idx, val in enumerate(my_list, 1):... print(idx, val)...1 a2 b3 c enumerate()对于跟踪某些值在列表中出现的位置是很有用的。 所以，如果你想将一个文件中出现的单词映射到它出现的行号上去，可以很容易的利用enumerate()来完成12345678910word_summary = defaultdict(list)with open('myfile.txt', 'r') as f: lines = f.readlines()for idx, line in enumerate(lines): # Create a list of words in current line words = [w.strip().lower() for w in line.split()] for word in words: word_summary[word].append(idx) 如果你处理完文件后打印 word_summary，会发现它是一个字典(准确来讲是一个defaultdict)，对于每个单词有一个key，每个key对应的值是一个由这个单词出现的行号组成的列表。如果某个单词在一行中出现过两次，那么这个行号也会出现两次，同时也可以作为文本的一个简单统计。 enumerate()函数返回的是一个enumerate对象实例，它是一个迭代器，返回连续的包含一个计数和一个值的元组，元组中的值通过在传入序列上调用next()返回。 还有一点可能并不很重要，但是也值得注意，有时候当你在一个已经解压后的元组序列上使用enumerate()函数时很容易调入陷阱。你得像下面正确的方式这样写：1234567data = [ (1, 2), (3, 4), (5, 6), (7, 8) ]# Correct!for n, (x, y) in enumerate(data): ...# Error!for n, x, y in enumerate(data): ... 同时迭代多个序列为了同时迭代多个序列，使用zip()函数123456789101112&gt;&gt;&gt; xpts = [1, 5, 4, 2, 10, 7]&gt;&gt;&gt; ypts = [101, 78, 37, 15, 62, 99]&gt;&gt;&gt; for x, y in zip(xpts, ypts):... print(x,y)...1 1015 784 372 1510 627 99&gt;&gt;&gt; zip(a, b)会生成一个可返回元组(x, y)的迭代器，其中x来自a，y来自b。一旦其中某个序列到底结尾，迭代宣告结束。因此迭代长度跟参数中最短序列长度一致。12345678&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = ['w', 'x', 'y', 'z']&gt;&gt;&gt; for i in zip(a,b):... print(i)...(1, 'w')(2, 'x')(3, 'y') 如果这个不是你想要的效果，那么还可以使用itertools.zip_longest()函数来代替。12345678910111213141516&gt;&gt;&gt; from itertools import zip_longest&gt;&gt;&gt; for i in zip_longest(a,b):... print(i)...(1, 'w')(2, 'x')(3, 'y')(None, 'z')&gt;&gt;&gt; for i in zip_longest(a, b, fillvalue=0):... print(i)...(1, 'w')(2, 'x')(3, 'y')(0, 'z') 最后强调一点就是，zip()会创建一个迭代器来作为结果返回。如果你需要将结对的值存储在列表中，要使用list()函数。1234&gt;&gt;&gt; zip(a, b)&lt;zip object at 0x1007001b8&gt;&gt;&gt;&gt; list(zip(a, b))[(1, 10), (2, 11), (3, 12)] 不同集合上元素的迭代当可迭代对象类型不一样的时候chain()同样可以很好的工作，它接受一个可迭代对象列表作为输入，并返回一个迭代器，有效的屏蔽掉在多个容器中迭代细节。1234567891011121314&gt;&gt;&gt; from itertools import chain&gt;&gt;&gt; a = [1, 2, 3, 4]&gt;&gt;&gt; b = ['x', 'y', 'z']&gt;&gt;&gt; for x in chain(a, b):... print(x)...1234xyz&gt;&gt;&gt; 展开嵌套的序列可以写一个包含yield from语句的递归生成器来轻松解决这个问题12345678910111213from collections import Iterabledef flatten(items, ignore_types=(str, bytes)): for x in items: if isinstance(x, Iterable) and not isinstance(x, ignore_types): yield from flatten(x) else: yield xitems = [1, 2, [3, 4, [5, 6], 7], 8]# Produces 1 2 3 4 5 6 7 8for x in flatten(items): print(x) 额外的参数ignore_types和检测语句isinstance(x,ignore_types)用来将字符串和字节排除在可迭代对象外，防止将它们再展开成单个的字符。这样的话字符串数组就能最终返回我们所期望的结果了。12345678&gt;&gt;&gt; items = ['Dave', 'Paula', ['Thomas', 'Lewis']]&gt;&gt;&gt; for x in flatten(items):... print(x)...DavePaulaThomasLewis 顺序迭代合并后的排序迭代对象1234567891011121314&gt;&gt;&gt; import heapq&gt;&gt;&gt; a = [1, 4, 7, 10]&gt;&gt;&gt; b = [2, 5, 6, 11]&gt;&gt;&gt; for c in heapq.merge(a, b):... print(c)...1245671011 heapq.merge可迭代特性意味着它不会立马读取所有序列。这就意味着你可以在非常长的序列中使用它，而不会有太大的开销。比如，下面是一个例子来演示如何合并两个排序文件123456with open('sorted_file_1', 'rt') as file1, \ open('sorted_file_2', 'rt') as file2, \ open('merged_file', 'wt') as outf: for line in heapq.merge(file1, file2): outf.write(line) 有一点要强调的是heapq.merge()需要所有输入序列必须是排过序的。特别的，它并不会预先读取所有数据到堆栈中或者预先排序，也不会对输入做任何的排序检测。它仅仅是检查所有序列的开始部分并返回最小的那个，这个过程一直会持续直到所有输入序列中的元素都被遍历完。 参考文献[1] A Byte of Python3[2] python3-cookbook]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现最长公共子序列-Longest Common Subsequences]]></title>
    <url>%2F2015%2F07%2F03%2FPython-implementation-of-the-longest-common-subsequences%2F</url>
    <content type="text"><![CDATA[定义一个数列S，如果分别是两个或多个已知数列的子序列，且是所有符合此条件序列中最长的，则S称为已知序列的最长公共子序列。例如序列X=ABCBDAB，Y=BDCABA。序列BCA是X和Y的一个公共子序列，但是不是X和Y的最长公共子序列，子序列BCBA是X和Y的一个LCS。 复杂度对于一般性的LCS问题（即任意数量的序列）是属于NP-hard。但当序列的数量确定时，问题可以使用动态规划（Dynamic Programming）在多项式时间解决。 解法动态规划的一个计算最长公共子序列的方法如下，以两个序列$X=\langle x_1,x_2,…,x_m \rangle$、$Y=\langle y_1,y_2,…,y_n \rangle$为例子，LCS(X,Y)表示X和Y的一个最长公共子序列：如果$x_m = y_n$，则$LCS(X,Y) = x_m + LCS(x_{m-1},y_{n-1})$如果$x_m != y_n$，则$LCS(X,Y) = max \lbrace LCS(x_{m-1},Y)，LCS(X,y_{n-1}) \rbrace$ 为了找到最长的LCS，我们定义dp[i][j]记录LCS的长度，如果X的长度为0或者Y的长度为0，那么LCS=0，即dp[i][j]=0，用i和j分别表示到序列X和序列Y的长度，状态转移方程如下: $$i=0||j=0，dp[i][j] = 0$$ $$X[i] = Y[j]，dp[i][j] = dp[i-1][j-1] + 1$$ $$X[i] != Y[j]，dp[i][j] = max \lbrace dp[i-1][j]，dp[i][j-1] \rbrace$$ Python实现LCS12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# -*- coding: utf-8 -*-"""Longest Common SubsequencesCreated on 2015/7/2 15:11@author: Wang Xu"""class LCS: def lcs_base(self, input_x, input_y): if len(input_x) == 0 or len(input_y) == 0: return "" else: a = input_x[0] b = input_y[0] if a == b: return self.lcs_base(input_x[1:], input_y[1:]) + a else: # get the max one return self.getMax(self.lcs_base(input_x[1:], input_y), self.lcs_base(input_x, input_y[1:])) # construct a list by the input string def getList(self, inputStr): listRes = list() if len(inputStr) != 0: for i in range(0, len(inputStr)): listRes.append(inputStr[i]) return listRes # return the max one between a and b, equivalently return the longest one def getMax(self, a, b): if len(a) &gt;= len(b): return a else: return bif __name__ == '__main__': lcs = LCS() l1 = lcs.getList('我的大中国') l2 = lcs.getList('大中国我的') l3 = lcs.lcs_base(l1, l2) print(l3[::-1]) l3 = lcs.lcs_base(l2, l1) print(l3[::-1]) l1 = '1233433236676' l2 = '98723765655423' l3 = lcs.lcs_base(l1, l2) print(l3[::-1]) l1 = '123s212346我的大中国啊33z' l2 = '33z的大中国' l3 = lcs.lcs_base(l1, l2) print(l3[::-1]) 该算法的空间、时间复杂度均为$O(n^{2})$，经过优化后，空间复杂度可为$O(n)$。 LCS变体-最长公共子串最长公共子串与最长公共子序列稍有区别，不过也算是LCS的一个变体，在LCS中，子序列是不必要求连续的，而子串则是“连续”的。题：给定两个字符串X，Y，求二者最长的公共子串，例如X=[aaaba]，Y=[ababaa]。二者的最长公共子串为[aba]，长度为3。 基本算法将X的每个子串与Y的每个子串做对比，求出最长公共子串12345678910111213141516171819202122232425262728293031323334353637383940# -*- coding: utf-8 -*-"""Created on 2015/7/2 16:13@author: Wang Xu"""# 使用基本算法获取最长公共子串（连续），最长公共子序列可以是非连续的def getcomlen(firststr, secondstr): comlen = 0 while firststr and secondstr: if firststr[0] == secondstr[0]: comlen += 1 firststr = firststr[1:] secondstr = secondstr[1:] else: break return comlendef lcs_base(input_x, input_y): max_common_len = 0 common_index = 0 for xtemp in range(0, len(input_x)): for ytemp in range(0, len(input_y)): com_temp = getcomlen(input_x[xtemp: len(input_x)], input_y[ytemp: len(input_y)]) if com_temp &gt; max_common_len: max_common_len = com_temp common_index = xtemp print('公共子串的长度是：%s' % max_common_len) print('最长公共子串是：%s' % input_x[common_index:common_index + max_common_len])if __name__ == '__main__': lcs_base('d11zabcdeabdcdbbcd', 'bbcd11yabcdefaaa')'''OutPut:公共子串的长度是：5最长公共子串是：abcde''' DP算法使用动态规划来解最长公共子串问题，可以考虑如何将arr[0,...,i]的问题转化为求解arr[0,...,i-1]的问题，此处考虑使用dp[i][j]表示以X[i]和Y[j]结尾的最长公共子串的长度，因为要求子串连续，所以对于X[i]和Y[j]来讲，它们要么与之前的公共子串构成新的公共子串；要么就是不构成公共子串；状态转移方程为 $$X[i] = Y[j]，dp[i][j] = dp[i-1][j-1] + 1$$ $$X[i] != Y[j]，dp[i][j] = 0$$ 对于初始化，i=0或者j=0，如果X[i] == Y[j]，dp[i][j] = 1；否则dp[i][j] = 0。 1234567891011121314151617181920212223class LCS3: def lcs_dp(self, input_x, input_y): # input_y as column, input_x as row dp = [([0] * len(input_y)) for i in range(len(input_x))] maxlen = maxindex = 0 for i in range(0, len(input_x)): for j in range(0, len(input_y)): if input_x[i] == input_y[j]: if i!=0 and j!=0: dp[i][j] = dp[i - 1][j - 1] + 1 if i == 0 or j == 0: dp[i][j] = 1 if dp[i][j] &gt; maxlen: maxlen = dp[i][j] maxindex = i + 1 - maxlen # print('最长公共子串的长度是:%s' % maxlen) # print('最长公共子串是:%s' % input_x[maxindex:maxindex + maxlen]) return input_x[maxindex:maxindex + maxlen]if __name__ == '__main__': lcs3 = LCS3() print(lcs3.lcs_dp('我是美abc国中defg国中间人', 'abdde我是美中国中国中国人')) print(lcs3.lcs_dp('cabdec','cbdec')) 参考文献[1] http://www.ahathinking.com/archives/115.html[2] http://dsqiu.iteye.com/blog/1701541[3] http://www.ahathinking.com/archives/122.html]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3入门手册之一]]></title>
    <url>%2F2015%2F07%2F02%2FPython3-beginner-s-handbook-one%2F</url>
    <content type="text"><![CDATA[Version：Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)] on win32 There are two ways of constructing a software design: one way is to make it so simple that there are obviously no deficiences; the other is to make it so complicated that there are no obvious deficiences.—- C.A.R.Hoare Success in life is a matter not so much of talent and opportunity as of concentration and perseverance—- C.W.Wendte 选择Python的原因 简单易学，功能强大，具有高效的高层数据结构，支持面向对象编程 解释性语言，可扩展性，可嵌入性，丰富的库 选择一个编辑器工欲善其事必先利其器，所以选择编辑器首当其冲。 IDLE：Python自带，极简利器，支持语法高亮 Vim/Emacs：Linux/FreeBSD平台上的开发利器，二者择其一 PyCharm：号称最智能的Python编辑器，的确是的，但是略微复杂 数据结构和算法解压序列赋值给多个变量1234567891011121314151617181920212223242526# 变量的数量需要和序列元素的数量一致data = ['a', 'b', 'c', 'd']a, b, c, d = dataprint(a, b, c, d)data = ['ACME', 50, 91.1, (2012, 12, 21)]name, shares, price, (year, month, day) = dataprint(year, month, day)# 解压一部分，其他值丢弃data = ['ACME', 50, 91.1, (2012, 12, 21)]_, shares, price, _ = dataprint(shares, price)# 只取第一个和最后一个，中间的所有元素用通配符接收record = ('Dave', 'dave@example.com', '773-555-1212', '847-555-1212')name, *_, phone = recordprint(name, phone)'''OutPut:a b c d2012 12 2150 91.1Dave 847-555-1212''' 查找最大或最小的N个元素12345678910111213141516171819202122232425import heapqnums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]print(heapq.nlargest(3, nums)) # Prints [42, 37, 23]print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2]# heapify会先将集合数据进行堆排序后放入一个列表中# heappop会先将第一个元素弹出来，然后用下一个最小的元素来取代被弹出元素heapq.heapify(nums)print(heapq.heappop(nums)) # Prints -4print(heapq.heappop(nums)) # Prints 1print(heapq.heappop(nums)) # Prints 2# 下面代码在对每个元素进行对比的时候，会以price的值进行比较。portfolio = [ &#123;'name': 'IBM', 'shares': 100, 'price': 91.1&#125;, &#123;'name': 'AAPL', 'shares': 50, 'price': 543.22&#125;, &#123;'name': 'FB', 'shares': 200, 'price': 21.09&#125;, &#123;'name': 'HPQ', 'shares': 35, 'price': 31.75&#125;, &#123;'name': 'YHOO', 'shares': 45, 'price': 16.35&#125;, &#123;'name': 'ACME', 'shares': 75, 'price': 115.65&#125;]cheap = heapq.nsmallest(3, portfolio, key=lambda s: s['price'])expensive = heapq.nlargest(3, portfolio, key=lambda s: s['price'])print(cheap)print(expensive) 字典操作相关123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 求字典中值的最小值、最大值prices = &#123; 'ACME': 45.23, 'AAPL': 612.78, 'IBM': 205.55, 'HPQ': 37.20, 'FB': 10.75&#125;min_price = min(zip(prices.values(), prices.keys()))# min_price is (10.75, 'FB')max_price = max(zip(prices.values(), prices.keys()))# max_price is (612.78, 'AAPL')# 还可以使用sorted()和zip()函数来排列字典数据prices_sorted = sorted(zip(prices.values(), prices.keys()))# prices_sorted is [(10.75, 'FB'), (37.2, 'HPQ'),# (45.23, 'ACME'), (205.55, 'IBM'),# (612.78, 'AAPL')]# 执行这些计算的时候，需要注意的是zip()函数创建的是一个只能访问一次的迭代器。 比如，下面的代码就会产生错误：prices_and_names = zip(prices.values(), prices.keys())print(min(prices_and_names)) # OKprint(max(prices_and_names)) # ValueError: max() arg is an empty sequence# 需要注意的是在计算操作中使用到了(值，键)对。当多个实体拥有相同的值的时候，键会决定返回结果。 比如，在执行min()和max()操作的时候，如果恰巧最小或最大值有重复的，那么拥有最小或最大键的实体会返回：prices = &#123;'AAA': 45.23, 'ZZZ': 45.23&#125;min(zip(prices.values(), prices.keys()))# OutPut:(45.23, 'AAA')max(zip(prices.values(), prices.keys()))# OutPut:(45.23, 'ZZZ')# 查找两字典的相同点a = &#123; 'x': 1, 'y': 2, 'z': 3&#125;b = &#123; 'w': 10, 'x': 11, 'y': 2&#125;# Find keys in commona.keys() &amp; b.keys() # &#123; 'x', 'y' &#125;# Find keys in a that are not in ba.keys() - b.keys() # &#123; 'z' &#125;# Find (key,value) pairs in commona.items() &amp; b.items() # &#123; ('y', 2) &#125; 序列中出现次数最多的元素123456789101112131415161718192021222324words = [ 'look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes', 'the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the', 'eyes', "don't", 'look', 'around', 'the', 'eyes', 'look', 'into', 'my', 'eyes', "you're", 'under']from collections import Counterword_counts = Counter(words)# 出现频率最高的3个单词top_three = word_counts.most_common(3)print(top_three)# Outputs [('eyes', 8), ('the', 5), ('look', 4)]# Counter 对象可以接受任意的 hashable 序列对象。 在底层实现上，一个 Counter 对象就是一个字典，将元素映射到它出现的次数上print(word_counts['not']) # Prints 1print(word_counts['eyes']) # Prints 8# 注意，对于Counter对象可以直接进行数学运算操作test = Counter(['addone'])word_counts = word_counts + testprint(word_counts)word_counts = word_counts - testprint(word_counts) 通过某个关键字排序一个字典列表1234567891011121314151617181920212223242526272829# 通过某个关键字排序一个字典列表rows = [ &#123;'fname': 'Brian', 'lname': 'Jones', 'uid': 1003&#125;, &#123;'fname': 'David', 'lname': 'Beazley', 'uid': 1002&#125;, &#123;'fname': 'John', 'lname': 'Cleese', 'uid': 1001&#125;, &#123;'fname': 'Big', 'lname': 'Jones', 'uid': 1004&#125;]# 根据任意的字典字段来排序输入结果行是很容易实现的，代码示例：from operator import itemgetterrows_by_fname = sorted(rows, key=itemgetter('fname'))rows_by_uid = sorted(rows, key=itemgetter('uid'))print(rows_by_fname)print(rows_by_uid)# 排序不支持原生比较的对象class User: def __init__(self, userId): self.userId = userId def __repr__(self): return 'User(&#123;&#125;)'.format(self.userId)from operator import attrgetterusers = [User(23), User(28), User(26)]print(sorted(users, key=attrgetter('userId'))) 通过某个字段将记录分组1234567891011121314151617181920212223242526272829303132333435363738rows = [ &#123;'address': '5412 N CLARK', 'date': '07/01/2012'&#125;, &#123;'address': '5148 N CLARK', 'date': '07/04/2012'&#125;, &#123;'address': '5800 E 58TH', 'date': '07/02/2012'&#125;, &#123;'address': '2122 N CLARK', 'date': '07/03/2012'&#125;, &#123;'address': '5645 N RAVENSWOOD', 'date': '07/02/2012'&#125;, &#123;'address': '1060 W ADDISON', 'date': '07/02/2012'&#125;, &#123;'address': '4801 N BROADWAY', 'date': '07/01/2012'&#125;, &#123;'address': '1039 W GRANVILLE', 'date': '07/04/2012'&#125;,]# 现在假设你想在按date分组后的数据块上进行迭代。为了这样做，你首先需要按照指定的字段(这里就是date)排序， 然后调用 itertools.groupby() 函数：from operator import itemgetterfrom itertools import groupby# Sort by the desired field firstrows.sort(key=itemgetter('date'))# Iterate in groupsfor date, items in groupby(rows, key=itemgetter('date')): print(date) for i in items: print(' ', i)'''OutPut:07/01/2012 &#123;'date': '07/01/2012', 'address': '5412 N CLARK'&#125; &#123;'date': '07/01/2012', 'address': '4801 N BROADWAY'&#125;07/02/2012 &#123;'date': '07/02/2012', 'address': '5800 E 58TH'&#125; &#123;'date': '07/02/2012', 'address': '5645 N RAVENSWOOD'&#125; &#123;'date': '07/02/2012', 'address': '1060 W ADDISON'&#125;07/03/2012 &#123;'date': '07/03/2012', 'address': '2122 N CLARK'&#125;07/04/2012 &#123;'date': '07/04/2012', 'address': '5148 N CLARK'&#125; &#123;'date': '07/04/2012', 'address': '1039 W GRANVILLE'&#125;''' 过滤序列元素12345678910values = ['1', '2', '-3', '-', '4', 'N/A', '5']def is_int(val): try: x = int(val) return True except ValueError: return Falseivals = list(filter(is_int, values))print(ivals)# Outputs ['1', '2', '-3', '4', '5'] 转换并同时计算数据1234567891011121314151617181920212223# Determine if any .py files exist in a directoryimport osfiles = os.listdir('dirname')if any(name.endswith('.py') for name in files): print('There be python!')else: print('Sorry, no python.')# Output a tuple as CSVs = ('ACME', 50, 123.45)print(','.join(str(x) for x in s)) # Prints ACME,50,123.45# Data reduction across fields of a data structureportfolio = [ &#123;'name':'GOOG', 'shares': 50&#125;, &#123;'name':'YHOO', 'shares': 75&#125;, &#123;'name':'AOL', 'shares': 20&#125;, &#123;'name':'SCOX', 'shares': 65&#125;]min_shares = min(s['shares'] for s in portfolio)# Original: Returns 20min_shares = min(s['shares'] for s in portfolio)# Alternative: Returns &#123;'name': 'AOL', 'shares': 20&#125;min_shares = min(portfolio, key=lambda s: s['shares']) 合并多个字典或映射12345678910111213a = &#123;'x': 1, 'z': 3 &#125;b = &#123;'y': 2, 'z': 4 &#125;from collections import ChainMapc = ChainMap(a,b)print(c['x']) # Outputs 1 (from a)print(c['y']) # Outputs 2 (from b)print(c['z']) # Outputs 3 (from a)# ChianMap使用原来的字典，它自己不创建新的字典。所以它并不会产生上面所说的结果，比如：a['x'] = 42print(c['x']) # Outputs 42 (from a) 字符串和文本使用多个界定符分割字符串string对象的split()方法只适应于非常简单的字符串分割情形，它并不允许有多个分隔符或者是分隔符周围不确定的空格。当你需要更加灵活的切割字符串的时候，最好使用re.split()方法。12345import reline = 'abcd efg; hijk , lmn'res = re.split(r'[;,\s]\s*', line)print(res) # Prints ['abcd', 'efg', 'hijk', '', 'lmn'] 函数re.split()是非常实用的，因为它允许你为分隔符指定多个正则模式。比如，在上面的例子中，分隔符可以是逗号(,)，分号(;)或者是空格，并且后面紧跟着任意个的空格。只要这个模式被找到，那么匹配的分隔符两边的实体都会被当成是结果中的元素返回。 返回结果为一个字段列表，这个跟str.split()返回值类型是一样的。当你使用re.split()函数时候，需要特别注意的是正则表达式中是否包含一个括号捕获分组。如果使用了捕获分组，那么被匹配的文本也将出现在结果列表中。比如，观察一下这段代码运行后的结果。12fields = re.split(r'(;|,|\s)\s*', line)print(fields) # Prints ['abcd', ' ', 'efg', ';', 'hijk', ' ', '', ',', 'lmn'] 如果你不想保留分割字符串到结果列表中去，但仍然需要使用到括号来分组正则表达式的话，确保你的分组是非捕获分组，形如(?:...)。12res = re.split(r'(?:,|;|\s)\s*', line)print(res) # Prints ['abcd', 'efg', 'hijk', '', 'lmn'] 字符串匹配1234567891011121314# 检查多种匹配可能，需要将所有的匹配项放入到一个元组中，然后传给startswith()或者endswith()方法strs = list()strs.append('a.txt')strs.append('b.doc')for s in strs: # startswith方法类似 if s.endswith(('.txt', '.doc')): print(str(s))# 请注意，该方法必须接收一个元组作为输入参数，如果你有一个list或者set类型的选择项，需要先调用tuple()将其转换为元组类型choices = ['.txt', '.doc']for s in strs: if s.endswith(tuple(choices)): print(str(s)) 12345678# fnmatch和fnmatchcase的使用from fnmatch import fnmatch, fnmatchcaseprint(fnmatch('foo.txt', '*.TXT')) # On OS X (Mac) Prints Falseprint(fnmatch('foo.txt', '*.TXT')) # On Windows Prints True# 完全使用你的模式大小写进行匹配print(fnmatchcase('foo.txt', '*.TXT')) # Prints False 字符串搜索和替换一个替换回调函数的参数是一个match对象，也就是match()或者find()返回的对象。使用group()方法来提取特定的匹配部分。回调函数最后返回替换字符串。如果除了替换后的结果外，你还想知道有多少替换发生了，可以使用re.subn()来代替。1234567891011121314151617181920# 使用re模块进行匹配和搜索text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'datepat = re.compile(r'(\d+)/(\d+)/(\d+)')list_match = datepat.findall(text) # findall方法返回的是所有匹配的列表print(list_match)for m in datepat.finditer(text): # finditer方法返回的是迭代器 print(m.groups())# 注意点，在正则的开头字母‘r’是指定不去解析反斜杠# r'(\d+)/(\d+)/(\d+)' 相当于 '(\\d+)/(\\d+)/(\\d+)'m = datepat.match('11/27/2012')print(m.group(0)) # Prints 11/27/2012print(m.group(1)) # Prints 11print(m.group(2)) # Prints 27print(m.group(3)) # Prints 2012newtext, n = datepat.subn(r'\3-\1-\2', text)print(newtext) # Prints Today is 2012-11-27. PyCon starts 2013-3-13.print(n) # Prints 2 为了在文本操作时忽略大小写，你需要在使用re模块的时候给这些操作提供re.IGNORECASE标志参数。123text = 'UPPER PYTHON, lower python, Mixed Python're.findall('python', text, flags=re.IGNORECASE) # ['PYTHON', 'python', 'Python']re.sub('python', 'snake', text, flags=re.IGNORECASE) # 'UPPER snake, lower snake, Mixed snake' 最短匹配模式在这个例子中，模式r&#39;\&quot;(.*)\&quot;&#39;的意图是匹配被双引号包含的文本。但是在正则表达式中*操作符是贪婪的，因此匹配操作会查找最长的可能匹配。1234567str_pat = re.compile(r'\"(.*)\"')text = 'Computer says "no." Phone says "yes."'str_pat.findall(text) # ['no." Phone says "yes.']# 如果希望获取最短匹配的结果，需要添加'?'str_pat = re.compile(r'\"(.*?)\"')str_pat.findall(text) # ['no.', 'yes.'] 这样就使得匹配变成非贪婪模式，从而得到最短的匹配，也就是我们想要的结果。 多行匹配模式这个问题很典型的出现在当你用点(.)去匹配任意字符的时候，忘记了点(.)不能匹配换行符的事实。为了修正这个问题，你可以修改模式字符串，增加对换行的支持。比如：123456text = '''/* this is amultiline comment */'''comment = re.compile(r'/\*((?:.|\n)*?)\*/')com = comment.findall(text)print(com) # Prints [' this is a\nmultiline comment '] 在这个模式中，(?:.|\n) 指定了一个非捕获组(也就是它定义了一个仅仅用来做匹配，而不能通过单独捕获或者编号的组)。re.compile()函数接受一个标志参数叫 re.DOTALL，在这里非常有用。它可以让正则表达式中的.匹配包括换行符在内的任意字符。比如：123comment = re.compile(r'/\*(.*?)\*/', re.DOTALL)com = comment.findall(text)print(com) # Prints [' this is a\nmultiline comment '] 删除字符串中不需要的字符strip()方法能用于删除开始或结尾的字符。lstrip()和rstrip()分别从左和从右执行删除操作。默认情况下，这些方法会去除空白字符，但是你也可以指定其他字符。如果你想处理中间的空格，那么你需要求助其他技术。比如使用replace()方法或者是用正则表达式替换。1234import res = 'Hello World!'m = re.sub('\s+', ' ', s)print(m) # Prints Hello World! 字符串对齐对于基本的字符串对齐操作，可以使用字符串的ljust(),rjust()和center()方法。123456789text = 'Hello World!'text = text.ljust(20)print(text) # Hello World!print(len(text)) # 20text = 'Hello World!'text = text.rjust(20)print(text) # Hello World!print(len(text)) # 20 所有这些方法都能接受一个可选的填充字符。12345text = 'Hello World!'print(text.rjust(20, '='))print(text.center(20, '*'))# 当格式化多个值的时候，这些格式代码也可以被用在 format() 方法中print('&#123;:+&gt;10s&#125; &#123;:-&gt;10s&#125;'.format('Hello', 'World')) # Prints +++++Hello -----World 字符串中插入变量12345678s = '&#123;name&#125; has &#123;n&#125; messages.'print(s.format(name='Guido', n=37))# 如果要被替换的变量能在变量域中找到，那么你可以结合使用format_map()和vars() 。就像下面这样：s = '&#123;name&#125; has &#123;n&#125; messages.'name = 'Guido'n = 37print(s.format_map(vars())) 以指定列宽格式化字符串123456789101112131415161718192021222324&gt;&gt;&gt; import textwrap&gt;&gt;&gt; print(textwrap.fill(s, 70))Look into my eyes, look into my eyes, the eyes, the eyes, the eyes,not around the eyes, don't look around the eyes, look into my eyes,you're under.&gt;&gt;&gt; print(textwrap.fill(s, 40))Look into my eyes, look into my eyes,the eyes, the eyes, the eyes, not aroundthe eyes, don't look around the eyes,look into my eyes, you're under.&gt;&gt;&gt; print(textwrap.fill(s, 40, initial_indent=' ')) Look into my eyes, look into myeyes, the eyes, the eyes, the eyes, notaround the eyes, don't look around theeyes, look into my eyes, you're under.&gt;&gt;&gt; print(textwrap.fill(s, 40, subsequent_indent=' '))Look into my eyes, look into my eyes, the eyes, the eyes, the eyes, not around the eyes, don't look around the eyes, look into my eyes, you're under. 在字符串中处理html和xml如果你想替换文本字符串中的‘&lt;’或者‘&gt;’，使用html.escape()函数可以很容易的完成。比如：1234567891011&gt;&gt;&gt; s = 'Elements are written as "&lt;tag&gt;text&lt;/tag&gt;".'&gt;&gt;&gt; import html&gt;&gt;&gt; print(s)Elements are written as "&lt;tag&gt;text&lt;/tag&gt;".&gt;&gt;&gt; print(html.escape(s))Elements are written as "&lt;tag&gt;text&lt;/tag&gt;".&gt;&gt;&gt; # Disable escaping of quotes&gt;&gt;&gt; print(html.escape(s, quote=False))Elements are written as "&lt;tag&gt;text&lt;/tag&gt;".&gt;&gt;&gt; 如果你接收到了一些含有编码值的原始文本，需要手动去做替换，通常你只需要使用HTML或者XML解析器的一些相关工具函数/方法即可。比如：1234567891011&gt;&gt;&gt; s = 'Spicy "Jalapeño".'&gt;&gt;&gt; from html.parser import HTMLParser&gt;&gt;&gt; p = HTMLParser()&gt;&gt;&gt; p.unescape(s)'Spicy "Jalapeño".'&gt;&gt;&gt;&gt;&gt;&gt; t = 'The prompt is &gt;&gt;&gt;'&gt;&gt;&gt; from xml.sax.saxutils import unescape&gt;&gt;&gt; unescape(t)'The prompt is &gt;&gt;&gt;'&gt;&gt;&gt; 字节字符串上的字符串操作12345678910&gt;&gt;&gt; data = b'Hello World'&gt;&gt;&gt; data[0:5]b'Hello'&gt;&gt;&gt; data.startswith(b'Hello')True&gt;&gt;&gt; data.split()[b'Hello', b'World']&gt;&gt;&gt; data.replace(b'Hello', b'Hello Cruel')b'Hello Cruel World'&gt;&gt;&gt; 这些操作同样也适用于字节数组。12345678910&gt;&gt;&gt; data = bytearray(b'Hello World')&gt;&gt;&gt; data[0:5]bytearray(b'Hello')&gt;&gt;&gt; data.startswith(b'Hello')True&gt;&gt;&gt; data.split()[bytearray(b'Hello'), bytearray(b'World')]&gt;&gt;&gt; data.replace(b'Hello', b'Hello Cruel')bytearray(b'Hello Cruel World')&gt;&gt;&gt; 大多数情况下，在文本字符串上的操作均可用于字节字符串。然而，这里也有一些需要注意的不同点。首先，字节字符串的索引操作返回整数而不是单独字符。1234567891011&gt;&gt;&gt; a = 'Hello World' # Text string&gt;&gt;&gt; a[0]'H'&gt;&gt;&gt; a[1]'e'&gt;&gt;&gt; b = b'Hello World' # Byte string&gt;&gt;&gt; b[0]72&gt;&gt;&gt; b[1]101&gt;&gt;&gt; 第二点，字节字符串不会提供一个美观的字符串表示，也不能很好的打印出来，除非它们先被解码为一个文本字符串。123456&gt;&gt;&gt; s = b'Hello World'&gt;&gt;&gt; print(s)b'Hello World' # Observe b'...'&gt;&gt;&gt; print(s.decode('ascii'))Hello World&gt;&gt;&gt; 参考文献[1] A Byte of Python3[2] python3-cookbook]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7.1 切换命令行模式与桌面模式]]></title>
    <url>%2F2015%2F06%2F22%2FCentOS-7-1-switch-in-command-line-mode-and-desktop-mode%2F</url>
    <content type="text"><![CDATA[首先你需要知道自己的Linux版本信息，下面介绍一些常用的查看Linux系统版本的命令 查看内核版本命令，以下三个命令任选 12345678[hadoop@localhost ~]$ cat /proc/versionLinux version 3.10.0-229.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) ) #1 SMP Fri Mar 6 11:36:42 UTC 2015[hadoop@localhost ~]$ uname -aLinux localhost.localdomain 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux[hadoop@localhost ~]$ uname -r3.10.0-229.el7.x86_64 查看linux版本 12[hadoop@localhost ~]$ cat /etc/redhat-release CentOS Linux release 7.1.1503 (Core) 那么知道了版本之后如何修改默认启动时进入命令行还是桌面环境呢？在CentOS7.x之前的版本都是通过修改/etc/inittab文件来设置启动顺序，具体可参考这里。但是此种方法并不适应于CentOS7.x版本，在该版本中，我们查看/etc/inittab文件可得1234567891011121314151617[hadoop@localhost ~]$ vim /etc/inittab# inittab is no longer used when using systemd.## ADDING CONFIGURATION HERE WILL HAVE NO EFFECT ON YOUR SYSTEM.## Ctrl-Alt-Delete is handled by /usr/lib/systemd/system/ctrl-alt-del.target## systemd uses 'targets' instead of runlevels. By default, there are two main targets:## multi-user.target: analogous to runlevel 3# graphical.target: analogous to runlevel 5## To view current default target, run:# systemctl get-default## To set a default target, run:# systemctl set-default TARGET.target 该文件中已经详细说明了，不再使用inittab文件而是使用systemd代替，并且还指出，现在只有multi-user相当于运行级别是3和graphical相当于运行级别是5，现在可以使用如下命令设置默认启动级别了，注意要以root用户，或者是使用sudo权限1234[root@localhost hadoop]# systemctl set-default multi-user.targetrm '/etc/systemd/system/default.target'ln -s '/usr/lib/systemd/system/multi-user.target' '/etc/systemd/system/default.target'[root@localhost hadoop]# 设置成功之后reboot一下，即可顺利进入命令行界面了，如果想要再次进入图形界面，在命令行中运行startx即可。]]></content>
      <categories>
        <category>Operating System</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring基础知识汇总]]></title>
    <url>%2F2015%2F06%2F21%2FBasic-knowledge-summary-of-Spring%2F</url>
    <content type="text"><![CDATA[Spring简介Spring框架由Rod Johnson开发，Rod Johnson’s twitter，Rod Johnson’s 百度百科，2004年发布了Spring框架的第一版。Spring是一个从实际开发中抽取出来的框架，因此它完成了大量开发中的通用步骤，留给开发者的仅仅是与特定应用相关的部分，从而大大提高了企业应用的开发效率。 Spring总结起来优点如下 低侵入式设计，代码的污染极低 独立于各种应用服务器，基于Spring框架的应用，可以真正实现Write Once，Run Anywhere的承诺 Spring的IoC容器降低了业务对象替换的复杂性，提高了组件之间的解耦 Spring的AOP支持允许将一些通用任务如安全、事务、日志等进行集中式管理，从而提供了更好的复用 Spring的ORM和DAO提供了与第三方持久层框架的良好整合，并简化了底层的数据库访问 Spring的高度开放性，并不强制应用完全依赖于Spring，开发者可自由选用Spring框架的部分或全部 Spring框架的组成结构图如下所示 Spring的核心机制管理Bean程序主要是通过Spring容器来访问容器中的Bean，ApplicationContext是Spring容器最常用的接口，该接口有如下两个实现类 ClassPathXmlApplicationContext: 从类加载路径下搜索配置文件，并根据配置文件来创建Spring容器 FileSystemXmlApplicationContext: 从文件系统的相对路径或绝对路径下去搜索配置文件，并根据配置文件来创建Spring容器 1234567public class BeanTest&#123; public static void main(String args[]) throws Exception&#123; ApplicationContext ctx = new ClassPathXmlApplicationContext("beans.xml"); Person p = ctx.getBean("person", Person.class); p.say(); &#125;&#125; Eclipse使用Spring在Eclipse等IDE工具中，用户可以自建User Library，然后把Spring的Jar包都放入其中，当然也可以将Jar包直接放在项目的/WEB-INF/lib目录下，但是如果使用User Library，在项目发布时，需要将用户库所引用的Jar文件随应用一起发布，就是将User Library所使用的Jar复制到/WEB-INF/lib目录下，这是因为对于一个Web应用，Eclipse部署Web应用时不会将用户库的Jar文件复制到/WEB-INF/lib下，需要手动复制。 依赖注入Spring框架的核心功能有两个 Spring容器作为超级大工厂，负责创建、管理所有的Java对象，这些Java对象被称为Bean Spring容器管理容器中Bean之间的依赖关系，Spring使用一种被称为“依赖注入”的方式来管理Bean之间的依赖关系 使用依赖注入，不仅可以为Bean注入普通的属性值，还可以注入其他Bean的引用。依赖注入是一种优秀的解耦方式，其可以让Bean以配置文件组织在一起，而不是以硬编码的方式耦合在一起。 理解依赖注入Rod Johnson是第一个高度重视以配置文件来管理Java实例的协作关系的人，他给这种方式起了一个名字：控制反转（Inverse of Control，IoC）。后来Martine Fowler为这种方式起了另一个名称：依赖注入（Dependency Injection），因此不管是依赖注入，还是控制反转，其含义完全相同。当某个Java对象（调用者）需要调用另一个Java对象（被依赖对象）的方法时，在传统模式下通常有两种做法 原始做法: 调用者主动创建被依赖对象，然后再调用被依赖对象的方法 简单工厂模式: 调用者先找到被依赖对象的工厂，然后主动通过工厂去获取被依赖对象，最后再调用被依赖对象的方法 注意上面的主动二字，这必然会导致调用者与被依赖对象实现类的硬编码耦合，非常不利于项目升级的维护。使用Spring框架之后，调用者无需主动获取被依赖对象，调用者只要被动接受Spring容器为调用者的成员变量赋值即可，由此可见，使用Spring后，调用者获取被依赖对象的方式由原来的主动获取，变成了被动接受——所以Rod Johnson称之为控制反转。 另外从Spring容器的角度来看，Spring容器负责将被依赖对象赋值给调用者的成员变量——相当于为调用者注入它依赖的实例，因此Martine Fowler称之为依赖注入。 设值注入设值注入是指IoC容器通过成员变量的setter方法来注入被依赖对象。这种注入方式简单、直观，因而在Spring的依赖注入里大量使用。 构造注入利用构造器来设置依赖关系的方式，被称为构造注入。通俗来说，就是驱动Spring在底层以反射方式执行带指定参数的构造器，当执行带参数的构造器时，就可利用构造器参数对成员变量执行初始化——这就是构造注入的本质。 两种注入方式的对比设值注入有如下优点 与传统的JavaBean的写法更相似，程序开发人员更容易理解、接受。通过setter方法设定依赖关系显得更加直观、自然 对于复杂的依赖关系，如果采用构造注入，会导致构造器过于臃肿，难以阅读。Spring在创建Bean实例时，需要同时实例化其依赖的全部实例，因而导致性能下降。而使用设值注入，则能避免这些问题。 尤其在某些成员变量可选的情况下，多参数的构造器更加笨重 构造注入优势如下 构造注入可以在构造器中决定依赖关系的注入顺序，优先依赖的优先注入 对于依赖关系无需变化的Bean，构造注入更有用处。因为没有setter方法，所有的依赖关系全部在构造器内设定，无须担心后续的代码对依赖关系产生破坏 依赖关系只能在构造器中设定，则只有组件的创建者才能改变组件的依赖关系，对组件的调用者而言，组件内部的依赖关系完全透明，更符合高内聚的原则 Notes建议采用设值注入为主，构造注入为辅的注入策略。对于依赖关系无须变化的注入，尽量采用构造注入；而其他依赖关系的注入，则考虑采用设值注入。 Spring容器中的Bean对于开发者来说，开发者使用Spring框架主要是做两件事：①开发Bean；②配置Bean。对于Spring框架来说，它要做的就是根据配置文件来创建Bean实例，并调用Bean实例的方法完成“依赖注入”——这就是所谓IoC的本质。 容器中Bean的作用域当通过Spring容器创建一个Bean实例时，不仅可以完成Bean实例的实例化，还可以为Bean指定特定的作用域。Spring支持如下五种作用域 singleton: 单例模式，在整个Spring IoC容器中，singleton作用域的Bean将只生成一个实例 prototype: 每次通过容器的getBean()方法获取prototype作用域的Bean时，都将产生一个新的Bean实例 request: 对于一次HTTP请求，request作用域的Bean将只生成一个实例，这意味着，在同一次HTTP请求内，程序每次请求该Bean，得到的总是同一个实例。只有在Web应用中使用Spring时，该作用域才真正有效 对于一次HTTP会话，session作用域的Bean将只生成一个实例，这意味着，在同一次HTTP会话内，程序每次请求该Bean，得到的总是同一个实例。只有在Web应用中使用Spring时，该作用域才真正有效 global session: 每个全局的HTTP Session对应一个Bean实例。在典型的情况下，仅在使用portlet context的时候有效，同样只在Web应用中有效 如果不指定Bean的作用域，Spring默认使用singleton作用域。prototype作用域的Bean的创建、销毁代价比较大。而singleton作用域的Bean实例一旦创建成果，就可以重复使用。因此，应该尽量避免将Bean设置成prototype作用域。 使用自动装配注入合作者BeanSpring能自动装配Bean与Bean之间的依赖关系，即无须使用ref显式指定依赖Bean，而是由Spring容器检查XML配置文件内容，根据某种规则，为调用者Bean注入被依赖的Bean。Spring自动装配可通过&lt;beans/&gt;元素的default-autowire属性指定，该属性对配置文件中所有的Bean起作用；也可通过对&lt;bean/&gt;元素的autowire属性指定，该属性只对该Bean起作用。 autowire和default-autowire可以接受如下值 no: 不使用自动装配。Bean依赖必须通过ref元素定义。这是默认配置，在较大的部署环境中不鼓励改变这个配置，显式配置合作者能够得到更清晰的依赖关系 byName: 根据setter方法名进行自动装配。Spring容器查找容器中全部Bean，找出其id与setter方法名去掉set前缀，并小写首字母后同名的Bean来完成注入。如果没有找到匹配的Bean实例，则Spring不会进行任何注入 byType: 根据setter方法的形参类型来自动装配。Spring容器查找容器中的全部Bean，如果正好有一个Bean类型与setter方法的形参类型匹配，就自动注入这个Bean；如果找到多个这样的Bean，就抛出一个异常；如果没有找到这样的Bean，则什么都不会发生，setter方法不会被调用 constructor: 与byType类似，区别是用于自动匹配构造器的参数。如果容器不能恰好找到一个与构造器参数类型匹配的Bean，则会抛出一个异常 autodetect: Spring容器根据Bean内部结构，自行决定使用constructor或byType策略。如果找到一个默认的构造函数，那么就会应用byType策略 当一个Bean既使用自动装配依赖，又使用ref显式指定依赖时，则显式指定的依赖覆盖自动装配依赖；对于大型的应用，不鼓励使用自动装配。虽然使用自动装配可减少配置文件的工作量，但大大将死了依赖关系的清晰性和透明性。依赖关系的装配依赖于源文件的属性名和属性类型，导致Bean与Bean之间的耦合降低到代码层次，不利于高层次解耦 12345&lt;!--通过设置可以将Bean排除在自动装配之外--&gt;&lt;bean id="" autowire-candidate="false"/&gt;&lt;!--除此之外，还可以在beans元素中指定，支持模式字符串，如下所有以abc结尾的Bean都被排除在自动装配之外--&gt;&lt;beans default-autowire-candidates="*abc"/&gt; 创建Bean的3种方式使用构造器创建Bean实例使用构造器来创建Bean实例是最常见的情况，如果不采用构造注入，Spring底层会调用Bean类的无参数构造器来创建实例，因此要求该Bean类提供无参数的构造器。 采用默认的构造器创建Bean实例，Spring对Bean实例的所有属性执行默认初始化，即所有的基本类型的值初始化为0或false；所有的引用类型的值初始化为null。 使用静态工厂方法创建Bean使用静态工厂方法创建Bean实例时，class属性也必须指定，但此时class属性并不是指定Bean实例的实现类，而是静态工厂类，Spring通过该属性知道由哪个工厂类来创建Bean实例。 除此之外，还需要使用factory-method属性来指定静态工厂方法，Spring将调用静态工厂方法返回一个Bean实例，一旦获得了指定Bean实例，Spring后面的处理步骤与采用普通方法创建Bean实例完全一样。如果静态工厂方法需要参数，则使用&lt;constructor-arg.../&gt;元素指定静态工厂方法的参数。 调用实例工厂方法创建Bean实例工厂方法与静态工厂方法只有一个不同：调用静态工厂方法只需使用工厂类即可，而调用实例工厂方法则需要工厂实例。使用实例工厂方法时，配置Bean实例的&lt;bean.../&gt;元素无须class属性，配置实例工厂方法使用factory-bean指定工厂实例。采用实例工厂方法创建Bean的&lt;bean.../&gt;元素时需要指定如下两个属性 factory-bean: 该属性的值为工厂Bean的id factory-method: 该属性指定实例工厂的工厂方法 若调用实例工厂方法时需要传入参数，则使用&lt;constructor-arg.../&gt;元素确定参数值。 协调作用域不同步的Bean当singleton作用域的Bean依赖于prototype作用域的Bean时，会产生不同步的现象，原因是因为当Spring容器初始化时，容器会预初始化容器中所有的singleton Bean，由于singleton Bean依赖于prototype Bean，因此Spring在初始化singleton Bean之前，会先创建prototypeBean——然后才创建singleton Bean，接下里将prototype Bean注入singleton Bean。解决不同步的方法有两种 放弃依赖注入: singleton作用域的Bean每次需要prototype作用域的Bean时，主动向容器请求新的Bean实例，即可保证每次注入的prototype Bean实例都是最新的实例 利用方法注入: 方法注入通常使用lookup方法注入，使用lookup方法注入可以让Spring容器重写容器中Bean的抽象或具体方法，返回查找容器中其他Bean的结果，被查找的Bean通常是一个non-singleton Bean。Spring通过使用JDK动态代理或cglib库修改客户端的二进制码，从而实现上述要求 建议采用第二种方法，使用方法注入。为了使用lookup方法注入，大致需要如下两步 将调用者Bean的实现类定义为抽象类，并定义一个抽象方法来获取被依赖的Bean 在&lt;bean.../&gt;元素中添加&lt;lookup-method.../&gt;子元素让Spring为调用者Bean的实现类实现指定的抽象方法 Notes Spring会采用运行时动态增强的方式来实现&lt;lookup-method.../&gt;元素所指定的抽象方法，如果目标抽象类实现过接口，Spring会采用JDK动态代理来实现该抽象类，并为之实现抽象方法；如果目标抽象类没有实现过接口，Spring会采用cglib实现该抽象类，并为之实现抽象方法。Spring4.0的spring-core-xxx.jar包中已经集成了cglib类库。 两种后处理器Spring提供了两种常用的后处理器 Bean后处理器: 这种后处理器会对容器中Bean进行后处理，对Bean进行额外加强 容器后处理器: 这种后处理器会对IoC容器进行后处理，用于增强容器功能 Bean后处理器Bean后处理器是一种特殊的Bean，这种特殊的Bean并不对外提供服务，它甚至可以无须id属性，它主要负责对容器中的其他Bean执行后处理，例如为容器中的目标Bean生成代理等，这种Bean称为Bean后处理器。Bean后处理器会在Bean实例创建成功之后，对Bean实例进行进一步的增强处理。Bean后处理器必须实现BeanPostProcessor接口，同时必须实现该接口的两个方法。 Object postProcessBeforeInitialization(Object bean, String name) throws BeansException: 该方法的第一个参数是系统即将进行后处理的Bean实例，第二个参数是该Bean的配置id Object postProcessAfterinitialization(Object bean, String name) throws BeansException: 该方法的第一个参数是系统即将进行后处理的Bean实例，第二个参数是该Bean的配置id 容器中一旦注册了Bean后处理器，Bean后处理器就会自动启动，在容器中每个Bean创建时自动工作，Bean后处理器两个方法的回调时机如下图 注意一点，如果使用BeanFactory作为Spring容器，则必须手动注册Bean后处理器，程序必须获取Bean后处理器实例，然后手动注册。123BeanPostProcessor bp = (BeanPostProcessor)beanFactory.getBean("bp");beanFactory.addBeanPostProcessor(bp);Person p = (Person)beanFactory.getBean("person"); 容器后处理器Bean后处理器负责处理容器中的所有Bean实例，而容器后处理器则负责处理容器本身。容器后处理器必须实现BeanFactoryPostProcessor接口，并实现该接口的一个方法postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory)实现该方法的方法体就是对Spring容器进行的处理，这种处理可以对Spring容器进行自定义扩展，当然也可以对Spring容器不进行任何处理。 类似于BeanPostProcessor，ApplicationContext可自动检测到容器中的容器后处理器，并且自动注册容器后处理器。但若使用BeanFactory作为Spring容器，则必须手动调用该容器后处理器来处理BeanFactory容器。 Spring的“零配置”支持搜索Bean类Spring提供如下几个Annotation来标注Spring Bean @Component: 标注一个普通的Spring Bean类 @Controller: 标注一个控制器组件类 @Service: 标注一个业务逻辑组件类 @Repository: 标注一个DAO组件类 在Spring配置文件中做如下配置，指定自动扫描的包1&lt;context:component-scan base-package="edu.shu.spring.domain"/&gt; 使用@Resource配置依赖@Resource位于javax.annotation包下，是来自JavaEE规范的一个Annotation，Spring直接借鉴了该Annotation，通过使用该Annotation为目标Bean指定协作者Bean。使用@Resource与&lt;property.../&gt;元素的ref属性有相同的效果。@Resource不仅可以修饰setter方法，也可以直接修饰实例变量，如果使用@Resource修饰实例变量将会更加简单，此时Spring将会直接使用JavaEE规范的Field注入，此时连setter方法都可以不要。 使用@PostConstruct和@PreDestroy定制生命周期行为@PostConstruct和@PreDestroy同样位于javax.annotation包下，也是来自JavaEE规范的两个Annotation，Spring直接借鉴了它们，用于定制Spring容器中Bean的生命周期行为。它们都用于修饰方法，无须任何属性。其中前者修饰的方法时Bean的初始化方法；而后者修饰的方法时Bean销毁之前的方法。 Spring4.0增强的自动装配和精确装配Spring提供了@Autowired注解来指定自动装配，@Autowired可以修饰setter方法、普通方法、实例变量和构造器等。当使用@Autowired标注setter方法时，默认采用byType自动装配策略。在这种策略下，符合自动装配类型的候选Bean实例常常有多个，这个时候就可能引起异常，为了实现精确的自动装配，Spring提供了@Qualifier注解，通过使用@Qualifier，允许根据Bean的id来执行自动装配。 Spring的AOP为什么需要AOPAOP（Aspect Orient Programming）也就是面向切面编程，作为面向对象编程的一种补充，已经成为一种比较成熟的编程方式。其实AOP问世的时间并不太长，AOP和OOP互为补充，面向切面编程将程序运行过程分解成各个切面。 AOP专门用于处理系统中分布于各个模块（不同方法）中的交叉关注点的问题，在JavaEE应用中，常常通过AOP来处理一些具有横切性质的系统级服务，如事务管理、安全检查、缓存、对象池管理等，AOP已经成为一种非常常用的解决方案。 使用AspectJ实现AOPAspectJ是一个基于Java语言的AOP框架，提供了强大的AOP功能，其他很多AOP框架都借鉴或采纳其中的一些思想。其主要包括两个部分：一个部分定义了如何表达、定义AOP编程中的语法规范，通过这套语法规范，可以方便地用AOP来解决Java语言中存在的交叉关注点的问题；另一个部分是工具部分，包括编译、调试工具等。 AOP实现可分为两类 静态AOP实现: AOP框架在编译阶段对程序进行修改，即实现对目标类的增强，生成静态的AOP代理类，以AspectJ为代表 动态AOP实现: AOP框架在运行阶段动态生成AOP代理，以实现对目标对象的增强，以Spring AOP为代表 一般来说，静态AOP实现具有较好的性能，但需要使用特殊的编译器。动态AOP实现是纯Java实现，因此无须特殊的编译器，但是通常性能略差。 AOP的基本概念关于面向切面编程的一些术语 切面（Aspect）: 切面用于组织多个Advice，Advice放在切面中定义 连接点（Joinpoint）: 程序执行过程中明确的点，如方法的调用，或者异常的抛出。在Spring AOP中，连接点总是方法的调用 增强处理（Advice）: AOP框架在特定的切入点执行的增强处理。处理有“around”、“before”和“after”等类型 切入点（Pointcut）: 可以插入增强处理的连接点。简而言之，当某个连接点满足指定要求时，该连接点将被添加增强处理，该连接点也就变成了切入点 Spring的AOP支持Spring中的AOP代理由Spring的IoC容器负责生成、管理，其依赖关系也由IoC容器负责管理。为了在应用中使用@AspectJ支持，Spring需要添加三个库 aspectjweaver.jar aspectjrt.jar aopalliance.jar 并在Spring配置文件中做如下配置1234567&lt;!--启动@AspectJ支持--&gt;&lt;aop:aspectj-autoproxy/&gt;&lt;!--指定自动搜索Bean组件、自动搜索切面类--&gt;&lt;context:component-scan base-package="edu.shu.sprint.service"&gt; &lt;context:include-filter type="annotation" expression="org.aspectj.lang.annotation.Aspect"/&gt;&lt;/context:component-scan&gt; 参考文献[1] 轻量级JavaEE企业应用实战-Struts2+Spring4+Hibernate整合开发]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hibernate基础知识汇总]]></title>
    <url>%2F2015%2F06%2F19%2FBasic-knowledge-summary-of-Hibernate%2F</url>
    <content type="text"><![CDATA[ORM简介ORM（Object/Relation Mapping），对象关系映射，ORM是一种规范，主要完成面向对象的编程语言到关系数据库的映射。ORM框架是面向对象程序设计语言与关系数据库发展不同步时的中间解决方案。ORM工具的唯一作用就是：把对持久化对象的保存、删除、修改等操作，转换成对数据库的操作，从此，就可以以面向对象的方式操作持久化对象，而ORM框架则负责转换成对应的SQL操作。 数据源简介数据源是一种提高数据库连接性能的常规手段，数据源会负责维持一个数据连接池，当程序创建数据源实例时，系统会一次性地创建多个数据库连接，并把这些数据库连接保存在连接池中。当程序需要进行数据库访问时，无需重新获得数据库连接，而是从连接池中取出一个空闲的数据库连接。当程序使用数据库连接访问数据库结束后，无需关闭数据库连接，而是将数据库连接归还给连接池即可。通过此种方式，可以避免频繁地获取数据库连接、关闭数据库连接所导致的性能下降。 Hibernate体系结构Hibernate的体系架构如下所示 下面对上图中各对象逐一解释 SessionFactory: 这是Hibernate的关键对象，它是单个数据库映射关系经过编译后的内存镜像，也是线程安全的。它是生成Session的工厂，本身需要依赖于ConnectionProvider。该对象可以在进程或集群的级别上，为那些事务之间可以重用的数据提供可选的二级缓存 Session: 它是应用程序与持久存储层直接交互的一个单线程对象。它也是Hibernate持久化操作的管家对象，所有的持久化对象必须在Session管理下才可以进行持久化操作。此对象生存期很短。它底层封装了JDBC连接，它也是Transaction的工厂 。Session对象持有必选的一级缓存，在显式执行flush之前，所有持久化操作的数据都在缓存中的Session对象处 PO（Persistent Object）: 系统创建的POJO实例，一旦与特定的Session关联，并对应数据表的指定记录，该对象就处于持久化状态，这一系列对象都被称为持久化对象。在程序中对持久化对象执行的修改，都将自动被转换为对持久层的修改。持久化对象完全可以是普通的JavaBeans/POJO，唯一的区别是它们正与一个Session关联 瞬态对象和脱管对象: 系统通过new关键字创建的Java实例，没有与Session关联，此时处于瞬态。瞬态实例可能是被应用程序实例化后，尚未进行持久化的对象。如果一个曾经持久化过的实例，如果Session被关闭则转为脱管状态 事务（Transaction）: 代表一次原子操作，它具有数据库事务的概念。Hibernate事务是对底层具体的JDBC、JTA以及CORBA事务的抽象。在某些情况下，一个Session之间可能包含多个Transaction对象。虽然事务操作是可选的，但所有持久化操作都应该在事务管理下进行，即使是只读操作 连接提供者（ConnectionProvider）: 它是生成JDBC连接的工厂，它通过抽象将应用程序与底层DataSource或DriverManager隔离开。这个对象无需应用程序直接访问，仅在应用程序需要扩展时使用 事务工厂（TransactionFactory）: 它是生成Transaction对象实例的工厂。该对象也无需应用程序直接访问。它负责对底层具体的事务实现进行封装，将底层具体的事务抽象成Hibernate事务 深入理解持久化对象持久化类的要求 提供一个无参数的构造器: 所有的持久化类都应该提供一个无参数的构造器，这个构造器可以不采用public访问控制符。只要提供了无参数的构造器，Hibernate就可以使用Constructor.newInstance()来创建持久化类的实例了。通常构造器的访问控制修饰符至少是包可见的 提供一个标识属性: 标识属性通常映射数据库表的主键字段，对于基本类型，建议使用其对应的包装类型 为持久化类的每个成员变量提供setter和getter方法: Hibernate默认采用属性方式来访问持久化类的成员变量 使用非final的类: 在运行时生成代理是Hibernate的一个重要功能，如果非要使用一个有public final方法的类，则必须通过设置lazy=”false”来明确地禁用代理 重写equals()和hashCode()方法: 如果需要把持久化类的实例放入Set中，则应该为持久化类重写equals()和hashCode()方法 持久化类对象的状态 在Hibernate中，PO（Persistent Object）有如下三种状态 瞬态: 如果PO实例从未与Session关联过，该PO实例处于瞬态状态，瞬态对象不会被持久化到数据库中，也不会被赋予持久化标识 持久化: 如果PO实例与Session关联起来，且该实例对应到数据库记录，则该实例处于持久化状态 脱管: 如果PO实例曾经与Session关联过，但因为Session的关闭等原因，PO实例脱离了Session管理，这种状态为脱管状态，脱管对象的引用仍然有效，对象可继续被修改。如果重新让脱管对象与某个Session关联，这个脱管对象会重新转换为持久化状态，而脱管期间的改动不会丢失，也可被写入数据库 更改持久化对象状态的方法持久化实体 save(): 该方法返回持久化对象的标识属性值（即对应记录的主键值），执行save方法时会立即将持久化对象对应的数据插入数据库 persist(): 该方法保存持久化对象，没有任何返回值，当该方法在一个事务外部被调用时，并不立即转换成insert语句 加载持久化实体 load(): 该方法具有延迟加载功能，其不会立即访问数据库，当试图加载的记录不存在时，load方法可能返回一个未初始化的代理对象 get(): 该方法总是立即访问数据库，当试图加载的记录不存在时，get方法将直接返回null 更新持久化实体程序对持久化实例所做的修改会在Session flush之前被自动保存到数据库，无需程序调用其他方法来将其持久化。也就是说，修改对象最简单的方法就是在Session处于打开状态时load()它，然后直接修改即可。 更新脱管实体当程序使用update()来保存程序对持久化对象所做的修改时，如果不清楚该对象是否曾经持久化过，那么程序可以选择使用updateOrSave()方法，该方法自动判断该对象是否曾经持久化过，如果曾经持久化过，就执行update()操作；否则执行save()操作。merge()方法不会持久化给定的对象，例如session.merge(object)代码后，object依然不是持久化状态，object依然不会被关联到session上，merge()方法会返回object对象的副本——该副本处于持久化状态。 删除持久化实体通过Session的delete()方法来删除持久化实例，一旦删除了该持久化实例，则对应的数据记录也将被删除。 Hibernate进阶Hibernate的关联映射关联关系大致有两类，一类是单向关系，另一类是双向关系，单向关系可分为 单向1-&gt;1 单向1-&gt;N 单向N-&gt;1 单向N-&gt;N 双向关联又可分为 双向1-&gt;1 双向1-&gt;N 双向N-&gt;N 单向N-1关联单向的N-1关联只需从N的一端可以访问1的一端，程序应该在N的一端的持久化类中增加一个属性，该属性引用1的一端的关联实体，对于N-1关联（不管是单向，还是双向）都需要在N的一端使用@ManyToOne修饰代表关联实体的属性。 单向1-1关联单向的1-1关联关系，需要在持久化类里增加代表关联实体的成员变量，并为该成员变量增加setter和getter方法。从持久化类的代码上看，单向1-1与单向N-1没有丝毫区别。因为N的一端或者1 的一端都是直接访问关联实体，只需增加代表关联实体的属性即可。对于1-1关联（不管是单向关联，还是双向关联），都需要使用@OneToOne修饰代表关联实体的属性。 单向1-N关联单向1-N关联的持久化类发生了改变，持久化类里需要使用集合属性。因为1的一端需要访问N的一端，而N的一端将以集合（Set）形式表现。对于单向1-N关联关系，只需要在1的一端增加Set类型的成员变量，该成员变量记录当前实体所有的关联实体。使用@OneToMany注解修饰1的一端对应N的一端的集合。 单向N-N关联单向的N-N关联和1-N关联的持久化类代码完全相同，控制关系的一端需要增加一个Set类型的属性，被关联的持久化实例以集合形式存在。N-N关联必须使用连接表，N-N关联与有连接表的1-N关联非常相似，因此都需要使用@JoinTable来映射连接表，区别是N-N关联要去掉@JoinTable注解的inverseJoinColumns属性所指定的@JoinColumn中的unique=true。 双向1-N关联对于1-N关联，Hibernate推荐使用双向关联，而且不要让1的一端控制关联关系，而使用N的一端控制关联关系。双向的1-N关联与N-1关联是完全相同的两种情形，两端都需要增加对关联属性的访问，N的一端增加引用到关联实体的属性，1的一端增加集合属性，集合元素为关联实体。无连接表的双向1-N关联，N的一端需要增加@ManyToOne注解来修饰代表关联实体的属性，而1的一端则需要使用@OneToMany注解来修饰代表关联实体的属性。对于有连接表的双向1-N关联而言，1的一端无需任何改变；只要在N的一端使用@JoinTable显式指定连接表即可。 双向N-N关联双向N-N关联需要两端都使用Set集合属性，两端都增加对集合属性的访问。双向N-N关联没有太多选择，只能采用连接表来建立两个实体之间的关联关系。双向N-N关联需要在两端分别使用@ManyToMany修饰Set集合属性，并在两端都使用@JoinTable显式映射连接表。在两端映射连接表时，两端指定的连接表的表明应该相同，而且两端使用@JoinTable时指定的外键列的列名也是相互对应的。如果某一段想放弃控制关联关系，则可在这一段的@ManyToMany注解中指定mappedBy属性，这一段就无需、也不能使用@JoinTable映射连接表了。 双向1-1关联双向1-1关联需要使用@OneToOne注解进行映射，并让两个持久化类都增加引用关联实体的属性，并为该属性提供setter和getter方法。 基于复合主键的关联关系在实际项目中并不推荐使用复合主键，总是建议采用没有物理意义的逻辑主键。复合主键的做法不仅会增加数据库建模的难度，而且会增加关联关系的维护成本。 持久化的传播性对于关联实体而言，Hibernate默认不会启用级联操作，当父对象被保存时，它关联的子实体不会被保存；父对象被删除时，它关联的子实体不会被删除。为了启用不同持久化操作的级联关系，Hibernate定义了如下级联风格 CascadeType.ALL: 指定Hibernate将所所有的持久化操作都级联到关联实体 CascadeType.MERGE: 指定Hibernate将merge操作级联到关联实体 CascadeType.PERSIST: 指定Hibernate将persist操作级联到关联实体 CascadeType.REFRESH: 指定Hibernate将refresh操作及镰刀关联实体 CascadeType.REMOVE: 指定Hibernate将remove操作级联到关联实体如果程序希望某个操作能被级联传播到关联实体，则可以在配置@OneToMany、@OneToOne、@ManyToMany时通过cascade属性来指定。注意级联风格是可组合的。 事务控制Session与事务SessionFactory对象的创建代价很高，它是线程安全的对象，被设计成可以被所有线程所共享。通常，SessionFactory会在应用程序启动时创建，一旦创建了SessionFactory就不会轻易关闭，只有当应用退出时才关闭SessionFactory。Session对象是轻量级的，它也是线程不安全的。对于单个业务进程、单个工作单元而言，Session只被使用一次。创建Session时，并不会立即打开与数据库之间的连接，只有需要进行数据库操作时，Session才会获取JDBC连接。因此，打开和关闭Session，并不会对性能造成很大的影响。 一级、二级缓存Hibernate包括两个级别的缓存 默认总是启用的Session级别的一级缓存 可选的SessionFactory级别的二级缓存 其中Session级别的一级缓存不需要开发者关心，默认总是有效的，当应用保存持久化实体、修改持久化实体时，Session并不会立即把这种改变flush到数据库，而是缓存在当前Session的一级缓存中，除非程序显式调用Session的flush()方法，或程序关闭Session时才会把这些改变一次性的flush到底层数据库。 SessionFactory级别的二级缓存是全局性的，应用的所有Session都共享这个二级缓存。不过SessionFactory级别的二级缓存默认是关闭的，必须由程序显式开启。一旦在应用程序中开启了二级缓存，当Session需要抓取数据时，Session将会先查找一级缓存，再查找二级缓存，只有当一级缓存和二级缓存中都没有要抓取的数据时，才会去查找底层数据库。 查询缓存一级、二级缓存都是对整个实体进行缓存，它不会缓存普通属性，如果想对普通属性进行缓存，则可以考虑使用查询缓存。 Notes 需要指出的是，在大部分情况下查询缓存并不能提高应用性能，甚至反而会降低应用性能，因此在实际项目中请慎重使用查询缓存。 对于查询缓存来说，它缓存的key就是查询所用的HQL或SQL语句。需要指出的是，查询缓存不仅要求所使用的HQL或SQL语句相同，甚至要求所传入的参数也相同，Hibernate才能直接从查询缓存中取得数据。 查询缓存默认是关闭的，为了开启需要在hibernate.cfg.xml中配置1&lt;property name="hibernate.cache.use_query_cache"&gt;true&lt;/property&gt; 除此之外，在程序中还必须调用Query对象的setCacheable(true)才会对查询结果进行缓存。12//设置查询缓存，在第二次查询时不会重新发出SQL语句进行查询session.createQuery("select name, age from person").setCacheable(true); 事件机制在Hibernate执行持久化的过程中，应用程序通常都无法参与其中。所有的数据持久化操作，对用户都是透明的，用户无法插入自己的动作。通过事件框架，Hibernate允许应用程序能响应特定的内部事件，从而允许实现某些通用的功能，或者对Hibernate功能进行扩展。Hibernate的事件框架由两部分组成 拦截器机制: 对于特定动作拦截，回调应用中的特定动作 事件系统: 重写Hibernate的事件监听器 拦截器拦截器通过Interceptor接口，可以从Session中回调应用程序的特定方法，这种回调机制可让应用程序在持久化对象被保存、更新、删除或加载之前，检查并修改其属性。通过Interceptor接口，可以在数据进入数据库之前，对数据进行最后的检查，如果数据不符合要求，则可以修改数据，从而避免非法数据进入数据库。使用拦截器按如下步骤进行 定义实现Interceptor接口的拦截器类 通过Session启用拦截器，或者通过Configuration启用全局拦截器 事件系统Hibernate的事件系统完全可以替代拦截器，也可以作为拦截器的补充来使用。使用事件系统按如下步骤进行 实现自己的事件监听器类 注册自定义事件监听器，代替系统默认的事件监听器 实现用户的自定义监听器有如下三种方法 实现对应的监听器接口: 实现接口必须实现接口内的所有方法，关键是必须实现Hibernate对应的持久化操作，即数据库访问，这以为这程序员完全取代了Hibernate的底层操作 继承事件适配器: 可以有选择则行地实现需要关注的方法，但依然试图取代Hibernate完成数据库的访问 继承系统默认的事件监听器: 扩展特定方法 通常推荐采用第三种方法实现自己的事件监听器。Hibernate默认的事件监听器都被声明成non-final，以便用户继承他们。 参考文献[1] 轻量级JavaEE企业应用实战-Struts2+Spring4+Hibernate整合开发]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Hibernate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Struts2基础知识汇总]]></title>
    <url>%2F2015%2F06%2F18%2FBasic-knowledge-summary-of-Struts2%2F</url>
    <content type="text"><![CDATA[Struts2简介Struts2是由传统的Struts1、WebWork两个经典的MVC框架发展起来，如下图所示，无论从Struts2设计的角度还是在实际项目中的易用性来看，Struts2都是一个非常优秀的MVC框架，当然目前还有另外一个非常优秀的MVC框架——SpringMVC，以后再对它进行介绍。 实现ActionStruts2的Action类是一个普通的POJO（通常应该包含一个无参数的execute方法），Struts2直接使用Action来封装HTTP请求参数，因此，Action类里还应该包含与请求参数对应的实例变量，并且为这些实例变量提供对应的setter和getter方法。注意其实实例变量是可以省略的，因为Struts2是通过对应的setter和getter方法来处理请求参数的，而不是通过实例变量名来处理请求参数的。 Action访问Servlet API123ActionContext ctx = ActionContext.getContext();//相当于JSP中内置的requestctx.getApplication();//返回Map对象，该对象模拟了ServletContext实例，相当于JSP中内置的applicationctx.getSession();//返回Map对象，该对象模拟了HttpSession实例，相当于JSP中内置的session Action直接访问Servlet APIStruts2提供了几个接口供我们直接访问ServletAPI ServletContextAware: 实现该接口的Action可以直接访问Web应用的ServletContext实例 ServletRequestAware: 实现该接口的Action可以直接访问用户请求的HttpServletRequest实例 ServletResponseAware: 实现该接口的Action可以直接访问服务器响应的HttpServletResponse实例 使用ServletActionContext访问Servlet APIStruts2还提供了一个ServletActionContext工具类，这个类包含如下几个静态方法 static PageContext getPageContext(): 取得Web应用的PageContext对象 static HttpServletRequest getRequest(): 取得Web应用的HttpServletRequest对象 static HttpServletResponse getResponse(): 取得Web应用的HttpServletResponse对象 static ServletContext getServletContext(): 取得Web应用的ServletContext对象 Action包和命名空间在struts.xml中配置Action，如果没有指定namespace属性，那么该包使用默认的命名空间，如果namespace=”/“表示指定根命名空间。默认命名空间里的Action可以处理任何命名空间下的Action请求，但根命名空间下的Action只处理根命名空间下的Action请求，这是根命名空间和默认命名空间的区别。 配置默认Action1234567&lt;package name="..." extends="struts-default"&gt; &lt;default-action-ref name="simpleViewAction"/&gt; //当用户请求找不到对应的Action时，系统默认的Action将处理用户请求 &lt;action name="simpleViewAction" class="..."&gt; &lt;result.../&gt; &lt;/action&gt;&lt;/package&gt; 将默认Action配置在默认命名空间里就可以让该Action处理所有用户请求，因为默认命名空间的Action可以处理任何命名空间的请求。 Struts2内建的支持结果类型 chain: Action链式处理的结果类型 dispatcher: 用于指定使用JSP作为视图的结果类型，默认值 freemarker: 用于指定使用FreeMarker模板作为视图的结果类型 httpheader: 用于控制特殊的HTTP行为的结果类型 redirect: 用于直接跳转到其他URL的结果类型 redirectAction: 用于直接跳转到其他Action的结果类型 stream: 用于向浏览器返回一个InputStream velocity: 用于指定使用Velocity模板作为视图的结果类型 xslt: 用于与XML/XSLT整合的结果类型 plainText: 用于显示某个页面的原始代码的结果类型 Notes dispatcher结果类型是将请求forward（转发）到指定的JSP资源；而redirect结果类型，是将请求redirect（重定向）到指定的视图资源，重定向会丢失所有的请求参数、请求属性——当然也丢失了Action的处理结果。 使用redirect类型的结果时，不能重定向到/WEB-INF/路径下任何资源，因为重定向相当于重新发送请求，而Web应用的/WEB-INF/路径下资源是受保护资源。 使用redirectAction结果类型时，系统将重新生成一个新请求，只是该请求的URL是一个Action，因此前一个Action处理结果、请求参数、请求属性都会丢失。 Struts2异常处理Struts2开启异常映射默认的在struts-default.xml中已经开启了异常映射，代码如下12345678910&lt;interceptors&gt; &lt;!--配置异常处理的拦截器--&gt; &lt;interceptor name="exception" class="com.opensymphony.xwork.interceptor.ExceptionMapping.Interceptor"/&gt; &lt;interceptor-stakc name="defaultStack"&gt; ... &lt;!--加入默认的拦截器栈--&gt; &lt;interceptor-ref name="exception"/&gt; ... &lt;/interceptor-stakc&gt;&lt;/interceptors&gt; Struts2输出异常信息&lt;s:property value=&quot;exception&quot;/&gt;: 输出异常对象本身&lt;s:property value=&quot;exceptionStck&quot;/&gt;: 输出异常堆栈信息&lt;s:property value=&quot;exception.message&quot;/&gt;: 输出异常的message消息 Convention插件与“约定”支持有Ruby on Rails开发经验的朋友知道Rails有一条重要原则：约定优于配置。Rails开发者只需要按约定开发ActiveRecord/ActiveController即可，无需进行配置。Struts2的Convention插件借鉴了Rails的创意。 Action的搜索和映射约定使用Convention插件，将Struts2项目下的struts2-convention-plugin-2.3.16.3.jar复制到项目的WEB-INF/lib下即可。对于Convention插件而言，它会自动搜索位于action、actions、struts、struts2包下的所有Java类，Convention插件会把如下两种Java类当成Action处理。 所有实现了com.opensymphony.xwork2.Action的Java类 所有类名以Action结尾的Java类 Struts2的Convention插件还允许设置如下三个常量 struts.convention.exclude.packages: 指定不扫描哪些包下的Java类，位于这些包结构下的Java类将不会被自动映射成Action struts.convention.package.locators: Convention插件使用该常量指定的包作为搜寻Action的根包。例如actions.base.LoginAction类，按约定原本映射到/base/login；如果将该常量设置为base，则该Action将会映射到/login struts.convention.action.packages: Convention插件以该常量指定包作为根包来搜索Action类，Convention插件除了扫描action、actions、struts、struts2四个包的类之外，还会扫描该常量指定的一个或多个包，Convention会视图从中发现Action类 部署Action时，action、actions、struts、struts2包会映射成根（/）命名空间，而这些包下的子包则被映射成对应的命名空间。例如：edu.shu.action.base.LoginAction 映射到/base/命名空间 Action的那么属性（即该Action处理的URL）根据Action的类名映射，需遵循如下两步规则： 如果该Action类名包含Action后缀，将该Acting类名的Action后缀去掉，否则不做任何处理 将Action类名的驼峰写法转成中划线写法，所有字母小写 例如edu.shu.action.base.UserLoginAction映射的URL是/base/user-login.action 按约定映射ResultConvention默认也为作为逻辑视图和物理视图之间的映射提供了约定，默认情况下，Convention总会到Web应用的WEB-INF/content路径下定位物理资源，定位资源的约定是actionName+resultcode+suffix。当某个逻辑视图找不到对应的视图资源时，Convention会自动视图使用actionName+sufix作为物理视图资源。 Action的URL 返回的逻辑视图名 结果类型 对应的物理视图 /login success Dispatcher \WEB-INF\content\login-success.jsp /login success Dispatcher \WEB-INF\content\login-success.html /login success Dispatcher \WEB-INF\content\login.jsp /login success Dispatcher \WEB-INF\content\login.html /shu/get-book error FreeMarker \WEB-INF\content\shu\get-book-error.ftl /shu/get-book error FreeMarker \WEB-INF\content\shu\get-book.ftl /shu/get-book input Velocity \WEB-INF\content\shu\get-book-input.vm /shu/get input Velocity \WEB-INF\content\shu\get-input.vm Action链的约定如果希望一个Action处理结束后不是进入视图页面，而是进入另一个Action形成的Action链，则通过Convention插件只需遵守如下三个约定 第一个Action返回的逻辑视图字符串没有对应的视图资源 第二个Action与第一个Action处于同一个包下 第二个Action映射的URL为: firstactionName+resultcode 自动重加载映射在struts.xml中配置如下两个常量即可使Convention插件重加载映射12&lt;constant name="struts.devMode" value="true"&gt;&lt;constant name="struts.convention.classes.reload" value="true"&gt; Struts2标签库Struts2标签库概述从最大的范围来分，Struts2可以将所有标签分成如下三类 UI标签: 主要用于生成HTML元素的标签1.1. 表单标签: 主要用于生成HTML页面的form元素，以及普通表单元素的标签1.2. 非表单元素: 主要用于生成页面的树、Tab页等标签 非UI标签: 主要用于数据访问、逻辑控制等的标签2.1 流程控制标签: 主要包含用于实现分支、循环等流程控制的标签2.2 数据访问标签: 主要包含用于输出ValueStack中的值、完成国际化等功能的标签 Ajax标签: 用于Ajax支持的标签 OGNL表达式语言OGNL的顶级对象是Stack Context，Stack Context对象就是一个Map类型的实例，其根对象就是Value Stack。OGNL的Stack Context里除了包括ValueStack这个根之外，还包括parameters、request、session、application、attr等命名对象，但这些命名对象都不是根。Stack Context“根”对象和普通命名对象的区别在于 访问Stack Context里的命名对象需要在对象名之前添加#前缀 访问OGNL的Stack Context里“根”对象的属性时，可以省略对象名 Struts2拦截器配置默认拦截器当配置一个包时，可以为其指定默认拦截器。一旦为某个包指定了默认的拦截器，如果该包中的Action没有显式指定拦截器，则默认的拦截器将会起作用。如果一旦为该包中的Action显式应用了某个拦截器，则默认的拦截器不会起作用，如果该Action还需要使用默认拦截器，则必须手动配置该拦截器的引用。 拦截器的执行顺序在Action的控制方法执行之前，位于拦截器链前面的拦截器将先发生作用；在Action的控制方法执行之后，位于拦截器链前面的拦截器将后发生作用。 参考文献[1] 轻量级JavaEE企业应用实战-Struts2+Spring4+Hibernate整合开发]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Struts2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSP/Servlet基础知识汇总]]></title>
    <url>%2F2015%2F06%2F17%2FBasic-knowledge-summary-of-JSP-Servlet%2F</url>
    <content type="text"><![CDATA[力求用最简洁的文字表述最全面的知识，本Blog不适合零基础人员 JSP与Servlet所有的JSP页面最终都会被编译成Servlet执行，而在Servlet类中主要有三个方法，分别是 init(): 初始化JSP/Servlet的方法 destroy(): 销毁JSP/Servlet的方法 service(): 对用户请求生成响应的方法 JSP页面必须放到应用服务器中运行，当第一次访问JSP页面时，该JSP页面会被编译成Servlet，如果JSP没有改动的话，以后访问的都是第一次编译成功的Servlet。 JSP的4种基本语法JSP注释&lt;%--JSP注释--%&gt; 这种注释在客户端浏览器中使用查看源代码是无法查看的&lt;!--HTML注释--&gt; 这种注释在客户端浏览器中使用查看源代码可以进行查看 JSP声明、输出、脚本&lt;%! 声明部分 %&gt; 相当于全局变量或方法，可使用private、public、static等&lt;%=count++%&gt; 输出表达式语法后不能有分号&lt;% scriptlet %&gt; JSP脚本会转换成_jspService方法里的可执行代码，而Java语法不允许在方法里定义方法，所以JSP脚本里不能定义方法；不能使用private、public、static等 JSP的3个编译指令 page: 该指令是针对当前页面的指令 include: 用于指定包含另一个页面 taglib: 用于定义和访问自定义标签 其中&lt;%@ include file=&quot;scriptlet.jsp&quot; %&gt;是静态包含指令，该指令包含进来的页面不需要是一个完整的页面，同时会将被包含页面的编译指令包含进来，如果两个页面的编译指令冲突，那么页面就会报错 动态包含如下所示123&lt;jsp:include page="&#123;scriptlet.jsp|&lt;%=expressi%&gt;&#125;"&gt; &#123;&lt;jsp:param name="parameterName" value="parameterValue"&gt;&#125;&lt;/jsp:include&gt; 该动态指令仅仅会将被包含页面的body内容插入本页面，这时候被包含页面的编译指令不会被插入本页面 JSP的7个动作指令 jsp:forward: 执行页面转向，将请求的处理转发到下一个页面，属于服务器端跳转，地址栏不变，执行forward时不会丢失请求参数 jsp:param: 用于传递参数，必须与其他支持参数的标签一起使用，主要是结合include、forward、plugin指令使用 jsp:include: 用于动态引入一个JSP页面 jsp:plugin: 用于下载JavaBean或Applet到客户端执行 jsp:useBean: 创建一个JavaBean的实例 jsp:setProperty: 设置JavaBean实例的属性值 jsp:getProperty: 输出JavaBean实例的属性值 useBean的用法&lt;jsp:useBean id=&quot;name&quot; class=&quot;classname&quot; scope=&quot;page|request|session|application&quot;/&gt; page: 该JavaBean实例仅在该页面有效 request: 该JavaBean实例在本次请求有效 session: 该JavaBean实例在本次session内有效 application: 该JavaBean实例在本次应用内一直有效 setProperty语法&lt;jsp:setProperty name=&quot;BeanName&quot; property=&quot;propertyName&quot; value=&quot;value&quot;&gt; getProperty语法&lt;jsp:getProperty name=&quot;BeanName&quot; property=&quot;propertyName&quot;&gt; 其实，当使用setProperty和getProperty时，底层是调用setBeanName()和getBeanName()方法来操作实例的属性的 JSP的9大内置对象 application: javax.servlet.ServletContext的实例，该实例代表JSP所属的Web应用本身，可用于JSP页面，或者在Servlet之间交换信息 config: javax.servlet.ServletConfig的实例，该实例代表JSP的配置信息，该对象使用较少 exception: java.lang.Throwable的实例，该实例代表其他页面中的异常和错误。只有当页面编译指令page的isErrorPage属性为true时，该对象才可以使用 out: javax.servlet.jsp.JspWriter的实例，该实例代表JSP页面的输出流，用于输出内容，形成HTML页面 page: 代表该页面本身，通常没有太大用处。也就是Servlet中的this，其类型就是生成的Servlet类，能用page的地方就能用this pageContext: javax.servlet.jsp.PageContext的实例，该对象代表该JSP页面上下文，使用该对象可以访问页面中的共享数据 request: javax.servlet.http.HttpServletRequest的实例，该对象封装了一次请求，客户端的请求参数都被封装在该对象里 response: javax.servlet.http.HttpServletResponse的实例，代表服务器对客户端的响应，通常很少使用该对象直接响应，而是使用out对象，除非需要生成非字符响应 session: javax.servlet.http.HttpSession的实例，该对象代表一次会话。当客户端浏览器与站点建立连接时，会话开始；当客户端关闭浏览器时，会话结束 Note 由于JSP内置对象都是在_jspService()方法中完成初始化的，因此只能在JSP脚本、JSP输出表达式中使用这些内置对象。千万不要在JSP声明中使用它们！否则，会提示找不到变量。 所有使用out的地方，都可使用输出表达式来代替，而且使用输出表达式更加简洁，&lt;%=...%&gt;表达式的本质就是out.write();。 pageContext可以访问page/request/session/application范围的变量，方法是取得指定范围的name属性，并指定其scope，如下所示 1pageContext.getAttribute(String name,[PageContext.PAGE_SCOPE|PageContext.REQUEST_SCOPE|PageContext.SESSION_SCOPE|PageContext.APPLICATION_SCOPE]) request有两个方法分别是forward和include 123request.getRequestDispatcher("/a.jsp").include(request,response)代替&lt;jsp:include&gt;指令request.getRequestDispatcher("/a.jsp").forward(request,response)代替&lt;jsp:forward&gt;指令 不要滥用session，通常只应该把与用户会话状态相关的信息放入session范围内，不要仅仅为了两个页面之间交换信息，就将该信息放入session范围内。如果仅仅为了两个页面交换信息，建议使用request，然后forward请求即可。session机制通常用于保存客户端的状态信息，这些状态信息需要保存到Web服务器的硬盘上，所以要求session里的属性值必须是可序列化的，否则将引发不可序列化的异常。 Filter创建Filter必须实现javax.servlet.Filter接口，在该接口中定义了如下三个方法 void init(FilterConfig config): 用于完成Filter的初始化 void destroy(): 用于Filter销毁前，完成某些资源的回收 void doFilter(ServletRequest request, ServletResponse response, FilterChain chain): 实现过滤功能，该方法就是对每个请求及响应增加的额外处理 Filter其实就是增强的Servlet，假设系统包含多个Servlet，这些Servlet都需要进行一些通用处理：比如权限控制、记录日志等，这将导致在这些Servlet的service方法中有部分代码是相同的——为了解决这种代码重复的问题，可以考虑把这些通用处理提取到Filter中完成，这样各Servlet中剩下的只是特定请求相关的处理代码，而通用处理则交给Filter完成。 使用URL Rewrite实现网站伪静态下载地址，目前最新版是urlrewritefilter-4.0.3.jar，在下载页面提供了使用方法，不再赘述。 Listener常用的Web事件监听器接口有如下几个 ServletContextListener: 用于监听Web应用的启动和关闭 ServletContextAttributeListener: 用于监听ServletContext范围（application）内属性的改变 ServletRequestListener: 用于监听用户请求 ServletRequestAttributeListener: 用于监听ServletRequest范围（request）内属性的改变 HttpSessionListener: 用于监听用户session的开始和结束 HttpSessionAttributeListener: 用于监听HttpSession范围（session）内属性的改变 JSP2特性目前Servlet3.1对应于JSP2.3规范，JSP2.3也被统称为JSP2。表达式语言是JSP2的一个重要特性，它并不是一种通用的程序语言，而仅仅是一种数据访问语言，可以方便地访问应用程序数据，避免使用JSP脚本。表达式语言的格式是${expression}，如果需要在支持表达式语言的页面中正常输出“\$”符号，则在“\$”符号前加转义字符“\”。 表达式语言的内置对象表达式语言包含如下11个内置对象 pageContext: 代表该页面的pageContext对象，与JSP的pageContext内置对象相同 pageScope: 用于获取page范围的属性值 requestScope: 用于获取request范围的属性值 sessionScope: 用于获取session范围的属性值 applicationScope: 用于获取application范围的属性值 param: 用于获取请求的参数值 paramValues: 用于获取请求的参数值，与param的区别在于，该对象用于获取属性值为数组的属性值 header: 用于获取请求头的属性值 headerValues: 用于获取请求头的属性值，与header的区别在于，该对象用于获取属性值为数组的属性值 initParam: 用于获取请求Web应用的初始化参数 cookie: 用于获取指定的Cookie值 Servlet3.0新特性Servlet3.0规范在javax.servlet.annotation包下提供了如下注解 @WebServlet: 用于修饰一个Servlet类，用于部署Servlet类 @WebInitParam: 用于与@WebServlet或@WebFilter一起使用，为Servlet、Filter配置参数 @WebListener: 用于修饰Listener类，用于部署Listener类 @WebFilter: 用于修饰Filter类，用于部署Filter类 @MultipartConfig: 用于修饰Servlet，指定该Servlet将会负责处理multipart/form-data类型的请求 @ServiceSecurity: 这是一个与JAAS有关的注解，修饰Servlet指定该Servlet的安全与授权控制 @HttpConstraint: 用于与@ServletSecurity一起使用，用于指定该Servlet的安全与授权控制 @HttpMethodConstraint: 用于与@ServletSecurity一起使用，用于指定该Servlet的安全与授权控制 参考文献[1] 轻量级JavaEE企业应用实战-Struts2+Spring4+Hibernate整合开发]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>JSP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL dump导入导出数据库命令汇总]]></title>
    <url>%2F2015%2F06%2F03%2FMySQL-dump-import-export-database-command-summary%2F</url>
    <content type="text"><![CDATA[导出所有的数据库 mysqldump -uuserName -ppassword –all-database &gt; D:/all.sql 需要注意的是，该命令需要在MySql的安装目录的bin目录下使用，例如在bin下输入mysqldump，会给出提示信息12345C:\Program Files\MySQL\MySQL Server 5.6\bin &gt; mysqldumpUsage: mysqldump [OPTIONS] database [tables]OR mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]OR mysqldump [OPTIONS] --all-databases [OPTIONS]For more options, use mysqldump --help 导入所有的数据库 source D:/all.sql 需要注意的是，该命令需要在MySql的命令行窗口中使用（MySql commond line client），例如在命令行中输入source，给出提示信息123mysql&gt; sourceERROR:Usage: \. &lt;filename&gt; | source &lt;filename&gt; 导出某个数据库123C:\Program Files\MySQL\MySQL Server 5.6\bin&gt;mysqldump -uroot -padmin --databases mysql mywebsite &gt; D:/mysqlmywebsite.sqlWarning: Using a password on the command line interface can be insecure. 这种方式会将mysql和mywebsite两个数据库导入到一个sql文件中，文件名称为mysqlmywebsite 导入某个数据库 mysql&gt;source D:/mysqlmywebsite.sql或者在cmd的命令行中指定导入的数据库C:\Program Files\MySQL\MySQL Server 5.6\bin&gt;mysql -uroot -padmin db1 &lt; D:/db1.sql 导出某些数据表 C:\Program Files\MySQL\MySQL Server 5.6\bin&gt; mysqldump -uusername -ppassword db1 table1 table2 &gt; tb1tb2.sql 导入某些数据表 在系统命令行 mysql -uusername -ppassword db1 &lt; tb1tb2.sql 或MySql命令行 mysql&gt; use db1; mysql&gt; source tb1tb2.sql; mysqldump字符集设置 mysqldump -uusername -ppassword –default-character-set=UTF8 db1 table1 &gt; tb1.sql 注意在MySql中，用UTF8表示UTF-8编码，比如在MySql命令行中输入show charset;可以查看到MySql所支持的所有字符集 12345678910111213141516171819202122232425262728293031323334353637383940414243444546mysql&gt; show charset;+----------+-----------------------------+---------------------+--------+| Charset | Description | Default collation | Maxlen |+----------+-----------------------------+---------------------+--------+| big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 || dec8 | DEC West European | dec8_swedish_ci | 1 || cp850 | DOS West European | cp850_general_ci | 1 || hp8 | HP West European | hp8_english_ci | 1 || koi8r | KOI8-R Relcom Russian | koi8r_general_ci | 1 || latin1 | cp1252 West European | latin1_swedish_ci | 1 || latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 || swe7 | 7bit Swedish | swe7_swedish_ci | 1 || ascii | US ASCII | ascii_general_ci | 1 || ujis | EUC-JP Japanese | ujis_japanese_ci | 3 || sjis | Shift-JIS Japanese | sjis_japanese_ci | 2 || hebrew | ISO 8859-8 Hebrew | hebrew_general_ci | 1 || tis620 | TIS620 Thai | tis620_thai_ci | 1 || euckr | EUC-KR Korean | euckr_korean_ci | 2 || koi8u | KOI8-U Ukrainian | koi8u_general_ci | 1 || gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 || greek | ISO 8859-7 Greek | greek_general_ci | 1 || cp1250 | Windows Central European | cp1250_general_ci | 1 || gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 || latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 || armscii8 | ARMSCII-8 Armenian | armscii8_general_ci | 1 || utf8 | UTF-8 Unicode | utf8_general_ci | 3 || ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 || cp866 | DOS Russian | cp866_general_ci | 1 || keybcs2 | DOS Kamenicky Czech-Slovak | keybcs2_general_ci | 1 || macce | Mac Central European | macce_general_ci | 1 || macroman | Mac West European | macroman_general_ci | 1 || cp852 | DOS Central European | cp852_general_ci | 1 || latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 || utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 || cp1251 | Windows Cyrillic | cp1251_general_ci | 1 || utf16 | UTF-16 Unicode | utf16_general_ci | 4 || utf16le | UTF-16LE Unicode | utf16le_general_ci | 4 || cp1256 | Windows Arabic | cp1256_general_ci | 1 || cp1257 | Windows Baltic | cp1257_general_ci | 1 || utf32 | UTF-32 Unicode | utf32_general_ci | 4 || binary | Binary pseudo charset | binary | 1 || geostd8 | GEOSTD8 Georgian | geostd8_general_ci | 1 || cp932 | SJIS for Windows Japanese | cp932_japanese_ci | 2 || eucjpms | UJIS for Windows Japanese | eucjpms_japanese_ci | 3 |+----------+-----------------------------+---------------------+--------+40 rows in set (0.00 sec)]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faceted Project Problem (Java Version Mismatch) Error Message]]></title>
    <url>%2F2015%2F06%2F02%2FFaceted-Project-Problem-Java-Version-Mismatch-Error-Message%2F</url>
    <content type="text"><![CDATA[在eclipse的 “problems” 选项卡中显示如下错误信息123Description:Type Project facet Java 1.8 is not supported by target runtime Apache Tomcat v7.0Resource:groupping... 由StackOverflow上的回答可知，Java facet的版本总是需要和Java编译器的版本一致，所以最好的方式是通过Project Facets Properties面板进行修改。 查看Problems面板信息 打开Project Facets Properties面板 修改configuration信息 调整到和Java编译器版本相匹配 参考文献[1] http://javahonk.com/project-facet-java-is-not-supported-by-target-runtime/[2] http://stackoverflow.com/questions/2239959/faceted-project-prblem-java-version-mismatch-error-message]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows客户端配置GitHub的SSH公钥]]></title>
    <url>%2F2015%2F06%2F02%2FWindows-client-configuration-of-SSH-public-key-on-GitHub%2F</url>
    <content type="text"><![CDATA[检查SSH keys的设置1$ cd ~/.ssh/ 如果显示”No such file or directory”，跳到第三步，否则继续。 备份和移除原来的SSH key设置如果已经存在key文件，需要备份该数据并删除之12345$ lsid_rsa id_rsa.pub known_hosts$ mkdir key_backup$ cp id_rsa* key_backup/$ rm id_rsa* 生成新的SSH key输入下面的代码，可以生成新的key文件，只需要使用默认的设置即可，当需要输入文件名的时候，回车即可 123456789101112131415161718192021$ ssh-keygen -t rsa -C "你的邮箱@qq.com"Generating public/private rsa key pair.Enter file in which to save the key (/c/Users/WX/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /c/Users/WX/.ssh/id_rsa.Your public key has been saved in /c/Users/WX/.ssh/id_rsa.pub.The key fingerprint is:1d:cc:7e:b3:7e:92:f7:ab:c6:75:56:73:62:30:bc:8c 你的邮箱@qq.comThe key's randomart image is:+--[ RSA 2048]----+| . || o + || +o + || oE.o o.o|| S o o. .+|| . o .o|| o....|| .ooo || o=.oo|+-----------------+ 添加SSH key到GitHub用文本编辑工具打开id_rsa.pub文件，如果看不到这个文件，你需要设置显示隐藏文件。准确的复制这个文件的内容，才能保证设置的成功。在GitHub的主页上点击设置按钮，选择SSH Keys项，把复制的内容粘贴进去，然后点击Add Key按钮即可，Title任意选择。 测试一下输入下面命令，测试是否设置成功12$ ssh -T git@github.comHi shijiebei2009! You've successfully authenticated, but GitHub does not provide shell access. 设置你的账号信息现在你已经可以通过SSH链接到GitHub了，还有一些个人信息需要完善的。 Git会根据用户的名字和邮箱来记录提交。GitHub也是用这些信息来做权限的处理，输入下面的代码进行个人信息的设置，把名称和邮箱替换成你自己的，名字必须是你的真名，而不是GitHub的昵称。12$ git config --global user.name "你的名字"$ git config --global user.email "your_email@youremail.com" 解决本地多个SSH key问题如果在eclipse或者其它云平台，可能也需要使用SSH key来认证，如果每次都覆盖原来的is_rsa文件，那么之前的认证就失效了，这个问题可以通过在~/.ssh目录下增加config文件来解决。 依然需要配置git用户名和邮箱12git config user.name "用户名"git config user.email "邮箱" 生成ssh key时同时指定保存的文件名1ssh-keygen -t rsa -f ~/.ssh/id_rsa.codepub -C "email" 上面的id_rsa.codepub就是我们指定的文件名，这时~/.ssh目录下会多出id_rsa.codepub和id_rsa.codepub.pub两个文件，id_rsa.codepub.pub里保存的就是我们要使用的key。 新增并配置config文件如果config文件不存在，先添加；存在则直接修改1touch ~/.ssh/config 在config文件里添加如下内容(User表示你的用户名)123Host *.codepub.cnIdentityFile ~/.ssh/id_rsa.codepubUser admin 上传key到云平台后台并测试1sh -T git@git.codepub.cn 成功的话，会输出welcome欢迎信息！日后如需添加，则按照上述配置生成key，并修改config文件即可。 参考文献[1] http://beiyuu.com/github-pages/[2] http://riny.net/2014/git-ssh-key/]]></content>
      <categories>
        <category>Git/GitHub</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中Web Project如何加载dll/DLL文件]]></title>
    <url>%2F2015%2F05%2F19%2FHow-to-load-dll-file-in-Java-Web-Project%2F</url>
    <content type="text"><![CDATA[基本上常用的项目有两种，一种是Java Project，另一种是Web Project，下面就以这两种项目为例，来阐释如何在项目中加载dll文件。 Java中调用dll的方式 System.load()123456/***Loads the native library specified by the filename argument. The filename argument must be an absolute path name.*/public static void load(String filename)//等价于Runtime.getRuntime().load(name) 由JDK的说明文档可知，load()方法接收的是绝对路径。 System.loadLibrary() 123456/**Loads the native library specified by the libname argument. The libname argument must not contain any platform specific prefix, file extension or path.*/public static void loadLibrary(String libname)//等价于Runtime.getRuntime().loadLibrary(name) 由JDK的说明文档可知，loadLibrary()方法不接收任何平台相关的特定前缀、文件扩展名或者路径。该方法会自动搜索一些固定路径，我们只需要把dll文件放入Path（系统的环境变量）路径下或者System32路径下，也可以直接放在Java项目的根目录下。以上几种方法都可以成功加载。 Java Project如果是Java Project，只要把dll文件放在Path或者System32或者项目根目录下,基本上都可以正确的加载到dll文件。 Web Project根据2014年最流行的应用服务器的统计结果，41%的应用服务器部署的是Tomcat，其中出乎我意料的是Jetty以31%的份额占据了第二把交椅，那就以最常用的Tomcat为例吧。 如果使用load()方法加载dll文件，那么代码应该这么写123String path = DBUtils.class.getResource("/").getPath();path = path.replaceAll("%20", " ");//排除中文空格System.load(path + "user.dll"); 如果使用loadLibrary()方法加载dll文件，简单的如上处理已经不能解决问题了，这是因为Web项目中，将 java.library.path 这个系统属性输出了一下，结果出来两个路径： %JAVA_HOME%/bin %TOMCAT_HOME%/bin 当然了其实还应该包括一个路径就是 %JRE_HOME%/bin 所以这时候，在Windows上的Web Project系统至少有三个地方可以放置我们的dll文件，你可以任取其一即可。对于JAVA_HOME和JRE_HOME，只要放入它们对应的bin目录下即可，但是TOMCAT_HOME就稍微复杂一点，一定要记住把dll文件放在和启动Tomcat的文件同一目录中，一般来说，放入tomcat的bin目录下即可，这是因为启动tomcat的命令startup.bat就在bin目录下，但是如果启动tomcat的命令不在tomcat的bin目录下，dll文件就应该放在启动tomcat的命令所在的目录。 也许有人说，难道有的tomcat的启动命令不是在bin目录下，是的，maybe就真的存在，比如在Eclipse中配置tomcat（免安装的tomcat版本），然后在Eclipse中启动tomcat，这时候，仅仅简单的将dll文件放在tomcat/bin/目录下是行不通的，大家可以测试一下。本人测试在JAVA_HOME的bin目录中是完全没问题的，但是这样的话当项目越来越多，dll文件越来越多，就很难区分开哪些dll文件是属于哪些项目的了。 路径测试环境是编译完成的本地LTP动态链接库，这些链接库可能还会加载LTP训练好的一些model-模型文件，所以作为测试不太合适，因为受model不定因素影响，但是可以借鉴一下。 在Eclipse中配置的Tomcat情况下，当在类中使用System.loadLibrary(“ner_jni”);方法时 将dll文件放在WEB-INF/lib/下面找不到dll文件将dll文件放在Tomcat/bin/目录下面找不到dll文件将dll文件放在Web Project的src目录下面找不到dll文件 当在类中使用123String str = NER.class.getResource("/").getPath();str = str.replaceAll("%20", "");System.load(str + "ner_jni.dll"); 方式时，并将dll文件放在Web Project的src目录下会出现 [java.lang.UnsatisfiedLinkError: D:\devSoft\apache-tomcat-7.0.52\webapps\DaseLab\WEB-INF\classes\segmentor_jni.dll: Can’t find dependent libraries] 这已经不是找不到dll文件的问题了，而是找不到依赖库，说明这种方式或许是可以加载到dll文件的，但是加载dll文件的顺序不正确，先加载的dll文件可能会依赖于后加载的dll文件，所以这种方式，一定要注意加载dll文件的顺序。解决办法可以参见【4】中的资料。 使用loadLibrary()方法时，将dll文件放在Java安装目录的bin/下，这种方式肯定可以加载到dll文件，但是可能会出现 Classloader, XXX.dll already loaded in another classloader 参考文献[1] http://www.cnblogs.com/zfc2201/archive/2011/09/02/2163268.html[2] http://www.programgo.com/article/4119403163/[3] http://blog.csdn.net/zhyh1986/article/details/9199063[4] http://www.111cn.net/jsp/Java/41523.htm]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>DLL</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git过滤上传文件的方法]]></title>
    <url>%2F2015%2F05%2F14%2FGit-filtering-method-of-uploaded-files%2F</url>
    <content type="text"><![CDATA[针对单一工程排除文件针对单一工程排除文件，这种方式会让这个工程的所有修改者在克隆代码的同时，也能克隆到过滤规则，而不用自己再写一份，这就能保证所有修改者应用的都是同一份规则。在工程根目录下建立.gitignore文件，将要排除的文件或目录写到.gitignore这个文件中，有两种写入方法：① 使用命令行增加排除文件例如排除以.class结尾的文件1echo "*.class" &gt; .gitignore 例如排除bin目录下的所有文件1echo "bin/*" &gt; .gitignore 注意，如果需要排除多级目录下的文件，比如bin/classes/A.class那么.gitignore中的内容应该写全路径，即bin/classes/A.class ② 最方便的办法是，用记事本打开，增加需要排除的文件或目录，一行增加一个，如：12345*.class*.apkbin/*.settings/*proguard/* 全局设置排除文件全局设置排除文件，这会在全局起作用，只要是Git管理的工程，在提交时都会自动排除不在控制范围内的文件或目录。这种方法对开发者来说，比较省事，只要一次全局配置，不用每次建立工程都要配置一遍过滤规则。但是这不保证其他的开发者在克隆你的代码后，他们那边的规则跟你的是一样的，这就带来了代码提交过程中的各种冲突问题。配置步骤如下： ① 同样需要建立一个.gitignore文件，把要排除的文件写进去。但在这里，我们不规定一定要把.gitnore文件放到某个工程下面，而是任何地方，比如我们这里放到了Git默认的Home路径下，在我的windows上就是C:\Users\yourusername。使用命令方式可以配置全局排除文件1git config --global core.excludesfile ~/.gitignore 你会发现在~/.gitconfig文件中会出现excludesfile = C:/Users/yourusername/.gitignore。说明Git把文件过滤规则应用到了Global的规则中。 ② 单个工程设置排除文件，在工程目录下找到.git/info/exclude，把要排除的文件写进去：1234*.class*.apkbin/gen/ 这种方法就不提倡了，只能针对单一工程配置，而且还不能将过滤规则同步到其他开发者，优势不大。 编写.gitignore规则文件的两种模式两种模式分别是开放模式和保守模式 ① 开放模式负责设置过滤哪些文件和文件夹过滤文件夹设置：/mtk/ 表示过滤这个文件夹过滤文件设置：123*.zip*.rar*.err 指定过滤某个文件：/mtk/do.doc ② 保守模式负责设置哪些文件不被过滤，也就是哪些文件要跟踪跟踪某个文件夹：!/people/master跟踪某类文件：12!*.c!*.h 跟踪某个指定文件：!/people/master/IT.h采用共享模式与保守模式结合配置的办法。eg：一个文件夹下有很多文件夹和文件，而我只想跟踪其中的一个文件，这样设置就可以满足这种情况，先用共享模式把整个目录 都设置为不跟踪，然后再用保守模式把这个文件夹中想要跟踪的文件设置为被跟踪，配置很简单，就可以跟踪想要跟踪的文件。 .gitignore文件失效问题比如在一个本地仓库中，产生了诸多的日志记录，而这些记录都是本地操作产生的，我们不必将其提交到远程仓库中，那么我们在.gitignore中添加了logs/20150514.log的过滤规则，但是在使用git status的时候，还是可以看到modified:logs/20150514.log，说明规则没有起作用。 为什么增加了.gitignore里的规则却没有效果呢？这是因为.gitignore文件只能作用于Untracked Files，也就是那些从来没有被Git记录过的文件（自添加以后，从未add及commit过的文件）。 之所以规则不生效，是因为那些.log文件曾经被Git记录过，因此.gitignore对它们完全无效，解决办法如下： 从Git的数据库中删除对于该文件的追踪； 把对应的规则写入.gitignore，让忽略真正生效； 提交+推送 只有这样做，所有的团队成员才会保持一致而不会有后遗症，也只有这样做，其他的团队成员根本不需要做额外的工作来维持对一个文件的改变忽略。 最后有一点需要注意的，git rm –cached logs/20150514.log 删除的是追踪状态，而不是物理文件；如果你真的是彻底不想要了，你也可以直接 rm＋忽略＋提交。 参考文献[1] http://blog.csdn.net/hustpzb/article/details/8649545[2] http://segmentfault.com/q/1010000000430426]]></content>
      <categories>
        <category>Git/GitHub</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[封装LTP4J的本地LTML调用接口]]></title>
    <url>%2F2015%2F05%2F13%2FLocal-call-interface-of-LTP4J-project-for-LTML-encapsulation%2F</url>
    <content type="text"><![CDATA[LTP4J简介LTP4J是对LTP的Java接口封装，众所周知LTP底层均是C++实现，所以对于需要Java接口的开发人员来说要通过调用LTP4J的接口实现调用LTP的目的，但是该项目仅仅封装了几个独立的方法，分别是NER/Parser/Postagger/Segmentor/SRL与之对应的实现功能是命名实体识别/依存句法分析/词性标注/分词/语义角色标注，LTP4J中封装的都是native方法，无法查看源代码。 Java之native关键字Java native Interface简称JNI，即Java本地接口，JNI是一个编程框架，可以使得运行在Java虚拟机上的Java程序调用与平台相关的其它语言实现的程序，比如C/C++/汇编等。 换言之，JNI允许用本地代码来解决纯粹用Java编程不能解决的平台相关的特性。对于Windows平台上的C++程序而言，在Java中将某个方法声明为native的，然后用C++程序实现该方法的具体功能，之后将C++程序编译为DLL文件即可。在Java虚拟机执行到该native方法的时候，会由操作系统去寻找对应的DLL文件进而调用该方法的C++实现。 所以使用LTP4J的前提是需要编译LTP源码及LTP4J的源码，在Windows X64平台上编译LTP源码及LTP4J源码请参见： 编译哈工大语言技术平台云LTP（C++）源码及LTP4J（Java）源码 在Windows平台上使用LTP4J需要处理BOM问题，解决办法请参见： Java处理带BOM文本的推荐方法 LTML简介有关LTML的详细信息参见LTML数据表示LTML 标准要求如下：结点标签分别为 xml4nlp, note, doc, para, sent, word, arg 共七种结点标签： xml4nlp 为根结点，无任何属性值； note 为标记结点，具有的属性分别为：sent, word, pos, ne, parser, srl；分别代表分句，分词，词性标注，命名实体识别，依存句法分析，词义消歧，语义角色标注；值为”n”，表明未做，值为”y”则表示完成，如pos=”y”，表示已经完成了词性标注； doc 为篇章结点，以段落为单位包含文本内容；无任何属性值； para 为段落结点，需含id 属性，其值从0 开始； sent 为句子结点，需含属性为id，cont；id 为段落中句子序号，其值从0 开始；cont 为句子内容； word 为分词结点，需含属性为id, cont；id 为句子中的词的序号，其值从0 开始，cont为分词内容；可选属性为 pos, ne, parent, relate；pos 的内容为词性标注内容；ne 为命名实体内容；parent 与relate 成对出现，parent 为依存句法分析的父亲结点id 号，relate 为相对应的关系； arg 为语义角色信息结点，任何一个谓词都会带有若干个该结点；其属性为id, type, beg，end；id 为序号，从0 开始；type 代表角色名称；beg 为开始的词序号，end 为结束的序号； LTP的在线版提供LTML格式的返回数据(亦即XML格式)，地址在这里，但是本地版不提供，需要自己封装。 封装本地版LTML的Java调用接口对外提供了两个接口，其一是根据文件内容获取完整的LTML标注结果，文件编码默认UTF-8，代码如下：12345678910111213141516171819package edu.shu.ltp4j.test;import org.junit.Test;import edu.shu.ltp4j.util.LTPUtil;public class TestGetFileLTML &#123; @Test public void test() &#123; String path = "test.txt"; LTPUtil ltpUtil = new LTPUtil(); ltpUtil.setNe(true); ltpUtil.setParser(true); ltpUtil.setPos(true); ltpUtil.setSrl(true); String ltml = ltpUtil.getLTML(path); System.out.println(ltml); &#125;&#125; 本地分析输出结果：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;xml4nlp&gt; &lt;note ne="y" parser="y" pos="y" sent="y" srl="y" word="y" /&gt; &lt;doc&gt; &lt;para id="0"&gt; &lt;sent cont="我爱你中国！" id="0"&gt; &lt;word cont="我" id="0" ne="O" parent="1" pos="r" relate="SBV" /&gt; &lt;word cont="爱" id="1" ne="O" parent="-1" pos="v" relate="HED"&gt; &lt;arg beg="0" end="0" id="0" type="A0" /&gt; &lt;arg beg="2" end="3" id="1" type="A1" /&gt; &lt;/word&gt; &lt;word cont="你" id="2" ne="O" parent="3" pos="r" relate="ATT" /&gt; &lt;word cont="中国" id="3" ne="S-Ns" parent="1" pos="ns" relate="VOB" /&gt; &lt;word cont="！" id="4" ne="O" parent="1" pos="wp" relate="WP" /&gt; &lt;/sent&gt; &lt;/para&gt; &lt;para id="1"&gt; &lt;sent cont="事件语料的建立对事件及其关系识别和推理有重要意义，因此针对该语料库的建立进行研究有一定的理论意义和应用价值。" id="0"&gt; &lt;word cont="事件" id="0" ne="O" parent="1" pos="n" relate="ATT" /&gt; &lt;word cont="语料" id="1" ne="O" parent="3" pos="n" relate="ATT" /&gt; &lt;word cont="的" id="2" ne="O" parent="1" pos="u" relate="RAD" /&gt; &lt;word cont="建立" id="3" ne="O" parent="-1" pos="v" relate="HED"&gt; &lt;arg beg="4" end="13" id="0" type="A1" /&gt; &lt;/word&gt; &lt;word cont="对" id="4" ne="O" parent="11" pos="p" relate="ADV" /&gt; &lt;word cont="事件" id="5" ne="O" parent="8" pos="n" relate="SBV" /&gt; &lt;word cont="及其" id="6" ne="O" parent="7" pos="c" relate="LAD" /&gt; &lt;word cont="关系" id="7" ne="O" parent="5" pos="n" relate="COO" /&gt; &lt;word cont="识别" id="8" ne="O" parent="4" pos="v" relate="POB"&gt; &lt;arg beg="5" end="7" id="0" type="A0" /&gt; &lt;/word&gt; &lt;word cont="和" id="9" ne="O" parent="10" pos="c" relate="LAD" /&gt; &lt;word cont="推理" id="10" ne="O" parent="8" pos="v" relate="COO" /&gt; &lt;word cont="有" id="11" ne="O" parent="3" pos="v" relate="VOB"&gt; &lt;arg beg="12" end="13" id="0" type="A1" /&gt; &lt;/word&gt; &lt;word cont="重要" id="12" ne="O" parent="13" pos="a" relate="ATT" /&gt; &lt;word cont="意义" id="13" ne="O" parent="11" pos="n" relate="VOB" /&gt; &lt;word cont="，" id="14" ne="O" parent="3" pos="wp" relate="WP" /&gt; &lt;word cont="因此" id="15" ne="O" parent="20" pos="c" relate="ADV" /&gt; &lt;word cont="针对" id="16" ne="O" parent="20" pos="p" relate="ADV" /&gt; &lt;word cont="该" id="17" ne="O" parent="18" pos="r" relate="ATT" /&gt; &lt;word cont="语料库" id="18" ne="O" parent="16" pos="n" relate="POB" /&gt; &lt;word cont="的" id="19" ne="O" parent="16" pos="u" relate="RAD" /&gt; &lt;word cont="建立" id="20" ne="O" parent="3" pos="v" relate="COO"&gt; &lt;arg beg="15" end="15" id="0" type="DIS" /&gt; &lt;arg beg="21" end="30" id="1" type="A1" /&gt; &lt;/word&gt; &lt;word cont="进行" id="21" ne="O" parent="30" pos="v" relate="ATT"&gt; &lt;arg beg="22" end="29" id="0" type="A1" /&gt; &lt;/word&gt; &lt;word cont="研究" id="22" ne="O" parent="21" pos="v" relate="VOB" /&gt; &lt;word cont="有" id="23" ne="O" parent="22" pos="v" relate="COO"&gt; &lt;arg beg="24" end="27" id="0" type="A1" /&gt; &lt;/word&gt; &lt;word cont="一定" id="24" ne="O" parent="27" pos="b" relate="ATT" /&gt; &lt;word cont="的" id="25" ne="O" parent="24" pos="u" relate="RAD" /&gt; &lt;word cont="理论" id="26" ne="O" parent="27" pos="n" relate="ATT" /&gt; &lt;word cont="意义" id="27" ne="O" parent="23" pos="n" relate="VOB" /&gt; &lt;word cont="和" id="28" ne="O" parent="29" pos="c" relate="LAD" /&gt; &lt;word cont="应用" id="29" ne="O" parent="23" pos="v" relate="COO" /&gt; &lt;word cont="价值" id="30" ne="O" parent="20" pos="n" relate="VOB" /&gt; &lt;word cont="。" id="31" ne="O" parent="3" pos="wp" relate="WP" /&gt; &lt;/sent&gt; &lt;/para&gt; &lt;/doc&gt;&lt;/xml4nlp&gt; 其二根据一个完整的句子获取到完整的LTML标注结果，完整的句子意思是使用“。|？|！|；”进行切分的结果，代码如下：12345678910111213141516171819package edu.shu.ltp4j.test;import org.junit.Test;import edu.shu.ltp4j.util.LTPUtil;public class TestGetSentectLTML &#123; @Test public void testSentence() &#123; LTPUtil ltpUtil = new LTPUtil(); ltpUtil.setNe(true); ltpUtil.setParser(true); ltpUtil.setPos(true); ltpUtil.setSrl(true); String s = "事件语料的建立对事件及其关系识别和推理有重要意义，因此针对该语料库的建立进行研究有一定的理论意义和应用价值。"; String ltmlBySentence = ltpUtil.getLTMLBySentence(s); System.out.println(ltmlBySentence); &#125;&#125; 本地分析输出结果：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;xml4nlp&gt; &lt;note ne="y" parser="y" pos="y" sent="y" srl="y" word="y" /&gt; &lt;doc&gt; &lt;para id="0"&gt; &lt;sent cont="事件语料的建立对事件及其关系识别和推理有重要意义，因此针对该语料库的建立进行研究有一定的理论意义和应用价值。" id="0"&gt; &lt;word cont="事件" id="0" ne="O" parent="1" pos="n" relate="ATT" /&gt; &lt;word cont="语料" id="1" ne="O" parent="3" pos="n" relate="ATT" /&gt; &lt;word cont="的" id="2" ne="O" parent="1" pos="u" relate="RAD" /&gt; &lt;word cont="建立" id="3" ne="O" parent="-1" pos="v" relate="HED"&gt; &lt;arg beg="4" end="13" id="0" type="A1" /&gt; &lt;/word&gt; &lt;word cont="对" id="4" ne="O" parent="11" pos="p" relate="ADV" /&gt; &lt;word cont="事件" id="5" ne="O" parent="8" pos="n" relate="SBV" /&gt; &lt;word cont="及其" id="6" ne="O" parent="7" pos="c" relate="LAD" /&gt; &lt;word cont="关系" id="7" ne="O" parent="5" pos="n" relate="COO" /&gt; &lt;word cont="识别" id="8" ne="O" parent="4" pos="v" relate="POB"&gt; &lt;arg beg="5" end="7" id="0" type="A0" /&gt; &lt;/word&gt; &lt;word cont="和" id="9" ne="O" parent="10" pos="c" relate="LAD" /&gt; &lt;word cont="推理" id="10" ne="O" parent="8" pos="v" relate="COO" /&gt; &lt;word cont="有" id="11" ne="O" parent="3" pos="v" relate="VOB"&gt; &lt;arg beg="12" end="13" id="0" type="A1" /&gt; &lt;/word&gt; &lt;word cont="重要" id="12" ne="O" parent="13" pos="a" relate="ATT" /&gt; &lt;word cont="意义" id="13" ne="O" parent="11" pos="n" relate="VOB" /&gt; &lt;word cont="，" id="14" ne="O" parent="3" pos="wp" relate="WP" /&gt; &lt;word cont="因此" id="15" ne="O" parent="20" pos="c" relate="ADV" /&gt; &lt;word cont="针对" id="16" ne="O" parent="20" pos="p" relate="ADV" /&gt; &lt;word cont="该" id="17" ne="O" parent="18" pos="r" relate="ATT" /&gt; &lt;word cont="语料库" id="18" ne="O" parent="16" pos="n" relate="POB" /&gt; &lt;word cont="的" id="19" ne="O" parent="16" pos="u" relate="RAD" /&gt; &lt;word cont="建立" id="20" ne="O" parent="3" pos="v" relate="COO"&gt; &lt;arg beg="15" end="15" id="0" type="DIS" /&gt; &lt;arg beg="21" end="30" id="1" type="A1" /&gt; &lt;/word&gt; &lt;word cont="进行" id="21" ne="O" parent="30" pos="v" relate="ATT"&gt; &lt;arg beg="22" end="29" id="0" type="A1" /&gt; &lt;/word&gt; &lt;word cont="研究" id="22" ne="O" parent="21" pos="v" relate="VOB" /&gt; &lt;word cont="有" id="23" ne="O" parent="22" pos="v" relate="COO"&gt; &lt;arg beg="24" end="27" id="0" type="A1" /&gt; &lt;/word&gt; &lt;word cont="一定" id="24" ne="O" parent="27" pos="b" relate="ATT" /&gt; &lt;word cont="的" id="25" ne="O" parent="24" pos="u" relate="RAD" /&gt; &lt;word cont="理论" id="26" ne="O" parent="27" pos="n" relate="ATT" /&gt; &lt;word cont="意义" id="27" ne="O" parent="23" pos="n" relate="VOB" /&gt; &lt;word cont="和" id="28" ne="O" parent="29" pos="c" relate="LAD" /&gt; &lt;word cont="应用" id="29" ne="O" parent="23" pos="v" relate="COO" /&gt; &lt;word cont="价值" id="30" ne="O" parent="20" pos="n" relate="VOB" /&gt; &lt;word cont="。" id="31" ne="O" parent="3" pos="wp" relate="WP" /&gt; &lt;/sent&gt; &lt;/para&gt; &lt;/doc&gt;&lt;/xml4nlp&gt; 整个项目的源代码已经上传到Github仓库，欢迎下载使用！]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>LTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java处理带BOM文本的推荐方法]]></title>
    <url>%2F2015%2F05%2F12%2FRecommended-processing-method-of-text-with-bom-in-Java%2F</url>
    <content type="text"><![CDATA[什么是BOM？参见维基百科，通过阅读维基百科，简单点说BOM（byte-order mark）是位于码点U+FEFF的统一码字符的名称，当以UTF-16或UTF-32来将UCS/统一码字符所组成的字符串编码时，这个字符被用来标示其字节序。它常被用来当做标示文件是以UTF-8、UTF-16或UTF-32编码的记号。 从Unicode3.2开始，U+FEFF只能出现在字节流的开头，只能用于标识字节序，就如它的名称——字节序标记——所表示的一样；除此以外的用法已被舍弃。 在类Unix系统中，它们都聪明的舍弃了BOM标记，而在牛叉哄哄却最不适合程序员的Windows中，BOM被光荣的保留了，并且Windows上的记事本程序会自动在文本中添加字节顺序标记（BOM）到UTF-8文件中。这个保留的BOM标记会使得我们在处理文本过程中遇到诸多问题。在你不知情的情况下，处理起来比较麻烦，因为BOM是不可见的。只有使用带16进制功能的编辑器才可见。Java对文本的通用操作中是无法识别BOM的，所以需要借助其它办法解决。 带BOM文本的解决办法——法一123456789101112131415161718192021222324252627282930313233343536373839import java.io.File;import java.io.IOException;import org.apache.commons.io.FileUtils;/** * * &lt;p&gt; * ClassName TestBom * &lt;/p&gt; * &lt;p&gt; * Description 先按照UTF-8编码读取文本，之后跳过前三个字符，重新构建一个新的字符串，需要Apache commons IO包 * &lt;/p&gt; * * @author TKPad wangx89@126.com * &lt;p&gt; * Date 2015年5月12日 下午8:08:32 * &lt;/p&gt; * @version V1.0.0 * */public class TestBom &#123; public static void main(String[] args) &#123; // test.txt是带BOM编码的UTF-8编码 try &#123; String content = FileUtils.readFileToString(new File("C:/Users/TKPad/Desktop/test.txt")); byte[] bytes = content.getBytes(); System.out.println(content); System.out.println("有BOM字节大小-&gt;" + bytes.length); System.out.println("有BOM字符串大小-&gt;" + content.length()); String contentNoBom = new String(bytes, 3, bytes.length - 3); System.out.println(contentNoBom); System.out.println("无BOM字节大小-&gt;" + contentNoBom.getBytes().length); System.out.println("无BOM字符串大小-&gt;" + contentNoBom.length()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出结果： 事件语料的建立对事件及其关系识别和推理有重要意义，因此针对该语料库的建立进行研究有一定的理论意义和应用价值。有BOM字节大小-&gt;165有BOM字符串大小-&gt;55事件语料的建立对事件及其关系识别和推理有重要意义，因此针对该语料库的建立进行研究有一定的理论意义和应用价值。无BOM字节大小-&gt;162无BOM字符串大小-&gt;54 带BOM文本的解决办法——法二使用牛人封装的UnicodeReader和UnicodeInputStream，地址在这里，该类可以识别带BOM标记的文本，同样可以向空文件中写入BOM标记。参见以下Demo Example code using UnicodeReader class12345678910111213141516171819202122232425public static char[] loadFile(String file) throws IOException &#123; // read text file, auto recognize bom marker or use // system default if markers not found. BufferedReader reader = null; CharArrayWriter writer = null; UnicodeReader r = new UnicodeReader(new FileInputStream(file), null); char[] buffer = new char[16 * 1024]; // 16k buffer int read; try &#123; reader = new BufferedReader(r); writer = new CharArrayWriter(); while( (read = reader.read(buffer)) != -1) &#123; writer.write(buffer, 0, read); &#125; writer.flush(); return writer.toCharArray(); &#125; catch (IOException ex) &#123; throw ex; &#125; finally &#123; try &#123; writer.close(); reader.close(); r.close(); &#125; catch (Exception ex) &#123; &#125; &#125; &#125; Example code to write UTF-8 with bom marker12345678910111213141516171819202122public static void saveFile(String file, String data, boolean append) throws IOException &#123; BufferedWriter bw = null; OutputStreamWriter osw = null; File f = new File(file); FileOutputStream fos = new FileOutputStream(f, append); try &#123; // write UTF8 BOM mark if file is empty if (f.length() &lt; 1) &#123; final byte[] bom = new byte[] &#123; (byte)0xEF, (byte)0xBB, (byte)0xBF &#125;; fos.write(bom); &#125; osw = new OutputStreamWriter(fos, "UTF-8"); bw = new BufferedWriter(osw); if (data != null) bw.write(data); &#125; catch (IOException ex) &#123; throw ex; &#125; finally &#123; try &#123; bw.close(); fos.close(); &#125; catch (Exception ex) &#123; &#125; &#125; &#125; 带BOM文本的解决办法——法三使用Apache Commons IO包中的工具类，API参见这里 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import org.apache.commons.io.ByteOrderMark;import org.apache.commons.io.input.BOMInputStream;/** * * &lt;p&gt; * ClassName TestBom * &lt;/p&gt; * &lt;p&gt; * Description 先按照UTF-8编码读取文本，之后跳过前三个字符，重新构建一个新的字符串 * &lt;/p&gt; * * @author TKPad wangx89@126.com * &lt;p&gt; * Date 2015年5月12日 下午8:08:32 * &lt;/p&gt; * @version V1.0.0 * */public class TestBom &#123; public static void main(String[] args) &#123; // test.txt是带BOM编码的UTF-8编码 BOMInputStream bomWithOut; try &#123; // 默认排除掉BOM，但是使用hasBOM方法仍然可以检测到BOM的存在 bomWithOut = new BOMInputStream(new FileInputStream(new File("C:/Users/TKPad/Desktop/test.txt"))); if (bomWithOut.hasBOM()) &#123; System.out.println("当前流中包含BOM！"); &#125; byte[] bytes = new byte[1024]; int length = 0; String res = null; while ((length = bomWithOut.read(bytes)) != -1) &#123; res += new String(bytes, 0, length); &#125; System.out.println("无BOM读取：" + res.getBytes().length); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; // 当然你也可以包含BOM读取 BOMInputStream bomWithIn; try &#123; bomWithIn = new BOMInputStream(new FileInputStream(new File("C:/Users/TKPad/Desktop/test.txt")), true); if (bomWithIn.hasBOM()) &#123; System.out.println("当前流中包含BOM！"); &#125; byte[] bytes = new byte[1024]; int length = 0; String res = null; while ((length = bomWithIn.read(bytes)) != -1) &#123; res += new String(bytes, 0, length); &#125; System.out.println("有BOM读取：" + res.getBytes().length); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; // 注意，此处要注释掉哦！！ try &#123; // 也可以指定检测多种编码的bom，但目前仅支持UTF-8/UTF-16LE/UTF-16BE三种，对于UTF32之类不支持。 BOMInputStream bomIn = new BOMInputStream(new FileInputStream(new File("")), ByteOrderMark.UTF_16LE, ByteOrderMark.UTF_16BE); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出结果： 当前流中包含BOM！无BOM读取：166当前流中包含BOM！有BOM读取：169]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译哈工大语言技术平台云LTP（C++）源码及LTP4J（Java）源码]]></title>
    <url>%2F2015%2F05%2F07%2FCompile-the-Language-Technology-Platform(C%2B%2B)-and-LTP4J(Java)source-code%2F</url>
    <content type="text"><![CDATA[JDK：java version “1.8.0_31” Java(TM) SE Runtime Environment (build 1.8.0_31-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode) OS：win7 64bit cmake：V2.8.12/V3.2.2 LTP：V3.2.0 LTP4J：V1.0 Microsoft VS C++：V2010 编译LTP4J.jar下载Antltp4j的源码使用ant进行编译，首先需要下载ant，点我下载 配置Ant环境变量我的电脑–&gt;属性–&gt;高级系统设置–&gt;环境变量 新建ANT_HOME值为：“D:\apache-ant-1.9.4” 编辑PATH，在PATH变量值的末尾添加：“;%ANT_HOME%\bin” 验证是否配置成功，打开cmd，输入ant： C:\Users\TKPad&gt;ant Buildfile: build.xml does not exist! Build failed 这样的话，说明配置成功，因为ant默认运行build.xml文件，该文件需要手动创建。 编译LTP4J首先需要下载ltp4j源代码，下载完成之后，解压，比如我的地址为：D:\ltp4j-master（推荐重命名为ltp4j），打开cmd，进入d盘ltp4j项目根目录下，运行ant命令，得到： D:\ltp4j&gt;ant Buildfile: D:\ltp4j\build.xml clean: [delete] Deleting directory D:\ltp4j\output compile: [mkdir] Created dir: D:\ltp4j\output\classes [javac] D:\ltp4j\build.xml:17: warning: &apos;includeantruntime&apos; was not set, def aulting to build.sysclasspath=last; set to false for repeatable builds [javac] Compiling 7 source files to D:\ltp4j\output\classes jar: [mkdir] Created dir: D:\ltp4j\output\jar [jar] Building jar: D:\ltp4j\output\jar\ltp4j.jar main: BUILD SUCCESSFUL Total time: 4 seconds 说明编译成功！查看目录：D:\ltp4j\output\jar，发现存在ltp4j.jar文件，这时该Jar包就可以使用了。 编译C++代理程序安装CMakeltp4j使用的C++代理程序使用编译工具CMake构建项目。 在编译代理程序之前，你需要首先安装CMake。 CMake的网站在这里。如果你是Windows用户，请下载CMake的二进制安装包；如果你是Linux，Mac OS或Cygwin的用户，可以通过编译源码的方式安装CMake，当然，你也可以使用Linux的软件源来安装。 下载地址点这里，我选择的是Windows ZIP cmake-3.2.2-win32-x86.zip 下载LTP的C++源码下载地址，下载完成之后解压，我的存放路径为：D:\ltp-master（推荐重命名为ltp）。 配置LTP的安装路径修改D:\ltp4j目录下的CMakeLists.txt文件，修改如下部分的斜体加粗部分为你自己的LTP源码路径 find_package(JNI) set (LTP_HOME “D:/ltp” CACHE STRING “Use to specified ltp path”) set (LIBRARY_OUTPUT_PATH \${PROJECT_SOURCE_DIR}/libs) set (JNI_SOURCE_DIR \${PROJECT_SOURCE_DIR}/jni) 构建VC Project在项目文件夹下新建一个名为build的文件夹，使用CMake Gui，在source code中填入项目文件夹，在binaries中填入build文件夹。然后Configure –&gt; Generate。 在点击configure之后出现如下错误： The C compiler identification is unknown The CXX compiler identification is unknown CMake Error at CMakeLists.txt:2 (project): No CMAKE_C_COMPILER could be found. CMake Error at CMakeLists.txt:2 (project): No CMAKE_CXX_COMPILER could be found.Configuring incomplete, errors occurred! See also “D:/ltp4j/build/CMakeFiles/CMakeOutput.log”. See also “D:/ltp4j/build/CMakeFiles/CMakeError.log”. 那么可能是你的电脑上还没有C++编译环境，需要安装Microsoft Visual Studio C++，VS 2010 express版下载地址为：点我下载VS2010 安装完成之后，再次编译，如果出现： The C compiler “C:/Program Files (x86)/Microsoft Visual Studio 10.0/VC/bin/cl.exe” is not able to compile a simple test program. 那么参考文章里面的解决办法，说简单点就是以管理员权限运行c1.exe 解决办法如下： Right-Click-&gt;Properties on cl.exe in your VS install directory (the exact path appears in the CMake error); Choose the Compatibility Tab; Check “Run this program as administrator” in the “Privilege Level” box. 之后再次尝试编译，出现如下错误： CMake Error at CMakeLists.txt:2 (project): Cannot enable 64-bit tools with Visual Studio 2010 Express. Install the Microsoft Windows SDK v7.1 to get 64-bit tools: http://msdn.microsoft.com/en-us/windows/bb980924.aspx Configuring incomplete, errors occurred! 主要是因为32位版本不兼容造成的，打开地址，点击download下载Microsoft Windows SDK for Windows 7 and .NET Framework 4，下载完成之后运行即是在线安装的形式。如果以cmd命令行的形式运行cmake，需要将cmake的bin目录路径加入到操作系统的path变量中。 在configure阶段，再次出错 The C compiler identification is MSVC 16.0.30319.1 The CXX compiler identification is MSVC 16.0.30319.1 Check for working C compiler using: Visual Studio 10 Check for working C compiler using: Visual Studio 10 – broken CMake Error at D:/cmake-2.8.12.1-win32-x86/share/cmake-2.8/Modules/CMakeTestCCompiler.cmake:61 (message): The C compiler “C:/Program Files (x86)/Microsoft Visual Studio 10.0/VC/bin/cl.exe” is not able to compile a simple test program. It fails with the following output: Change Dir: D:/ltp4j/build/CMakeFiles/CMakeTmp Run Build Command:C:\Windows\Microsoft.NET\Framework\v4.0.30319\MSBuild.exe cmTryCompileExec1255268599.vcxproj /p:Configuration=Debug /p:VisualStudioVersion=10.0 Microsoft(R) 生成引擎版本 4.0.30319.17929 [Microsoft .NET Framework 版本 4.0.30319.18063] 版权所有(C) Microsoft Corporation 2007。保留所有权利。 生成启动时间为 2015/5/6 9:46:05。 节点 1 上的项目“D:\ltp4j\build\CMakeFiles\CMakeTmp\cmTryCompileExec1255268599.vcxproj”(默认目标)。 PrepareForBuild: 正在创建目录“cmTryCompileExec1255268599.dir\Debug\”。 正在创建目录“D:\ltp4j\build\CMakeFiles\CMakeTmp\Debug\”。 InitializeBuildStatus: 正在创建“cmTryCompileExec1255268599.dir\Debug\cmTryCompileExec1255268599.unsuccessfulbuild”，因为已指定“AlwaysCreate”。 ClCompile: C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\bin\CL.exe /c /Zi /W3 /WX- /Od /Ob0 /Oy- /D WIN32 /D _WINDOWS /D _DEBUG /D &quot;CMAKE_INTDIR=\&quot;Debug\&quot;&quot; /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Fo&quot;cmTryCompileExec1255268599.dir\Debug\\&quot; /Fd&quot;cmTryCompileExec1255268599.dir\Debug\vc100.pdb&quot; /Gd /TC /analyze- /errorReport:queue testCCompiler.c 用于 80x86 的 Microsoft (R) 32 位 C/C++ 优化编译器 16.00.30319.01 版 版权所有(C) Microsoft Corporation。保留所有权利。 cl /c /Zi /W3 /WX- /Od /Ob0 /Oy- /D WIN32 /D _WINDOWS /D _DEBUG /D &quot;CMAKE_INTDIR=\&quot;Debug\&quot;&quot; /D _MBCS /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Fo&quot;cmTryCompileExec1255268599.dir\Debug\\&quot; /Fd&quot;cmTryCompileExec1255268599.dir\Debug\vc100.pdb&quot; /Gd /TC /analyze- /errorReport:queue testCCompiler.c testCCompiler.c ManifestResourceCompile: C:\Program Files (x86)\Microsoft SDKs\Windows\v7.0A\bin\rc.exe /nologo /fo&quot;cmTryCompileExec1255268599.dir\Debug\cmTryCompileExec1255268599.exe.embed.manifest.res&quot; cmTryCompileExec1255268599.dir\Debug\cmTryCompileExec1255268599_manifest.rc Link: C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\bin\link.exe /ERRORREPORT:QUEUE /OUT:&quot;D:\ltp4j\build\CMakeFiles\CMakeTmp\Debug\cmTryCompileExec1255268599.exe&quot; /INCREMENTAL /NOLOGO kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST /ManifestFile:&quot;cmTryCompileExec1255268599.dir\Debug\cmTryCompileExec1255268599.exe.intermediate.manifest&quot; /MANIFESTUAC:&quot;level=&apos;asInvoker&apos; uiAccess=&apos;false&apos;&quot; /DEBUG /PDB:&quot;D:/ltp4j/build/CMakeFiles/CMakeTmp/Debug/cmTryCompileExec1255268599.pdb&quot; /SUBSYSTEM:CONSOLE /TLBID:1 /DYNAMICBASE /NXCOMPAT /IMPLIB:&quot;D:/ltp4j/build/CMakeFiles/CMakeTmp/Debug/cmTryCompileExec1255268599.lib&quot; /MACHINE:X86 cmTryCompileExec1255268599.dir\Debug\cmTryCompileExec1255268599.exe.embed.manifest.res cmTryCompileExec1255268599.dir\Debug\testCCompiler.obj /machine:X86 /debug LINK : fatal error LNK1123: 转换到 COFF 期间失败: 文件无效或损坏 [D:\ltp4j\build\CMakeFiles\CMakeTmp\cmTryCompileExec1255268599.vcxproj] 已完成生成项目“D:\ltp4j\build\CMakeFiles\CMakeTmp\cmTryCompileExec1255268599.vcxproj”(默认目标)的操作 失败。 生成失败。 “D:\ltp4j\build\CMakeFiles\CMakeTmp\cmTryCompileExec1255268599.vcxproj”(默认目标) (1) -&gt; (Link 目标) -&gt; LINK : fatal error LNK1123: 转换到 COFF 期间失败: 文件无效或损坏 [D:\ltp4j\build\CMakeFiles\CMakeTmp\cmTryCompileExec1255268599.vcxproj] 0 个警告 1 个错误 已用时间 00:00:00.81 CMake will not be able to correctly generate this project. Call Stack (most recent call first): CMakeLists.txt:2 (project) Configuring incomplete, errors occurred! See also “D:/ltp4j/build/CMakeFiles/CMakeOutput.log”. See also “D:/ltp4j/build/CMakeFiles/CMakeError.log”. 该问题可能是由于.NET Framework 4.5导致，解决办法是： CMake doesn’t seem to work if .net 4.5 is installed. If you run into this issue you have to uninstall .net 4.5. Uninstalling .net 4.5 seems to break Visual Studio so then you have to re-install VS2010. 简单点说就是，卸载.net 4.5，安装.net 4.0（如果电脑已经安装，就不必装了），在卸载.net 4.5会破坏VS2010，所以需要重新再安装一遍VS2010。参考文章1、参考文章2；安装完毕之后，重新以管理员身份的权限运行cmake-gui.exe，点击configure之后，会弹出对话框，如果是VC++2010的话，建议选择Visual Studio 10，如下图： 点击finish-&gt;configure，此时进度条开始变为绿色滚动。 The C compiler identification is MSVC 16.0.30319.1 The CXX compiler identification is MSVC 16.0.30319.1 Check for working C compiler using: Visual Studio 10 Check for working C compiler using: Visual Studio 10 – works Detecting C compiler ABI info Detecting C compiler ABI info - done Check for working CXX compiler using: Visual Studio 10 Check for working CXX compiler using: Visual Studio 10 – works Detecting CXX compiler ABI info Detecting CXX compiler ABI info - done Found JNI: D:/Program Files/Java/jdk1.8.0_31/lib/jawt.lib Configuring done configure结束之后点击generate。 如果您依然没有运行成功，那么建议使用cmake3.x之前的版本，原因是： Visual Studio 11 Win64 is the wrong generator for Visual Studio 2013. You need to use Visual Studio 12 2013 Win64 if you are using cmake-3.x or Visual Studio 12 Win64 for cmake-2.8.x. cmake3.x之前的版本下载地址：http://www.cmake.org/files/v2.8/ 编译源码构建后得到ALL_BUILD、RUN_TESTS、ZERO_CHECK三个VC Project。使用VS打开ALL_BUILD项目，选择Release(*)方式构建项目。 (注：boost::multi_array与VS2010不兼容的bug已经在3.1.0中得到修复，3.1.x及以上版本已经可以使用Debug方式构建，但出于效率考虑，仍旧建议使用Release方式构建。) 注意事项 该处编译需要设置Java环境变量JAVA_HOME。 需要保持c++编译器与JDK同是32位或者64位，否则JVM不能加载生成的动态库 编译源码出现 LINK : fatal error LNK1104: cannot open file ‘kernel32.lib’ 原因是 Check the VC++ directories, in VS 2010 these can be found in your project properties. Check whether “\$(WindowsSdkDir)\lib” is included in the directories list, if not, manually add it. If you’re building for X64 platform, you should select X64 from the “Platform” ComboBox, and make sure that “$(WindowsSdkDir)\lib\x64” is included in the directories list. 如果X64目录不包含在库目录中，需要手动添加到库目录中。如下图所示 编译LTP再次编译LTP4J出现如下错误： LINK : fatal error LNK1181: cannot open input file ‘postagger.lib’ LINK : fatal error LNK1181: cannot open input file ‘ner.lib’ 这是因为在编译LTP4J之前需要编译LTP，使用cmake构建LTP，如下图 之后在LTP的build目录下会生成VS Project，用VS打开D:\ltp\build\ALL_BUILD.vcxproj，使用Release X64方式进行编译，如果顺利的话，编译结果见下图 再次编译LTP4J在LTP编译成功之后，将D:\ltp\lib\Release下的所有文件拷贝到D:\ltp4j\build目录下，然后在VS中打开D:\ltp4j\build\ALL_BUILD.vcxproj，选择Release X64进行编译，编译结果如下： 1&gt;—— 已启动生成: 项目: ZERO_CHECK, 配置: Release x64 —— 2&gt;—— 已启动生成: 项目: split_sentence_jni, 配置: Release x64 —— 3&gt;—— 已启动生成: 项目: segmentor_jni, 配置: Release x64 —— 4&gt;—— 已启动生成: 项目: srl_jni, 配置: Release x64 —— 5&gt;—— 已启动生成: 项目: parser_jni, 配置: Release x64 —— 2&gt; Creating library D:/ltp4j/libs/Release/split_sentence_jni.lib and object D:/ltp4j/libs/Release/split_sentence_jni.exp 3&gt; Creating library D:/ltp4j/libs/Release/segmentor_jni.lib and object D:/ltp4j/libs/Release/segmentor_jni.exp 3&gt; segmentor_jni.vcxproj -&gt; D:\ltp4j\libs\Release\segmentor_jni.dll 4&gt; Creating library D:/ltp4j/libs/Release/srl_jni.lib and object D:/ltp4j/libs/Release/srl_jni.exp 5&gt; Creating library D:/ltp4j/libs/Release/parser_jni.lib and object D:/ltp4j/libs/Release/parser_jni.exp 2&gt; split_sentence_jni.vcxproj -&gt; D:\ltp4j\libs\Release\split_sentence_jni.dll 5&gt; parser_jni.vcxproj -&gt; D:\ltp4j\libs\Release\parser_jni.dll 4&gt; srl_jni.vcxproj -&gt; D:\ltp4j\libs\Release\srl_jni.dll 6&gt;—— 已启动生成: 项目: ner_jni, 配置: Release x64 —— 7&gt;—— 已启动生成: 项目: postagger_jni, 配置: Release x64 —— 6&gt; Creating library D:/ltp4j/libs/Release/ner_jni.lib and object D:/ltp4j/libs/Release/ner_jni.exp 7&gt; Creating library D:/ltp4j/libs/Release/postagger_jni.lib and object D:/ltp4j/libs/Release/postagger_jni.exp 6&gt; ner_jni.vcxproj -&gt; D:\ltp4j\libs\Release\ner_jni.dll 7&gt; postagger_jni.vcxproj -&gt; D:\ltp4j\libs\Release\postagger_jni.dll 8&gt;—— 已跳过生成: 项目: ALL_BUILD, 配置: Release x64 —— 8&gt;没有为此解决方案配置选中要生成的项目 ========== 生成: 成功 7 个，失败 0 个，最新 0 个，跳过 1 个 ========== 在Eclipse中使用离线版LTP配置Eclipse中项目所需资源在Eclipse中新建Java Project 导入ltp4j.jar windows下将ltp4j的libs文件夹中生成的所有动态库、以及原ltp lib文件夹下的splitsnt、segmentor、postagger、ner、parser、srl 6个动态库拷贝到项目根目录 linux下export LD_LIBRARY_PATH=#jni动态库路径# 配置完成之后，项目视图如下所示： 本地接口使用示例分词接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import java.util.ArrayList;import java.util.List;import edu.hit.ir.ltp4j.Segmentor;/**** &lt;p&gt;* ClassName TestSegment* &lt;/p&gt;* &lt;p&gt;* Description 分词接口* &lt;/p&gt;** @author TKPad wangx89@126.com* &lt;p&gt;* Date 2015年5月7日 下午10:09:23* &lt;/p&gt;* @version V1.0.0**/public class TestSegment &#123; public static void main(String[] args) &#123; // /MyTest/ if (Segmentor.create("ltp_data/cws.model") &lt; 0) &#123; System.err.println("load failed"); return; &#125; String sent = "我是中国人"; List&lt;String&gt; words = new ArrayList&lt;String&gt;(); int size = Segmentor.segment(sent, words); for (int i = 0; i &lt; size; i++) &#123; System.out.print(words.get(i)); if (i == size - 1) &#123; System.out.println(); &#125; else &#123; System.out.print("\t"); &#125; &#125; Segmentor.release(); &#125;&#125; 词性标注接口 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import java.util.ArrayList;import java.util.List;import edu.hit.ir.ltp4j.Postagger;/**** &lt;p&gt;* ClassName TestPostag* &lt;/p&gt;* &lt;p&gt;* Description 词性标注接口* &lt;/p&gt;** @author TKPad wangx89@126.com* &lt;p&gt;* Date 2015年5月7日 下午10:09:55* &lt;/p&gt;* @version V1.0.0**/public class TestPostag &#123; public static void main(String[] args) &#123; if (Postagger.create("ltp_data/pos.model") &lt; 0) &#123; System.err.println("load failed"); return; &#125; List&lt;String&gt; words = new ArrayList&lt;String&gt;(); words.add("我"); words.add("是"); words.add("中国"); words.add("人"); List&lt;String&gt; postags = new ArrayList&lt;String&gt;(); int size = Postagger.postag(words, postags); for (int i = 0; i &lt; size; i++) &#123; System.out.print(words.get(i) + "_" + postags.get(i)); if (i == size - 1) &#123; System.out.println(); &#125; else &#123; System.out.print("|"); &#125; &#125; Postagger.release(); &#125;&#125; 命名实体识别接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124import java.util.ArrayList;import java.util.List;import edu.hit.ir.ltp4j.NER;/**** &lt;p&gt;* ClassName TestNer* &lt;/p&gt;* &lt;p&gt;* Description 命名实体识别接口* &lt;/p&gt;** @author TKPad wangx89@126.com* &lt;p&gt;* Date 2015年5月7日 下午10:08:26* &lt;/p&gt;* @version V1.0.0**/public class TestNer &#123; public static void main(String[] args) &#123; if (NER.create("ltp_data/ner.model") &lt; 0) &#123; System.err.println("load failed"); return; &#125; List&lt;String&gt; words = new ArrayList&lt;String&gt;(); List&lt;String&gt; tags = new ArrayList&lt;String&gt;(); List&lt;String&gt; ners = new ArrayList&lt;String&gt;(); words.add("中国"); tags.add("ns"); words.add("国际"); tags.add("n"); words.add("广播"); tags.add("n"); words.add("电台"); tags.add("n"); words.add("创办"); tags.add("v"); words.add("于"); tags.add("p"); words.add("1941年"); tags.add("m"); words.add("12月"); tags.add("m"); words.add("3日"); tags.add("m"); words.add("。"); tags.add("wp"); NER.recognize(words, tags, ners); for (int i = 0; i &lt; words.size(); i++) &#123; System.out.println(ners.get(i)); &#125; NER.release(); &#125;&#125; 依存句法分析接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import java.util.ArrayList;import java.util.List;import edu.hit.ir.ltp4j.Parser;/**** &lt;p&gt;* ClassName TestParser* &lt;/p&gt;* &lt;p&gt;* Description 依存句法分析接口* &lt;/p&gt;** @author TKPad wangx89@126.com* &lt;p&gt;* Date 2015年5月7日 下午10:11:09* &lt;/p&gt;* @version V1.0.0**/public class TestParser &#123; public static void main(String[] args) &#123; if (Parser.create("ltp_data/parser.model") &lt; 0) &#123; System.err.println("load failed"); return; &#125; List&lt;String&gt; words = new ArrayList&lt;String&gt;(); List&lt;String&gt; tags = new ArrayList&lt;String&gt;(); words.add("一把手"); tags.add("n"); words.add("亲自"); tags.add("d"); words.add("过问"); tags.add("v"); words.add("。"); tags.add("wp"); List&lt;Integer&gt; heads = new ArrayList&lt;Integer&gt;(); List&lt;String&gt; deprels = new ArrayList&lt;String&gt;(); int size = Parser.parse(words, tags, heads, deprels); for (int i = 0; i &lt; size; i++) &#123; System.out.print(heads.get(i) + ":" + deprels.get(i)); if (i == size - 1) &#123; System.out.println(); &#125; else &#123; System.out.print(" "); &#125; &#125; Parser.release(); &#125;&#125; 语义角色标注接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import java.util.ArrayList;import java.util.List;import edu.hit.ir.ltp4j.Pair;import edu.hit.ir.ltp4j.SRL;/**** &lt;p&gt;* ClassName TestSrl* &lt;/p&gt;* &lt;p&gt;* Description 语义角色标注接口* &lt;/p&gt;** @author TKPad wangx89@126.com* &lt;p&gt;* Date 2015年5月7日 下午10:11:46* &lt;/p&gt;* @version V1.0.0**/public class TestSrl &#123; public static void main(String[] args) &#123; SRL.create("ltp_data/srl"); ArrayList&lt;String&gt; words = new ArrayList&lt;String&gt;(); words.add("一把手"); words.add("亲自"); words.add("过问"); words.add("。"); ArrayList&lt;String&gt; tags = new ArrayList&lt;String&gt;(); tags.add("n"); tags.add("d"); tags.add("v"); tags.add("wp"); ArrayList&lt;String&gt; ners = new ArrayList&lt;String&gt;(); ners.add("O"); ners.add("O"); ners.add("O"); ners.add("O"); ArrayList&lt;Integer&gt; heads = new ArrayList&lt;Integer&gt;(); heads.add(2); heads.add(2); heads.add(-1); heads.add(2); ArrayList&lt;String&gt; deprels = new ArrayList&lt;String&gt;(); deprels.add("SBV"); deprels.add("ADV"); deprels.add("HED"); deprels.add("WP"); List&lt;Pair&lt;Integer, List&lt;Pair&lt;String, Pair&lt;Integer, Integer&gt;&gt;&gt;&gt;&gt; srls = new ArrayList&lt;Pair&lt;Integer, List&lt;Pair&lt;String, Pair&lt;Integer, Integer&gt;&gt;&gt;&gt;&gt;(); SRL.srl(words, tags, ners, heads, deprels, srls); System.out.println(srls.size()); for (int i = 0; i &lt; srls.size(); ++i) &#123; System.out.println(srls.get(i).first + ":"); for (int j = 0; j &lt; srls.get(i).second.size(); ++j) &#123; System.out.println(" tpye = " + srls.get(i).second.get(j).first + " beg = " + srls.get(i).second.get(j).second.first + " end = " + srls.get(i).second.get(j).second.second); &#125; &#125; SRL.release(); &#125;&#125; 等等，最后一步注意点 在ltp4j中提供的ltp_data中的model仅仅是作为你测试代码的正确性，其并非完整的model，如果需要在生产环境中使用，需要到这里下载完整版的model，将下载的完整版ltp_data拷贝到项目中即可。 其他错误 Cannot open include file: ‘ammintrin.h’: No such file or directory 参考如下文章 http://wishmesh.com/2011/04/fatal-error-c1083-cannot-open-include-file-ammintrin-h-no-such-file-or-directory/ https://connect.microsoft.com/VisualStudio/feedback/details/660584/ 参考文献 【1】 哈工大LTP官网 【2】 LTP4J的github地址 【3】 LTP的github地址 【4】 LTP3.0参考文档 【5】 LTP4J1.0参考文档 【6】 VS2010与64位系统 【7】 安装 Microsoft Visual C++ 2010 和 Microsoft Windows SDK 7.1]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>LTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java代码实现按照文本的自然段落进行切分]]></title>
    <url>%2F2015%2F04%2F25%2FSegmentation-of-natural-paragraph-in-Java-code%2F</url>
    <content type="text"><![CDATA[本文给出了五种自然段落的组合方式，具体形式参见底部给出的链接，无积分下载，不管各个段落形式如何，只要段落之间存在换行分隔，该程序即可正确运行。在此提供两种切分段落的方法，分别见下面的法一和法二。法一：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.ArrayList;import java.util.List;/** * * &lt;p&gt; * ClassName SplitParagraph * &lt;/p&gt; * &lt;p&gt; * Description 对文本按照自然段落进行切分 * &lt;/p&gt; * * @author TKPad wangx89@126.com * &lt;p&gt; * Date 2015年4月25日 下午3:37:42 * &lt;/p&gt; * @version V1.0.0 * */public class SplitParagraph &#123; /** * * &lt;p&gt; * Title: getParagraph * &lt;/p&gt; * &lt;p&gt; * Description: 根据给出的文本路径，读取文本内容，并按照段落切分，以段落为单位，存储在List集合中 * &lt;/p&gt; * * @param filePath * @return List&lt;String&gt; 段落集合 * @throws IOException * */ private List&lt;String&gt; getParagraph(String filePath, String charset) throws IOException &#123; List&lt;String&gt; res = new ArrayList&lt;String&gt;(); StringBuilder sb = new StringBuilder();// 拼接读取的内容 InputStreamReader isr = new InputStreamReader(new FileInputStream(filePath), charset); BufferedReader br = new BufferedReader(isr); String temp; while ((temp = br.readLine()) != null) &#123; sb.append(temp.trim() + "\n"); &#125; // \s A whitespace character: [ \t\n\x0B\f\r] String p[] = sb.toString().split("\\s*\n"); for (String string : p) &#123; res.add(string.replaceAll("\\s*", "")); &#125; if (br != null) br.close(); if (isr != null) isr.close(); return res; &#125;&#125; 法二：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.io.BufferedReader;import java.io.File;import java.io.FileReader;import java.io.IOException;import java.util.ArrayList;import java.util.Iterator;/** * * &lt;p&gt; * ClassName GetParagraph * &lt;/p&gt; * &lt;p&gt; * Description 使用Java完成对一篇文本的自然段落的切分，在此给出了五种文本格式作为示例，对任一种格式，该程序均可以正确切分。 * &lt;/p&gt; * * @author TKPad wangx89@126.com * &lt;p&gt; * Date 2015年2月11日 下午1:33:03 * &lt;/p&gt; * @version V1.0.0 * */public class GetParagraph &#123; public static void main(String[] args) throws IOException &#123; ArrayList&lt;String&gt; res = new ArrayList&lt;String&gt;();// 段落切分结果 StringBuilder sb = new StringBuilder();// 拼接读取的内容 String temp = null;// 临时变量，存储sb去除空格的内容 String filePath = "C:\\Users\\TKPad\\Desktop\\test/a.txt"; BufferedReader reader = new BufferedReader(new FileReader(new File(filePath))); int ch = 0; while ((ch = reader.read()) != -1) &#123; temp = sb.toString().trim().replaceAll("\\s*", "");// 取出前后空格，之后去除中间空格 if ((char) ch == '\r') &#123; // 判断是否是空行 if (!"".equals(temp)) &#123; // 说明到了段落结尾，将其加入链表，并清空sb res.add(temp); &#125; sb.delete(0, sb.length()); &#125; else &#123; // 说明没到段落结尾，将结果暂存 sb.append((char) ch); &#125; &#125; if (reader.read() == -1) &#123; System.out.println("哈哈，你读到了末尾嘞！"); &#125; // 最后一段如果非空， 将最后一段加入，否则不处理 if (!"".equals(temp)) &#123; res.add(temp); &#125; Iterator&lt;String&gt; iterator = res.iterator(); while (iterator.hasNext()) &#123; String next = iterator.next(); System.out.println("段落开始："); System.out.println(next); &#125; System.out.println("段落的个数是：" + res.size()); &#125;&#125; 测试用例点我：http://download.csdn.net/download/shijiebei2009/8440133]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jar包中如何正确地加载资源文件]]></title>
    <url>%2F2015%2F04%2F22%2FHow-to-load-resource-file-in-a-Jar-package-correctly%2F</url>
    <content type="text"><![CDATA[JDK：java version “1.8.0_31”Java(TM) SE Runtime Environment (build 1.8.0_31-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode) OS：win7 64bit 在日常开发中，经常需要在项目中加载各种资源，包括图片、文本、声音等资源，在本地项目中运行代码可以正确地加载资源，但是当把本地项目打包成Jar包并发布到web项目中之后，即便将各种资源文件一并打包，依然会出现无法正确加载资源的问题。这说明，在你希望打包的项目中加载资源的方式是错误的。那么你或许会问，为什么在单独的项目中可以正确加载资源，而打包之后就无法加载了呢？这是因为，打包之后，jar包中的所加载的文件路径发生了变化，例如res.txt打包到C盘之后，其路径变为file:/C:/ResourceJar.jar!/resource/res.txt，如果你在原项目中使用new File(filePath)之类的方法来加载的话，肯定会找不到资源文件。主要是因为Jar包是一个单独的文件而非文件夹，绝对不可能通过file:/C:/.../ResourceJar.jar/resource /res.txt这种形式的文件URL来定位res.txt。所以即使是相对路径，也无法定位到Jar包内的资源文件。 以将要打包的项目jarProj为例，介绍如何在将要打包的项目中正确的加载资源，jarProj项目目录结构如下 (conf是Source Folder，创建方法是在项目上右击-&gt;New-&gt;Other-&gt;Java-&gt;Source Folder，使用该方式所创建的目录效果和src目录相同，在编译之后，其下的文件和src下的文件都会发布到bin目录下)。 jarProj- src– edu/shu/reference/jar/LoadFile.java conf– reource/text.txt 如果需要在jarProj项目中加载一些资源文件，之后将其打包，发布到其他项目之下，该jarProj项目依然可以正确的加载到资源文件，需要使用到getResource()和getResourceAsStream()两个方法，而这两个方法大多数人都是无法区分的，需要做一个简单介绍 Class.getResource(String path)path 不以’/‘开头时，默认是从此类所在的包下取资源；path 以’/‘开头时，则是从加载该类的class loader下获取；如果该类是由bootstrap class loader加载，那么该方法等效于ClassLoader.getSystemResource(java.lang.String) 如果使用this.getClass().getResource(&quot;myFile.ext&quot;)getResource方法会尽力从相对于该类所在包的路径下去寻找资源 如果使用this.getClass().getResource(&quot;/myFile.ext&quot;)getResource方法会把它看成绝对路径，并且会简单的从class loader根下去加载资源 如果使用this.getClass().getClassLoader().getResource(&quot;myFile.ext&quot;)这时候，传递的参数不能够以/作为路径的开始，因为所有的ClassLoader路径都是绝对路径，不需要参数以/作为合法的路径开始符号 在上述三项中，2和3是等效的。 当项目被打包之后，其中的资源文件的路径其实已经发生变化，所以用简单的方法去加载肯定是找不到资源的，一般jar包中资源路径如下：file:\D:\jarProj.jar!\resource\test.txt，这时有两种解决办法，第一：用URL根据Jar包规范构造输入流（jar包中的资源有其专门的URL形式：jar:&lt;url&gt;!/{entry}），然后进行读取；第二种是使用getResourceAsStream()进行加载，获取输入流，之后从输入流中读取，但是这样就无法使用诸如1new BufferedReader(new FileReader(new File(filepath))) 一类的方法了。 测试代码如下：第一种在加载资源类中使用getResourceAsStream()方法 1234567891011121314151617181920212223242526272829303132333435363738394041package edu.shu.reference.jar;import java.io.File;import java.io.IOException;import java.io.InputStream;import java.net.JarURLConnection;import java.net.URL;import java.util.Arrays;import java.util.Iterator;import java.util.List;import org.apache.commons.io.FileUtils;import org.apache.commons.io.IOUtils;import org.junit.Test;/** * * &lt;p&gt; * ClassName LoadFile * &lt;/p&gt; * &lt;p&gt; * Description 将Java项目打包，发布到Web项目下，依然可以正确的加载资源 * &lt;/p&gt; * * @author TKPad wangx89@126.com * &lt;p&gt; * Date 2015年4月22日 下午9:06:07 * &lt;/p&gt; * @version V1.0.0 * */public class LoadFile &#123; public void read() throws IOException &#123; getResource(); &#125; public void getResource() throws IOException &#123; InputStream is = this.getClass().getResourceAsStream("/resource/test.txt"); String fileContent = IOUtils.toString(is); System.out.println(fileContent); &#125;&#125; 将该项目打包，发布到其他项目下，在其他项目下编写测试类，代码如下：12345678910import java.io.IOException;import edu.shu.reference.jar.LoadFile;public class Test &#123; public static void main(String[] args) throws IOException &#123; LoadFile lf = new LoadFile(); lf.read(); &#125;&#125; 可以看到正确的输出结果，加载资源没问题！ 第二种使用URL加载Jar资源的方式加载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package edu.shu.reference.jar;import java.io.File;import java.io.IOException;import java.io.InputStream;import java.net.JarURLConnection;import java.net.URL;import java.util.Arrays;import java.util.Iterator;import java.util.List;import org.apache.commons.io.FileUtils;import org.apache.commons.io.IOUtils;import org.junit.Test;/** * * &lt;p&gt; * ClassName LoadFile * &lt;/p&gt; * &lt;p&gt; * Description 将Java项目打包，发布到Web项目下，依然可以正确的加载资源 * &lt;/p&gt; * * @author TKPad wangx89@126.com * &lt;p&gt; * Date 2015年4月22日 下午9:06:07 * &lt;/p&gt; * @version V1.0.0 * */public class LoadFile &#123; public void read() throws IOException &#123; String f = "/resource/test.txt"; URL resource = this.getClass().getResource(f); loadJar(resource.getFile()); &#125; /** * * &lt;p&gt; * Title: loadJar * &lt;/p&gt; * &lt;p&gt; * Description: ③ 封装了一个加载Jar包的方法，使用URL根据jar包加载规范获取输入流，并据此输入流做进一步的操作 * &lt;/p&gt; * * @param filePath * @throws IOException * */ public void loadJar(String filePath) throws IOException &#123; // 注意在WINDOWS环境一定要使用正斜杠 filePath = "jar:" + filePath; URL url = new URL(filePath); JarURLConnection openStream = (JarURLConnection) url.openConnection(); InputStream inputStream = openStream.getInputStream(); // 使用IO工具类将输入流输出到控制台 IOUtils.copy(inputStream, System.out); // 使用IO工具类从输入流中按行读取并存入List集合 List&lt;String&gt; contents = IOUtils.readLines(inputStream); System.out.println(contents); &#125;&#125; 将该项目打包，发布到其他项目下，在其他项目下编写测试类，代码如下：12345678910import java.io.IOException;import edu.shu.reference.jar.LoadFile;public class Test &#123; public static void main(String[] args) throws IOException &#123; LoadFile lf = new LoadFile(); lf.read(); &#125;&#125; 可以看到正确的输出结果，加载资源没问题！ 如果你经常使用Apache的commons-io包的话，那么在Jar包中就不适合使用FileUtils去加载资源文件了，例如下面的代码123FileUtils.readFileToString(new File(ClassName.class.getClassLoader().getResource("config.properties").toURI().toString()), Charset.forName("UTF-8"));FileUtils.readFileToString(new File(ClassName.class.getClassLoader().getResource("config.properties").toURI()), Charset.forName("UTF-8")); 这两种方式都无法正确的加载到资源文件，但是commons-io包的另一个类仍然非常有用，就是org.apache.commons.io.IOUtils类，一行代码就可以帮助我们读取资源文件中的所有内容1List&lt;String&gt; contents = IOUtils.readLines(ClassName.class.getClassLoader().getResourceAsStream("config.properties"), Charset.forName("UTF-8")); 即使该资源文件被打包到Jar文件中，这种方式依然没有任何问题，推荐使用。 说明 另外还有一种比较常见的形式ClassName.class.getClassLoader().getResourceAsStream(&quot;config.properties&quot;)，在这个时候使用Class.getClassLoader().getResource()和Class.getClassLoader().getResourceAsStream()在路径选择上是一样的，只不过两个方法返回的对象不同，前者返回URL对象，后者返回InputStream对象。 在使用ClassName.class.getClassLoader().getResourceAsStream(&quot;config.properties&quot;)的时候千万不要写成ClassName.class.getClass().getClassLoader().getResourceAsStream(&quot;config.properties&quot;)，后者会始终抛NullPointerException，因为class.getClass().getClassLoader().getResourceAsStream(...)使用的是系统类加载器，而你的Jar对系统类加载器是不可见的。 整个项目的下载地址在：http://download.csdn.net/detail/shijiebei2009/8631005 参考文献[1] http://hxraid.iteye.com/blog/483115[2] http://www.cnblogs.com/yejg1212/p/3270152.html[3] http://stackoverflow.com/questions/16570523/getresourceasstream-returns-null]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Commons入门]]></title>
    <url>%2F2015%2F04%2F19%2FIntroduction-to-Apache-Commons%2F</url>
    <content type="text"><![CDATA[Apache Commons是Apache软件基金会的项目，曾隶属于Jakarta项目。Commons的目的是提供可重用的、开源的Java代码。Commons由三部分组成：Proper（是一些已发布的项目）、Sandbox（是一些正在开发的项目）和Dormant（是一些刚启动或者已经停止维护的项目）。 目前Commons-IO包稳定版本是Version 2.4，可惜的是，对于我目前很需要的copyInputStreamToFile（final InputStream source, final File destination, boolean closeSource）方法，只能等到Version 2.5了，关于详情参见：https://issues.apache.org/jira/browse/IO-381 Apache Commons提供了全方位可重用的Java组件，在我们日常开发中，诸多问题的解决方法均可在Commons包中找到实现，使用Commons包提供的组件可以极大的提高开发效率，减少重复劳动，下表是Commons提供的组件的详细信息，鉴于英文水平一般，不予翻译，采用官网提供的标准信息。 以最常用的Commons包封装的方法为例，介绍一些简单的使用示例 将输入流转换成文本 12345678910111213141516package edu.shu.commons.io.test;import java.io.IOException;import java.io.InputStream;import java.net.URL;import org.apache.commons.io.IOUtils;public class TestCommonsIO &#123;public static void main(String[] args) throws IOException &#123;URL url = new URL("http://www.baidu.com");InputStream openStream = url.openStream();String string = IOUtils.toString(openStream, "UTF-8");System.out.println(string);&#125;&#125; 读文件与写文件 123456789101112131415161718package edu.shu.commons.io.test;import java.io.File;import java.io.IOException;import org.apache.commons.io.FileUtils;public class TestCommonsIO &#123;public static void main(String[] args) throws IOException &#123;String content = FileUtils.readFileToString(new File("test.txt"));System.out.println(content);FileUtils.writeStringToFile(new File("destination.txt"), content);// 指定编码的读content = FileUtils.readFileToString(new File("test.txt"), "UTF-8");// 指定编码的写FileUtils.writeStringToFile(new File("destination.txt"), content, "UTF-8");&#125;&#125; 输出结果为HTML 1234567891011121314package edu.shu.commons.io.test;import java.io.IOException;import org.apache.commons.lang3.StringEscapeUtils;public class TestCommonsIO &#123;public static void main(String[] args) throws IOException &#123;String content = "";String escapeHtml4 = StringEscapeUtils.escapeHtml4(content);System.out.println(escapeHtml4);&#125;&#125; 将文本按行读取并以行为元素存入List列表 1234567891011121314package edu.shu.commons.io.test;import java.io.File;import java.io.IOException;import java.util.List;import org.apache.commons.io.FileUtils;public class TestCommonsIO &#123;public static void main(String[] args) throws IOException &#123;List contents= FileUtils.readLines(new File("test.txt"));System.out.println(contents);&#125;&#125; 从ClassLoader加载器加载资源，并且读出文本内容、 1234567891011121314package edu.shu.commons.io.test;import java.io.IOException;import java.io.InputStream;import org.apache.commons.io.IOUtils;public class TestCommonsIO &#123;public static void main(String[] args) throws IOException &#123;InputStream is = TestCommonsIO.class.getClass().getResourceAsStream("/resource/d.txt");String content = IOUtils.toString(is);System.out.println(content);&#125;&#125; commons mail包主要是对java mail的封装，可以方便快速的发送邮件 12345678910111213141516171819import org.apache.commons.mail.DefaultAuthenticator;import org.apache.commons.mail.Email;import org.apache.commons.mail.EmailException;import org.apache.commons.mail.SimpleEmail;public class TestEmail &#123;public static void main(String args[]) throws EmailException &#123;Email email = new SimpleEmail();email.setHostName("smtp.qq.com");email.setSmtpPort(465);email.setAuthenticator(new DefaultAuthenticator("发送者账号", "发送者账号密码"));email.setSSLOnConnect(true);email.setFrom("sender mail@qq.com");email.setSubject("TestMail");email.setMsg("This is a test mail ... :-)");email.addTo("receiver mail@qq.com");email.send();&#125;&#125; 参考文献1 http://zh.wikipedia.org/wiki/Apache_Commons2 http://zhoualine.iteye.com/blog/1770014]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之volatile boolean，AutomaticBoolean分析]]></title>
    <url>%2F2015%2F04%2F10%2FJava's-volatile-boolean-AutomaticBoolean-analysis%2F</url>
    <content type="text"><![CDATA[volatile概述volatile关键字是一个类型修饰符，被设计用来修饰被不同线程访问和修改的变量，在JVM1.2之前，Java的内存模型实现总是从主存读取变量，是不需要进行特别的注意的。而随着JVM的成熟和优化，现在在多线程环境下volatile关键字的使用变得非常重要。 在当前的Java内存模型下，线程可以把变量保存在本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。 要解决这个问题，需要把变量声明为volatile（不稳定的），以后用到该变量都会到主存中进行存取，一般多任务环境下各任务间共享的标志都应该加volatile修饰。volatile修饰的成员变量在每次被线程访问时，都强迫从共享内存中重读该成员变量的值，而且当成员变量发生变化时，强迫线程将变化值回写到共享内存。 Java语言中的volatile变量可以被看作是一种“程度较轻的synchronized”；与synchronized块相比，volatile变量所需的编码较少，并且运行时开销也较少，但是它所能实现的功能也仅是synchronized的一部分。Volatile变量具有synchronized的可见性特性，但是不具备原子特性。volatile仅仅用来保证该变量对所有线程的可见性，但不保证原子性。 volatile是线程不安全的123456789101112131415public class Test implements Runnable &#123; public static volatile boolean flag = true; public static void main(String[] args) &#123; for (int j = 0; j &lt; 20; j++) &#123; Thread t = new Thread(new Test()); t.start(); &#125; &#125; public void run() &#123; if (flag) &#123; System.out.println("我成功了！"); flag = false; &#125; &#125;&#125; 输出结果：我成功了！我成功了！我成功了！我成功了！ 注意：不要将volatile用在getAndOperate场合，仅仅set或者get的场景是适合volatile的。 由以上测试用例可以看出，volatile boolean是无法保证线程安全的，在一个线程占用flag变量的时候，其他线程也可以占用flag变量，所以多个线程均可以打印出消息。 Atomic概述java.util.concurrent.atomic是在JDK1.5之后引入的包，在JDK中的说明如下 A small toolkit of classes that support lock-free thread-safe programming on single variables. In essence, the classes in this package extend the notion of volatile values, fields, and array elements to those that also provide an atomic conditional update operation of the form: 1boolean compareAndSet(expectedValue, updateValue); This method (which varies in argument types across different classes) atomically sets a variable to the updateValue if it currently holds the expectedValue, reporting true on success. The classes in this package also contain methods to get and unconditionally set values, as well as a weaker conditional atomic update operation weakCompareAndSet described below. 该包提供了一组原子类，在多线程环境下，当有多个线程同时执行这些类的实例包含的方法时，具有排他性，即当某个线程进入方法，执行其中的指令时，不会被其他线程打断，而别的线程就像自旋锁一样，一直等到该方法执行完成，才由JVM从等待队列中选择另一个线程进入。 Atomic是线程安全的123456789101112131415import java.util.concurrent.atomic.AtomicBoolean;public class Test implements Runnable &#123; public static volatile AtomicBoolean flag = new AtomicBoolean(true); public static void main(String[] args) &#123; for (int j = 0; j &lt; 20; j++) &#123; Thread t = new Thread(new Test()); t.start(); &#125; &#125; public void run() &#123; if (flag.compareAndSet(true, false)) &#123; System.out.println("我成功了！"); &#125; &#125;&#125; 输出结果：我成功了！ Atomic类不仅仅提供了对数据操作的线程安全保证，而且提供了一系列的语义清晰的方法，如incrementAndGet() 、getAndIncrement() 等，使用方便，Atomic的内部实现使用的是更加高效的CAS（compareand swap）+volatile，从而避免了synchronized的高开销，执行效率大幅提升，出于性能考虑强烈建议使用Atomic，而不是synchronized关键字。 参考文献[1] http://www.blogjava.net/aoxj/archive/2012/06/16/380926.html[2] http://blog.csdn.net/xieyuooo/article/details/8594713]]></content>
      <categories>
        <category>Programming Notes</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github Pages个人博客，从Octopress转向Hexo]]></title>
    <url>%2F2015%2F04%2F06%2FGithub-Pages-personal-blog-from-Octopress-to-Hexo%2F</url>
    <content type="text"><![CDATA[环境&amp;版本OS：win7 X64Hexo：V3.0.0Node.js：V0.12.2Git：Version 1.9.5.msysgit.1 关于为什么要开博客？请参见《为什么你要写博客？》《我的博客时代》 下面就让我们一起开启使用Hexo的全新旅程吧！ 安装Node.js下载Node.js参考地址：安装Node.js 安装Git下载地址：http://git-scm.com/download/ 注册GitHub访问：http://www.github.com/注册过程参见：一步步在GitHub上创建博客主页 全系列 配置和使用Github参见：如何搭建一个独立博客——简明Github Pages与Hexo教程 安装Hexo及主题设置安装hexo1234567$ cd /d/$ mkdir hexo$ cd hexo$ npm install -g hexo$ hexo init$ hexo g # 或者hexo generate$ hexo s # 或者hexo server，可以在http://localhost:4000/查看 注意在使用npm安装hexo的时候，如果在Git Bash中出现1sh.exe": npm: command not found 那么需要右击Git Bash以管理员身份运行，再次在Git Bash中输入npm install -g hexo即可。 复制主题12$ hexo clean$ git clone https://github.com/wuchong/jacman.git themes/jacman 启用主题修改Hexo目录下的_config.yml配置文件中的theme属性，将其设置为jacman。 更新主题1234$ cd themes/jacman$ git pull$ hexo g # 生成$ hexo s # 启动本地服务，进行文章预览调试 在浏览器中输入http://localhost:4000/如果显示的是繁体中文，那么修改_config.xml中的language: zh-CN。 绑定独立域名购买域名在你的域名注册提供商那里配置DNS解析，获取GitHub的IP地址点击，进入source目录下，添加CNAME文件12345$ cd source/$ touch CNAME$ vim CNAME # 输入你的域名，例如codepub.cn$ git add CNAME$ git commit -m "add CNAME" 修改_config.xml文件，添加你的Github中仓库地址，该仓库名称必须是yourusername.github.io，添加如下内容到_config.xml中 1234deploy: type: git repository: git@github.com:shijiebei2009/shijiebei2009.github.io.git # 注意换成自己的username branch: master 不会建仓库的童鞋参见hexo系列教程：（二）搭建hexo博客 推送到远程仓库12345$ git init$ git add .$ git commit -m "first commit"$ git remote add origin git@github.com:shijiebei2009/shijiebei2009.github.io.git$ git push -u origin master 如果出现123456$ git push -u origin masterUsername for 'https://github.com': shijiebei2009Password for 'https://shijiebei2009@github.com':remote: Repository not found.fatal: repository 'git://github.com/shijiebei2009/shijiebei2009.github.io.git/' not found 那是因为你在github网站上还没有建立该仓库导致。部署12$ hexo generate$ hexo deploy #或者组合命令 hexo d -g 如果出现：1ERROR Deployer not found: git 需要运行：1$ npm install hexo-deployer-git --save 进阶篇-高级定制添加插件添加sitemap和feed插件12$ npm install hexo-generator-feed$ npm install hexo-generator-sitemap 修改_config.yml，增加以下内容1234567891011121314# ExtensionsPlugins:- hexo-generator-feed- hexo-generator-sitemap#Feed Atomfeed: type: atom path: atom.xml limit: 20#sitemapsitemap: path: sitemap.xml 配完之后，就可以访问http://codepub.cn/atom.xml和http://codepub.cn/sitemap.xml，发现这两个文件已经成功生成了。 添加404公益页面GitHub Pages有提供制作404页面的指引：Custom 404 Pages。 直接在根目录下创建自己的404.html或者404.md就可以。但是自定义404页面仅对绑定顶级域名的项目才起作用，GitHub默认分配的二级域名是不起作用的，使用hexo server在本机调试也是不起作用的。 推荐使用腾讯公益404。 添加about页面1$ hexo new page "about" 之后在\source\about\index.md目录下会生成一个index.md文件，打开输入个人信息即可，如果想要添加版权信息，可以在文件末尾添加： 123456789101112&lt;div style="font-size:12px;border-bottom: #bbbbbb 1px solid; BORDER-LEFT: #bbbbbb 1px solid; BACKGROUND: #f6f6f6; HEIGHT: 120px; BORDER-TOP: #bbbbbb 1px solid; BORDER-RIGHT: #bbbbbb 1px solid" class=shijiebei2009right&gt;&lt;div style="MARGIN-TOP: 10px; FLOAT: left; MARGIN-LEFT: 5px; MARGIN-RIGHT: 10px"&gt;&lt;IMG alt="" src="https://avatars3.githubusercontent.com/u/4994697?v=3&amp;u=70a4ca810fb1908f2deeed95f3b962eec64e1787&amp;s=140" width=90 height=100&gt;&lt;/div&gt;&lt;div style="LINE-HEIGHT: 200%; MARGIN-TOP: 10px; COLOR: #000000"&gt;作者： &lt;a href="http://shijiebei2009.github.io/"&gt;王旭&lt;/a&gt; &lt;br/&gt;出处： &lt;a href="http://shijiebei2009.github.io/"&gt;http://shijiebei2009.github.io/&lt;/a&gt;&lt;br/&gt;本文基于&lt;a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt; 知识共享署名-相同方式共享 4.0 &lt;/a&gt;国际许可协议发布，欢迎转载，演绎或用于商业目的，但是必须保留本文的署名 &lt;a href="http://shijiebei2009.github.io/"&gt;王旭&lt;/a&gt;及链接。&lt;/div&gt;&lt;/div&gt; 发表文章的markdown语法12345678910111213141516title: postName #文章页面上的显示名称，可以任意修改，不会出现在URL中date: 2013-12-02 15:30:16 #文章生成时间，一般不改，当然也可以任意修改categories: example #分类tags: [tag1,tag2,tag3] #文章标签，可空，多标签请用格式，注意:后面有个空格description: 附加一段文章摘要，字数最好在140字以内。---设置摘要有两种方法1、使用&lt;!--more--&gt;标签2、不使用&lt;!--more--&gt;标签，仅显示部分摘要。在`D:/hexo/themes/jacman/_config.xml`文件中修改为index: expand: false ## default is unexpanding,so you can only see the short description of each post. excerpt_link: Read More以下正文 使用图床使用七牛云存储或者采用开源的图床，使用新浪SAE平台，但是鉴于不知道能否持久，自己掂量办，在线图床 markdown工具Windows在线markdown工具：https://www.zybuluo.com/mdeditorWindows本地markdown工具：markdownpad 添加Fork me on Github获取代码，选择你喜欢的代码添加到hexo/themes/jacman/layout/layout.ejs的末尾即可，注意要将代码里的you改成你的Github账号名。 添加支付宝捐赠按钮及二维码支付支付宝捐赠按钮在D:\hexo\themes\jacman\layout_widget目录下新建一个zhifubao.ejs文件，内容如下1234567891011&lt;p class="asidetitle"&gt;打赏他&lt;/p&gt;&lt;div&gt;&lt;form action="https://shenghuo.alipay.com/send/payment/fill.htm" method="POST" target="_blank" accept-charset="GBK"&gt; &lt;br/&gt; &lt;input name="optEmail" type="hidden" value="your 支付宝账号" /&gt; &lt;input name="payAmount" type="hidden" value="默认捐赠金额(元)" /&gt; &lt;input id="title" name="title" type="hidden" value="博主，打赏你的！" /&gt; &lt;input name="memo" type="hidden" value="你Y加油，继续写博客！" /&gt; &lt;input name="pay" type="image" value="转账" src="http://7xig3q.com1.z0.glb.clouddn.com/alipay-donate-website.png" /&gt;&lt;/form&gt;&lt;/div&gt; 添加完该文件之后，要在D:/hexo/themes/jacman/_config.yml文件中启用，如下所示，添加zhifubao1234567widgets:- category- tag- links- tagcloud- zhifubao- rss 二维码捐赠首先需要到这里获取你的支付宝账户的二维码图片，支付宝提供了自定义功能，可以添加自定义文字。 我的二维码扫描捐赠添加在about页面，当然你也可以添加到其它页面，在D:\hexo\source\about下有index.md，打开，在适当位置添加123456&lt;center&gt;欢迎您捐赠本站，您的支持是我最大的动力！![][1][1]: http://7xig3q.com1.z0.glb.clouddn.com/alipay-donate.png&lt;/center&gt;&lt;br/&gt; &lt;center&gt;可以让图片居中显示，注意将图片链接地址换成你的即可。 其它实用功能插件推荐hexo官方文档hexo插件大全自定义网站logoMarkDown中文网MarkDown语法说明社交分享推荐使用jiathis评论插件推荐使用duoshuo网站流量统计推荐cnzz网站流量统计推荐百度统计 作者信息需要修改与作者有关的一系列信息，修改D:/hexo/themes/jacman/_config.xml中的author/imglogo/favicon/author_img/apple_icon一系列属性即可。 删除warning: LF will be replaced by CRLF警告信息在hexo deploy时，有时会出现这个提示信息warning: LF will be replaced by CRLF，虽然看起来挺乱糟糟的，但不影响使用，可以忽略不计。若想不提示，可以使用如下方法：切换到博客的根目录，执行如下命令：123$ git config --global core.autocrlf false $ rm -rf .git #删除掉该目录下的.git文件夹$ git init #重新初始化 再deploy试试吧，清新脱俗了。 写博客或添加页面12345$ hexo new "postName" #新建文章$ hexo new page "pageName" #新建页面$ hexo generate #生成静态页面至public目录$ hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）$ hexo deploy #将.deploy目录部署到GitHub 常用简写1234$ hexo n == hexo new$ hexo g == hexo generate$ hexo s == hexo server$ hexo d == hexo deploy 常用组合12$ hexo d -g #生成部署$ hexo s -g #生成预览 安装hexo-generator-baidu-sitemap插件，专为百度量身打造1$ npm install hexo-generator-baidu-sitemap --save 然后在 Hexo 根目录下的 _config.yml 里配置一下12baidusitemap: path: baidusitemap.xml 推广博客与提交Sitemap百度网址提交入口360网址提交入口 添加百度站内搜索点击进入，点击其它工具-&gt;站内检索-&gt;现在使用-&gt;新建搜索引擎-&gt;查看代码，将代码里的id值复制，打开/d/hexo/themes/jacman/_config.xml，配置成如下即可。 1234baidu_search: ## http://zn.baidu.com/ enable: true id: "1433674487421172828" ## e.g. "783281470518440642" for your baidu search id site: http://zhannei.baidu.com/cse/search ## your can change to your site instead of the default site 使用不蒜子添加访客统计详情参考搞定你的网站计数，具体做法很简单，就是在你的themes/your themes/layout/_partial/footer.ejs底部加入这段脚本1&lt;script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 然后在&lt;p class=&quot;copyright&quot;&gt;&lt;/p&gt;中间添加如下统计信息即可1本站总访问量 &lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt; 次, 访客数 &lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt; 人次, 本文总阅读量 &lt;span id="busuanzi_value_page_pv"&gt;&lt;/span&gt; 次 不蒜子的官方服务网站是不蒜子，目前最大的弊端就是不开放注册，所以对于运行了一段时间的网站，不蒜子的数据都是从1开始，没办法设置，只有等后期开放注册之后，登入网站才能对统计计数进行设置。 Jacman主题相关为jacman主题添加最新评论本方法针对使用hexo搭建Github Pages静态博客，并且使用jacman主题的童鞋们。 首先在\themes\jacman\layout\_widget目录下新建latest_comment.ejs，放入“多说”最新评论代码，其中“多说”的最新评论代码点我获取，注意修改var duoshuoQuery = {short_name:&quot;您的多说二级域名&quot;};将其中的“short_name”设置为在多说配置的二级域名即可。在latest_comment.ejs的首行注意添加1&lt;p class="asidetitle"&gt;最新评论&lt;/p&gt; 然后进入\themes\jacman\_config.yml：在widgets下添加latest_comment即可，注意Windows下编码一定要采用UTF-8无BOM编码。 为jacman主题添加热评文章与上一条类似，首先在\themes\jacman\layout\_widget目录下新建hot_comments.ejs，然后去你在多说网站的后台管理界面http://codepub.duoshuo.com/admin/tools/中点击工具-&gt;热评文章获取代码，将代码复制到hot_comments.ejs文件中。同样在行首添加1&lt;p class="asidetitle"&gt;热评文章&lt;/p&gt; 然后在_config.yml中启用该widgets即可。 Jacman主题问题解答Q：如何添加数学公式mathjax？A：主题支持写LaTex数学公式。只需要在文章文件开头的front-matter中，加上一行mathjax: true，即可在文中写LaTex公式。 Q：自定义字体 ShowCustomFontA：是否启用自定义字体，默认开启，主要用于显示网站底部的字体。如果你有一定前端基础可以修改 font.styl 替换为你喜欢的字体。 Q：图片默认都是居左的，我怎么设置能让图片居中呢？A：使用 &lt;img src=&quot;&quot; style=&quot;display:block;margin:auto&quot;/&gt;的HTML标签或者是使用&lt;center&gt;包裹图片。 Q：如何建立一篇图片类文章（Gallery Post）？A：使用hexo new photo “your titile”建立图片类文章，或者直接新建一个 Markdown 文件，将其front-matter修改为如下，即可看到主题为图片类文章提供的样式。1234567---layout: phototitle: Gallery Postphotos:- http://i.minus.com/ibobbTlfxZgITW.jpg- http://i.minus.com/iedpg90Y0exFS.jpg--- Q：我在配置文件中给某一项设置了值，但为什么总是看不到效果啊？A：_config.yml文件中的每个属性值前面必须留一个空格，建议在Sublime/Notepad++中开启显示所有空格模式。另每篇文章的front-matter也要注意这个问题。 Q：如何建立自我介绍页面（About页面）？A：首先在主目录找到_config.yml，找到url添加about_dir: about到这个板块。然后在/source里面建立about文件夹。在about文件夹里建立index.md。编辑index.md就和发布其他的文章一样，格式都一样。 Q：楼主我不喜欢你的配色，怎么换主题的颜色呢？A：包括颜色在内的很多变量都在jacman/source/css/_base/variable.styl文件中，可以修改成你喜欢的。 参考文献[1] http://wuchong.me/blog/2014/11/20/how-to-use-jacman/#[2] https://pages.github.com/[3] http://www.jianshu.com/p/05289a4bc8b2[4] http://www.pchou.info/web-build/2014/07/04/build-github-blog-page-08.html[5] http://wuchong.me/blog/2014/11/20/how-to-use-jacman/]]></content>
      <categories>
        <category>Git/GitHub</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Github</tag>
        <tag>Octopress</tag>
      </tags>
  </entry>
</search>
